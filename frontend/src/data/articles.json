[
  {
    "id": 5,
    "title": "50张图，直观理解混合专家（MoE）大模型",
    "url": "https://blog.csdn.net/OneFlow_Official/article/details/144131666",
    "content": "\n<img alt=\"ba4d4c2ccfcf76a14f931690b2243f4d.png\" height=\"512\" src=\"https://img-blog.csdnimg.cn/img_convert/ba4d4c2ccfcf76a14f931690b2243f4d.png\" width=\"768\" referrerpolicy=\"no-referrer\">\n\n\n\n**[Mixtral 8x7B]()**的高效训练与推理效果曾引发AI社区对混合专家（MoE）模型的广泛关注，后来居上的国产开源大模型**[De‍epSeek]()**以及**[腾讯近期开源的Hunyuan-Large]()**（基于Transformer的最大MoE模型）也选择了MoE框架路线。为何大语言模型总是离不开MoE的身影？\n\n 借助50多个图例，数据科学家Maarten Grootendorst由浅入深多维度剖析了MoE模型，从基础概念出发，逐步介绍MoE核心组件专家和路由机制，以及它们在典型LLM架构中的应用。\n\n（本文经作者授权后由OneFlow编译发布。原文：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts）\n\n**作者 |Maarten Grootendorst**\n\n**OneFlow编译**\n\n<strong>翻译｜张雪聃、林心宇\n**<strong>题图由****[SiliconCloud]()平台生成**</strong></strong>\n \n\n**1**\n\n### 什么是混合专家（MoE）？\n\n混合专家（MoE）是一种利用多个不同的子模型（或称为“专家”）来提升LLM质量的技术。\n\nMoE的两个主要组成部分是：\n    专家：每个前馈神经网络（FFNN）层现在都有一组“专家”，可以选择其中的一部分。这些“专家”通常也是FFNN。        路由或门控网络：决定哪些词元发送到哪些专家。    \n在每个具有MoE的模型层中，我们会找到（相对专业化的）专家：\n\n<img alt=\"b784290bc715b1d660f1db747145cb6c.png\" src=\"https://img-blog.csdnimg.cn/img_convert/b784290bc715b1d660f1db747145cb6c.png\" referrerpolicy=\"no-referrer\">\n\n\n\n需要注意的是，“专家”并不专注于特定领域，如“心理学”或“生物学”。专家在学习过程中最多只能掌握关于单词层面的句法信息：\n\n<img alt=\"cb36bc3b00a93702eea6ba30700f9a7c.png\" src=\"https://img-blog.csdnimg.cn/img_convert/cb36bc3b00a93702eea6ba30700f9a7c.png\" referrerpolicy=\"no-referrer\">\n\n\n\n更具体地说，专家的专长是在特定上下文中处理特定词元。\n\n路由（门控网络）选择最适合特定输入的专家：\n\n<img alt=\"17ef16aceb0832f55e6cc3a5ed26d26d.png\" src=\"https://img-blog.csdnimg.cn/img_convert/17ef16aceb0832f55e6cc3a5ed26d26d.png\" referrerpolicy=\"no-referrer\">\n\n\n\n单个专家并不是整个LLM，而是LLM架构中的一个子模型部分。\n\n**2**\n\n### 专家\n\n为了探讨专家的含义及其工作方式，我们首先需要了解MoE所替代的内容：密集层。\n\n#### 密集层\n\n混合专家（MoE）始于LLM的相对基本功能，即前馈神经网络（FFNN）。\n\n请记住，标准的仅解码Transformer架构在层归一化后应用FFNN：\n\n<img alt=\"d87f24e698c131dbb10e7d51aab65925.png\" src=\"https://img-blog.csdnimg.cn/img_convert/d87f24e698c131dbb10e7d51aab65925.png\" referrerpolicy=\"no-referrer\">\n\n\n\nFFNN使模型能够利用由注意力机制创建的上下文信息，进一步转化以捕捉数据中更复杂的关系。\n\n然而，FFNN的规模会迅速增长。为了学习这些复杂关系，它通常会扩展接收到的输入：\n\n<img alt=\"94053b88e26e4aa753e8d53c5e3a2524.png\" src=\"https://img-blog.csdnimg.cn/img_convert/94053b88e26e4aa753e8d53c5e3a2524.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### 稀疏层\n\n传统Transformer中的FFNN被称为密集模型，因为所有参数（权重和偏置）都会被激活。没有任何东西被遗漏，所有东西都用于计算输出。\n\n如果我们仔细观察密集模型，会发现输入在一定程度上激活了所有参数：\n\n<img alt=\"bd0dc4d41e937cc4e4b8907330f58681.png\" src=\"https://img-blog.csdnimg.cn/img_convert/bd0dc4d41e937cc4e4b8907330f58681.png\" referrerpolicy=\"no-referrer\">\n\n\n\n相比之下，稀疏模型仅激活其总参数的一部分，与混合专家密切相关。\n\n为了说明这一点，我们可以将密集模型分割成片段（即专家），重新训练，并在给定时间仅激活一部分专家：\n\n<img alt=\"78bdc7a09c44e4149dc106f9c67ed290.png\" src=\"https://img-blog.csdnimg.cn/img_convert/78bdc7a09c44e4149dc106f9c67ed290.png\" referrerpolicy=\"no-referrer\">\n\n\n\n其基本思想是每个专家在训练过程中学习不同的信息。然后，在运行推理时，仅使用与特定任务最相关的专家。\n\n当收到问题时，我们可以选择最适合特定任务的专家：\n\n<img alt=\"16620f73d124bfa8af81bebc596fe8be.png\" src=\"https://img-blog.csdnimg.cn/img_convert/16620f73d124bfa8af81bebc596fe8be.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### 专家学习的内容\n\n如前所述，专家学习到的信息比整个领域的信息更加精细。因此，称它们为“专家”有时被视为误导。\n\n<img alt=\"81455202e36ad3ad2a9a220269d3aa46.png\" src=\"https://img-blog.csdnimg.cn/img_convert/81455202e36ad3ad2a9a220269d3aa46.png\" referrerpolicy=\"no-referrer\">\n\n\n\n（ST-MoE论文中编码器模型的专家专业化）\n\n然而，解码器模型中的专家似乎并没有同样类型的专业化。但这并不意味着所有专家都是平等的（具有相同的能力）。\n\nMixtral 8x7B论文(https://arxiv.org/pdf/2401.04088) 中有一个很好的例子，其中每个词元都标记了第一个专家的选择。\n\n<img alt=\"cd7ab4f998f152d194347fddbe260926.png\" src=\"https://img-blog.csdnimg.cn/img_convert/cd7ab4f998f152d194347fddbe260926.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这一图片也表明，专家往往专注于句法而非特定领域。\n\n因此，尽管解码器专家似乎没有专业化，但它们似乎在特定类型的词元上使用得相对一致。\n\n#### 专家的架构\n\n尽管将专家可视化为切成块的密集模型的隐藏层很不错，但它们通常是完整的FFNN：\n\n<img alt=\"36b3a8ec615af5e5185cc411214b7c57.png\" src=\"https://img-blog.csdnimg.cn/img_convert/36b3a8ec615af5e5185cc411214b7c57.png\" referrerpolicy=\"no-referrer\">\n\n\n\n由于大多数LLM有多个解码器块，给定的文本在生成之前会经过多个专家：\n\n<img alt=\"0b63c3f186fc9ae2abad81e6c0329dac.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0b63c3f186fc9ae2abad81e6c0329dac.png\" referrerpolicy=\"no-referrer\">\n\n\n\n选择的专家可能因词元而异，从而导致采取不同的“路径”：\n\n<img alt=\"3af5b42089995816937ef2cdc675f2f7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/3af5b42089995816937ef2cdc675f2f7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n如果我们更新解码器块的图解，它现在将包含更多的FFNN（每个专家各一个）：\n\n<img alt=\"0b1cb121797d79d5a403d3912a37a148.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/0b1cb121797d79d5a403d3912a37a148.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n解码器块现在有多个FFNN（每个都是一个“专家”），可以在推理过程中使用。\n\n**3**\n\n### 路由机制\n\n现在我们有了一组专家，那么模型如何知道使用哪些专家呢？\n\n我们可以在专家层之前添加一个路由（也称为门控网络），它是专门训练用来选择针对特定词元的专家。\n\n#### 路由\n\n路由（或门控网络）也是一个前馈神经网络（FFNN），用于根据特定输入选择专家。它可以输出概率，用于选择最匹配的专家：\n\n<img alt=\"3b55be9bfcc4906037e71ce671af2bae.png\" src=\"https://img-blog.csdnimg.cn/img_convert/3b55be9bfcc4906037e71ce671af2bae.png\" referrerpolicy=\"no-referrer\">\n\n\n\n\n 专家层返回所选专家的输出，乘以门控值（选择概率）。\n\n路由与专家（其中只有少数被选择）共同构成**MoE层**：\n\n<img alt=\"ed98c53ab4bea08b33773107e7f04cfc.png\" src=\"https://img-blog.csdnimg.cn/img_convert/ed98c53ab4bea08b33773107e7f04cfc.png\" referrerpolicy=\"no-referrer\">\n\n\n\n给定的MoE层有两种类型：稀疏混合专家或密集混合专家。\n\n两者都使用路由器来选择专家，但稀疏MoE仅选择少数专家，而密集MoE则选择所有专家，但可能在不同的分布中。\n\n<img alt=\"d5904b334261713c1bacd60fd55094ca.png\" src=\"https://img-blog.csdnimg.cn/img_convert/d5904b334261713c1bacd60fd55094ca.png\" referrerpolicy=\"no-referrer\">\n\n\n\n例如，给定一组词元，MoE会将词元分配到所有专家，而稀疏MoE仅选择少数专家。\n\n在当前的LLM状态下，当看到“MoE”时，通常指的是稀疏MoE，因为它允许使用一部分专家。这在计算上更为经济（消耗的资源更少），这是LLM的重要特性。\n\n#### 选择专家\n\n门控网络可以说是任何MoE中最重要的组件，因为它不仅决定推理期间选择哪些专家，还决定训练时的选择。\n\n在最基本的形式中，我们将输入（x）乘以路由权重矩阵（**W**）：\n\n<img alt=\"53bdd80a3dc5eff339c5bc2b116c08f4.png\" src=\"https://img-blog.csdnimg.cn/img_convert/53bdd80a3dc5eff339c5bc2b116c08f4.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然后，我们对输出应用**SoftMax**，创建每个专家的概率分布**G(x)**：\n\n<img alt=\"42f12f2ff365403534b5337366cac966.png\" src=\"https://img-blog.csdnimg.cn/img_convert/42f12f2ff365403534b5337366cac966.png\" referrerpolicy=\"no-referrer\">\n\n\n\n路由使用这个概率分布来选择最匹配的专家。\n\n最后，我们将每个路由的输出与每个选定的专家相乘，并将结果相加。\n\n<img alt=\"f8b5444bec8cf711d59a0e8eed3cd873.png\" src=\"https://img-blog.csdnimg.cn/img_convert/f8b5444bec8cf711d59a0e8eed3cd873.png\" referrerpolicy=\"no-referrer\">\n\n\n\n我们可以将所有内容结合在一起，探索输入如何通过路由和专家流动：\n\n<img alt=\"9e9724df942c417ee6bee295388abbba.png\" src=\"https://img-blog.csdnimg.cn/img_convert/9e9724df942c417ee6bee295388abbba.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### <img alt=\"4ad184d60cc584f8906518875d7b7052.png\" src=\"https://img-blog.csdnimg.cn/img_convert/4ad184d60cc584f8906518875d7b7052.png\" referrerpolicy=\"no-referrer\">\n\n\n \n\n#### 路由的复杂性\n\n然而，这个简单的函数通常导致路由器选择相同的专家，因为某些专家可能学习得比其他专家更快：\n\n<img alt=\"ebe53efa8b467d7230fa423413a94721.png\" src=\"https://img-blog.csdnimg.cn/img_convert/ebe53efa8b467d7230fa423413a94721.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这不仅会导致选择的专家分布不均，而且一些专家几乎无法受到训练。这在训练和推理期间都会产生问题。\n\n相反，我们希望在训练和推理期间让专家之间保持均等的重要性，这称为负载均衡。这样可以防止对同一专家的过度拟合。\n\n### 负载均衡\n\n为平衡专家的重要性，我们需要把关注点放在路由上，因为它是在特定时间决定选用哪些专家的关键组件。\n\nKeepTopK\n\n对路由进行负载均衡的一种方式是借助\"KeepTopK\"（https://arxiv.org/pdf/1701.06538）直接扩展。通过引入可训练的（高斯）噪声，可以避免重复选择相同的专家。\n\n<img alt=\"91bb801dac14d559b7b870e3adc5910a.png\" src=\"https://img-blog.csdnimg.cn/img_convert/91bb801dac14d559b7b870e3adc5910a.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然后，除了想要激活的前k个专家（例2）之外，其余专家的权重将被设置为-∞：\n\n<img alt=\"1c4ac765b549725d35efa137ce340ed1.png\" src=\"https://img-blog.csdnimg.cn/img_convert/1c4ac765b549725d35efa137ce340ed1.png\" referrerpolicy=\"no-referrer\">\n\n\n\n通过将这些权重设置为-∞，这些权重上的SoftMax输出所产生的概率将会是0：\n\n<img alt=\"73976f43eb26df3d97972ff3f4759c4f.png\" src=\"https://img-blog.csdnimg.cn/img_convert/73976f43eb26df3d97972ff3f4759c4f.png\" referrerpolicy=\"no-referrer\">\n\n\n\n尽管许多替代方案都很有前景，但许多语言模型仍然使用KeepTopK策略。请注意，KeepTopK也可以在不添加额外噪声的情况下使用。\n    词元选择    \nKeepTopK策略将每个词元路由到少数选定的专家。这种方法称为词元选择，它允许将给定的词元发送给一个专家（top-1路由）：\n\n<img alt=\"59eb4b4dbe38873e79e1d348047aacaa.png\" src=\"https://img-blog.csdnimg.cn/img_convert/59eb4b4dbe38873e79e1d348047aacaa.png\" referrerpolicy=\"no-referrer\">\n\n\n\n或者发送给多个专家（top-k路由）：\n\n<img alt=\"f239c79f916f298484e511b12bc2e516.png\" src=\"https://img-blog.csdnimg.cn/img_convert/f239c79f916f298484e511b12bc2e516.png\" referrerpolicy=\"no-referrer\">\n\n\n\n一个主要的好处是它允许权衡和整合专家各自的贡献。\n    辅助损失    \n为了在训练期间使专家的分布更加均匀，辅助损失（也称为负载均衡损失）被添加到网络的常规损失中。\n\n它增加了一个约束条件，迫使专家具有同等的重要性。\n\n这个辅助损失的第一个组成部分是对整个批次中每个专家的路由值进行求和：\n\n<img alt=\"70c3ac634838a2d24cf4ddc61c6d932a.png\" src=\"https://img-blog.csdnimg.cn/img_convert/70c3ac634838a2d24cf4ddc61c6d932a.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这为我们提供了每个专家的重要性得分，它代表了在任何输入下，给定专家被选中的可能性。\n\n我们可以用这个来计算变异系数（CV），它告诉我们专家之间的重要性得分有多大差异。\n\n<img alt=\"92d5852f54919f67050ea4160775651e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/92d5852f54919f67050ea4160775651e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n例如，如果重要性得分有很大差异，变异系数就会很高：\n\n<img alt=\"28945d977fb63b696ab38c04f246bff5.png\" src=\"https://img-blog.csdnimg.cn/img_convert/28945d977fb63b696ab38c04f246bff5.png\" referrerpolicy=\"no-referrer\">\n\n\n\n相反，如果所有专家的重要性得分相似，变异系数就会很低（这是我们的目标）：\n\n<img alt=\"8c8c8221b5b2109570178dfcf661a112.png\" src=\"https://img-blog.csdnimg.cn/img_convert/8c8c8221b5b2109570178dfcf661a112.png\" referrerpolicy=\"no-referrer\">\n\n\n\n利用这个变异系数得分，我们可以在训练期间更新辅助损失，使其目标是尽可能降低变异系数得分（从而给予每个专家同等的重要性）：\n\n<img alt=\"6588aa93d06706853836998ec261c85d.png\" src=\"https://img-blog.csdnimg.cn/img_convert/6588aa93d06706853836998ec261c85d.png\" referrerpolicy=\"no-referrer\">\n\n\n\n最后，辅助损失被单独添加进来，作为一个独立的损失项在训练期间进行优化。\n\n#### 专家容量\n\n不平衡现象不仅存在于被选中的专家中，还存在于发送给专家的词元分布中。\n\n例如，如果输入的词元在分配给不同专家时比例失调，过多地发送给一个专家而较少地发送给另一个专家，那么可能会出现训练不足的问题。\n\n<img alt=\"5cfed29cab0ecb19f4981c116e0965f6.png\" src=\"https://img-blog.csdnimg.cn/img_convert/5cfed29cab0ecb19f4981c116e0965f6.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这里，问题不仅仅在于使用了哪些专家，还在于对它们的使用程度。\n\n这个问题的一个解决方案是限制给定专家可以处理的词元数量，即专家容量。当一位专家达到其容量时，后续的词元将被发送给下一位专家：\n\n<img alt=\"3e3e75724d22a3a2ff05d36649046bbe.png\" src=\"https://img-blog.csdnimg.cn/img_convert/3e3e75724d22a3a2ff05d36649046bbe.png\" referrerpolicy=\"no-referrer\">\n\n\n\n如果两位专家都达到了他们的容量，那么该词元将不会被任何专家处理，而是被发送到下一层。这被称为词元溢出（token overflow）。\n\n<img alt=\"00b871c70c20c0d6449330b0f4456f34.png\" src=\"https://img-blog.csdnimg.cn/img_convert/00b871c70c20c0d6449330b0f4456f34.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### 借助Switch Transformer简化MoE\n\n首批解决了基于Transformer的MoE（例如负载均衡等）训练不稳定性问题的模型之一是Switch Transformer。它极大地简化了架构和训练过程，同时提高了训练的稳定性。\n    切换层    \nSwitch Transformer是一个T5模型（编码器-解码器），它用切换层取代了传统的前馈神经网络层。切换层是一个稀疏的MoE层，它为每个词元选择一个专家（Top-1路由）。\n\n<img alt=\"93f5a9972a20669974abdd8e32ebafd7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/93f5a9972a20669974abdd8e32ebafd7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n路由在计算选择哪个专家时没有特殊技巧，它只是对输入乘以专家权重后的结果取Softmax（与我们之前所做的相同）。\n\n<img alt=\"0d3a57f21baf3a4ea6dfca3ed9514a6e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0d3a57f21baf3a4ea6dfca3ed9514a6e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这种架构（Top-1 路由）假定路由只需要一个专家就能学会如何对输入进行路由。这与我们之前看到的情况形成对比，之前我们假设词元应该被路由到多个专家（Top-k 路由）以学习路由行为。\n    容量因子    \n容量因子是一个重要的值，因为它决定了一个专家能够处理多少个词元。Switch Transformer在此基础上进行了扩展，直接引入了一个容量因子，它对专家容量产生直接影响。\n\n<img alt=\"0ace8f6781c3d2b20f598875a3b3fd0e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0ace8f6781c3d2b20f598875a3b3fd0e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n专家容量的组成部分很直接：\n\n<img alt=\"323cd5c60b2ab176b4bf0c13a9e33432.png\" src=\"https://img-blog.csdnimg.cn/img_convert/323cd5c60b2ab176b4bf0c13a9e33432.png\" referrerpolicy=\"no-referrer\">\n\n\n\n如果我们增加容量因子，每个专家将能够处理更多的词元。\n\n<img alt=\"4a1cb59e0ffb11054d48922a803fc5a4.png\" src=\"https://img-blog.csdnimg.cn/img_convert/4a1cb59e0ffb11054d48922a803fc5a4.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然而，如果容量因子太大，我们会浪费计算资源。相反，如果容量因子太小，由于词元溢出，模型性能将会下降。\n    辅助损失    \n为了进一步防止丢弃词元，引入了一个简化版的辅助损失。\n\n这个简化后的损失并非去计算变异系数，而是依据每个专家的路由概率所占的比例，来对分配给各个专家的词元的比例进行权衡。\n\n<img alt=\"ab6bff0473c5e1292d01c79899119734.png\" src=\"https://img-blog.csdnimg.cn/img_convert/ab6bff0473c5e1292d01c79899119734.png\" referrerpolicy=\"no-referrer\">\n\n\n\n由于目标是在N个专家之间实现词元的均匀路由，我们希望向量P和f的值为 1/N。\n\nα是一个超参数，我们可以在训练期间使用它来微调这个损失的重要性。过高的值将主导主要的损失函数，而过低的值对负载均衡的作用很小。\n\n**4**\n\n### 视觉模型中的混合专家\n\n混合专家（MoE）技术并非仅适用于语言模型。视觉模型（例如视觉transformerViT）利用基于transformer的架构，因此也有使用混合专家的潜力。\n\nViT（视觉transformer）是一种将图像分割成小块（patch）的架构，这些小块的处理方式与词元类似。\n\n<img alt=\"7fcb67b6de9749779b871f24a48aa965.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/7fcb67b6de9749779b871f24a48aa965.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n这些小块（或词元）随后被映射到嵌入中（带有额外的位置嵌入），然后再输入到常规编码器中：\n\n<img alt=\"74962afb106b9d9626bd0ee842562ddc.png\" src=\"https://img-blog.csdnimg.cn/img_convert/74962afb106b9d9626bd0ee842562ddc.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这些小块一旦进入编码器，就会像词元一样被处理，这使得这种架构非常适合用于混合专家。\n\n#### 视觉混合专家（Vision-MoE）\n\n视觉混合专家（V-MoE）是在图像模型中最早实现混合专家的方法之一。它采用我们之前看到的视觉变换器（ViT），并将编码器中的密集前馈神经网络替换为稀疏混合专家（Sparse MoE）。\n\n<img alt=\"0fe801b55975c52bd2bfeefac3cb4a9d.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0fe801b55975c52bd2bfeefac3cb4a9d.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这使得通常比语言模型规模更小的ViT模型能够通过添加专家而大规模扩展。\n\n由于图像通常有很多小块，所以为每个专家使用了一个预先定义的小专家容量，以减少硬件限制。然而，低容量往往会导致小块被丢弃（类似于词元溢出）。\n\n<img alt=\"3fe37bb2bd6775f23e7c1fab11f78df8.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/3fe37bb2bd6775f23e7c1fab11f78df8.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n为了保持低容量，网络为小块分配重要性得分，并首先处理那些重要的小块，这样溢出的小块通常就不太重要了。这被称为批量优先级路由。\n\n<img alt=\"9b51109b0da0b4cc319c7f0e92cd4a0d.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/9b51109b0da0b4cc319c7f0e92cd4a0d.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n因此，如果词元的百分比降低，我们仍然应该看到重要的小块被路由。\n\n<img alt=\"0e7d74b9321d3fa438753fa094fb35c6.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/0e7d74b9321d3fa438753fa094fb35c6.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n优先级路由通过专注于最重要的小块，使得较少的小块被处理。\n\n#### 从稀疏混合专家到软混合专家\n\n在视觉混合专家（V-MoE）中，优先级评分器有助于区分更重要和不太重要的小块。然而，小块被分配给每个专家，未处理小块中的信息会丢失。\n\n软混合专家（Soft-MoE）旨在通过混合小块将离散的小块（词元）分配转变为软小块（词元）。\n\n在第一步中，我们将输入x（小块嵌入）与一个可学习的矩阵Φ相乘。这为我们提供了路由信息，告诉我们某个词元与给定专家的相关程度。\n\n<img alt=\"533430165adcfcbb7fde1be7670e9aa5.png\" src=\"https://img-blog.csdnimg.cn/img_convert/533430165adcfcbb7fde1be7670e9aa5.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然后，通过对路由信息矩阵（在列上）取Softmax，我们更新每个小块的嵌入。\n\n<img alt=\"38c6f85b90e7a9770eb52e1a0ccbd9c7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/38c6f85b90e7a9770eb52e1a0ccbd9c7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n更新后的小块嵌入本质上是所有小块嵌入的加权平均值。\n\n<img alt=\"9960418f70a86276561a86f592a70c95.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/9960418f70a86276561a86f592a70c95.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n从视觉上看，就好像所有小块都被混合了。然后，这些组合后的小块被发送给每个专家。在生成输出后，它们再次与路由矩阵相乘。\n\n<img alt=\"615dd5c8de9267b10b4328e8b584e863.png\" src=\"https://img-blog.csdnimg.cn/img_convert/615dd5c8de9267b10b4328e8b584e863.png\" referrerpolicy=\"no-referrer\">\n\n\n\n路由矩阵在词元级别影响输入，在专家级别影响输出。\n\n结果，我们得到了被处理的“软”小块/词元，而不是离散的输入。\n\n#### Mixtral 8x7B的活跃参数与稀疏参数\n\n混合专家之所以有趣的很大一部分原因在于其计算需求。由于在给定时间只使用一部分专家，所以我们可以访问比实际使用更多的参数。\n\n虽然给定的混合专家模型有更多的参数要加载（稀疏参数），但由于在推理期间我们只使用一些专家，所以激活的参数较少（活跃参数）。\n\n<img alt=\"037624952b665e1bbf44e1a654d4ea95.png\" src=\"https://img-blog.csdnimg.cn/img_convert/037624952b665e1bbf44e1a654d4ea95.png\" referrerpolicy=\"no-referrer\">\n\n\n\n换句话说，我们仍然需要将整个模型（包括所有专家）加载到你的设备上（稀疏参数），但当我们进行推理时，我们只需要使用一部分（活跃参数）。混合专家模型需要更多的显存（VRAM）来加载所有专家，但在推理期间运行得更快。\n\n让我们以Mixtral 8x7B来探讨稀疏参数与活跃参数的数量。\n\n<img alt=\"b855a0b527927d70272a5bc795805a59.png\" src=\"https://img-blog.csdnimg.cn/img_convert/b855a0b527927d70272a5bc795805a59.png\" referrerpolicy=\"no-referrer\">\n\n\n\n在这里，我们可以看到每个专家的大小是5.6B，而不是7B（尽管有8个专家）。\n\n<img alt=\"f04a3085eef0ec98a0aacfc8f68c9fc7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/f04a3085eef0ec98a0aacfc8f68c9fc7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n我们将不得不加载8×5.6B（46.7B）的参数（以及所有共享参数），但在推理时我们只需要使用2×5.6B（12.8B）的参数。\n\n**5**\n\n### 总结\n\n至此，我们对混合专家的探索之旅就结束了。希望这篇文章能让你更好地理解这一有趣技术的潜力。如今，几乎所有的模型系列中都至少包含一种混合专家的变体，它将会持续存在下去。\n\n其他人都在看\n    [推算LLM训练的GPU内存需求]()        [用初中数学理解LLM工作原理]()        [企业AI调查:AI支出激增6倍，多模型部署盛行]()        [10倍工程师编码工具：Cursor x SiliconCloud]()        [强化学习之父Rich Sutton：AGI研究下一个范式]()        [大模型训练秘方:1000次超参数优化实验的发现]()        [1%预训练成本，实现最高20倍算力扩展效果]()    \n**让超级产品开发者实现“Token自由”**\n \n\n**<em>**[邀请好友体验SiliconCloud]()，****</em>**<em>**狂送2000万Token/人****</em>\n\n****邀请越多，Token奖励越多****\n****[siliconflow.cn/zh-cn/siliconcloud](http://siliconflow.cn/zh-cn/siliconcloud)****\n",
    "labels": [],
    "created_at": "2024-12-20T16:40:27Z"
  },
  {
    "id": 6,
    "title": "释放数字金融向新力奇富科技金融AI大模型成果落地上海金融科技国际论坛特展周活动",
    "url": "https://blog.csdn.net/csdnnews/article/details/144600708",
    "content": "\n12 月 19 日，在第六届上海金融科技国际论坛之“2024年度人工智能大模型金融领域示范场景及创新应用案例成果特展”活动周上，由上海金融科技产业联盟创新监管联合实验室指导，奇富科技主办的“攻坚金融科技创新与大模型应用、释放数字金融向新力” 主题成果展在沪举办，全面展示了奇富科技在金融科技领域的创新实践。同时，上海金融科技产业联盟创新监管实验室与奇富科技现场达成创新伙伴合作，双方将共同探索金融科技创新的路径，推动新技术在金融领域高效、安全的应用，助力上海全球金融科技中心建设。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/c0eba35f052d4581b55bfc49e6e27826.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n奇富科技CEO吴海生在特展现场强调，科技是驱动金融创新的关键，能打破传统边界，以智能化、个性化方案满足多元化金融需求。他表示，奇富科技将深化研发、加强与金融机构合作，紧抓这一千载难逢的变革机遇，积极探索金融科技无限可能。金融行业天然就适合大模型落地，奇富科技将勇于尝试，利用大模型无穷尽的潜力，为金融行业带来实实在在的提效。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/77df226e98da4f4cb3cd6fa922cb0019.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n 上海金融科技产业联盟副秘书长雎晓燕介绍，上海金融科技产业联盟自2020年成立以来，始终以“开放交流、前沿展示、合作共赢、生态创新”为宗旨，积极搭建具有国际视野的技术合作与产业促进平台，加快推进金融科技产业协同与生态建设，为上海打造全球金融科技中心提供必要支撑。未来，希望奇富科技积极参与到联盟的创新生态建设中来，抢抓数字金融发展的大机遇，为金融科技中心建设凝聚新力量、贡献新动能。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/3c77a3fc6d3b4813a3ce594bb00194bf.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n奇富科技技术委员会主席宋荣鑫阐述了公司从安全根基出发，技术深入落地场景，成为提升金融科技服务品质核心动能的技术理念。在模型层面，通过构建企业级知识库和AI Flow，AI STUDIO使得智能体开发最快可一天上线，实现矩阵式覆盖创新、智能体重塑业务流程。其中，金融专家大模型JARVIS AI生成代码占比高达30%，AI生成测试案例40%；奇富ChatBI通过NL2SQL、语义建模等能力构建金融科技BI新范式。奇富科技超百亿投入沉淀的技术能力，通过科技无损冷链模式，与合作银行实现小时级端到端能力传输，在包含风控、获客、信贷业务、增长等平台能力的FocusPRO 数字普惠信贷系统加持下共建业务生态。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/31a8ddcfc4f949cd9452397b6d6deb81.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n奇富科技首席算法科学家费浩峻深入剖析了金融大模型产品矩阵落地的三要素：一是深耕场景，以坐席提效助手奇富Copilot为例，通过精准定位业务场景，深度分析上亿通历史通话记录，提炼出高效沟通策略与话术模板，助力坐席更精准迅速地捕捉用户意图；二是数据飞轮，以大模型在小微金融中构建的关系图谱为例，通过持续迭代的数据反馈机制，确保模型性能形成良性循环；三是多智能体协作，奇富营销智能体目前可调用多个智能体协作，实现了营销从任务优化升级到自主智能解决问题。三要素相辅相成，共同构成了奇富科技在大模型领域的核心竞争力。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/baf0097b003045daa544e2e7d8691928.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n 活动中，奇富科技发布AI产品小奇，这一基于大模型技术的 AI 伴侣可全天候服务，深度理解并预测用户的金融需求。小奇基于奇富科技8年累计的海量金融交易数据、行为日志及客服对话语料进行训练。据奇富科技产品副总裁卢瑶介绍，小奇具备全程智能无人工、多轮交互及语控功能，可提升用户额度申请与营销效率，降低进线率。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/8474248c7c5d415ab6ab3d411b52cd24.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n作为奇富科技技术输出的核心平台，奇富数科在此次特展上正式发布了数字普惠信贷解决方案FocusPRO 2.0，旨在破解小微信贷领域的风险、成本与规模难题。奇富数科总裁周旭强表示，数字化、智能化技术重构成本结构并管控风险是打破困境的关键。FocusPRO 2.0不仅优化了消费贷服务，还拓展至小微经营贷，满足了市场更多元、更精细的金融需求。今年1月至9月，在FocusPRO的助力下，奇富数科放款月均复合增长率达到14%，业务增长势头强劲。\n\n中国社科院国家金融与发展实验室副主任杨涛表示，央行等七部门联合印发《推动数字金融高质量发展行动方案》，既提到“强化数字技术支撑能力”以助力金融业数字化转型，也突出要“运用数字技术提升重点领域金融服务质效”。我们看到，伴随新技术的快速演进与迭代，金融业作为特殊的信息处理行业，其核心要素、基本功能、运行模式、风险管理等都受到巨大冲击与影响。在保障合规与安全的前提下，更广泛地把前沿技术应用于金融服务创新，已成为金融高质量服务实体经济的重要“抓手”。尤其是生成式AI大模型的金融应用逐渐深入，虽然还存在许多挑战，但可以预见将对提升金融业的自动化和智能化水平，以及决策、管理和风控效率都带来积极效应。\n\n中关村互联网金融研究院首席研究员董希淼认为：日前，中国人民银行等七部门联合印发《推动数字金融高质量发展行动方案》，提出数字金融发展总体要求和具体措施，为数字金融发展明确了方向、目标，将加快金融机构数字化转型，促进数字金融健康发展。金融机构、科技公司如果能够深度运用以大模型所代表的生成式人工智能（GAI），大力推进数字化转型，持续创新产品服务，不断提升用户体验，就可能走出一条差异化、特色化的发展之路。从这个意义上讲，大模型不但将成为金融业的数字化劳动力，还将促进金融机构数字化转型，推动数字金融深度发展。\n\n奇富科技表示，AI大模型在金融行业中的价值已经从降本增效逐步升级到高效提升业务核心收益，这将带动全行业颠覆式产品服务模式创新，逐步实现为用户提供无缝嵌入生活的高度个性化金融服务。今年9月，上海市发布《上海高质量推进全球金融科技中心建设行动方案》，作为设立在上海的金融科技企业，奇富科技将持续加大在AI大模型技术上的研发与投入，利用其在数据处理、自然语言理解、决策优化等方面的强大能力，深入挖掘AI技术在金融业务的潜在价值，推动国产金融大模型技术迭代升级，为上海金融科技发展贡献力量，助力中国数字金融的高质量发展。\n",
    "labels": [],
    "created_at": "2024-12-20T16:40:25Z"
  },
  {
    "id": 7,
    "title": "AI半导体技术、市场与未来",
    "url": "https://blog.csdn.net/OneFlow_Official/article/details/144279431",
    "content": "\n<img alt=\"0adce903ec321617a1e752ee1d67a11f.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0adce903ec321617a1e752ee1d67a11f.png\" referrerpolicy=\"no-referrer\">\n\n\n\n过去两年，英伟达崛起是科技领域的一个经典案例。通过CUDA系统，他们创建了一个使用GPU进行机器学习的开发者生态系统；通过Mellanox，他们成为了数据中心网络的领导者。然后，他们将所有硬件集成到服务器中，提供垂直集成的算力一体机。\n\n 凭借这一系列组合性技术优势，英伟达在“AI淘金热”中提供的铲子占据行业核心地位，这导致它成为有史以来最成功的公司之一。随之而来的是，不少挑战者入局以求从英伟达主导的市场分一杯羹。半导体行业的竞争愈加热烈。\n\n 在此背景下，AI半导体研究专家Austin Lyons与Eric Flaningam从AI与GPU行业的背景知识切入，结合当前AI半导体生态系统，通过行业关键数据，对未来发展趋势进行了深入分析。\n\n（本文由OneFlow编译发布，转载请联系授权。原文：https://www.generativevalue.com/p/the-ai-semiconductor-landscape）\n\n**来源 |****Eric Flaningam、****Austin Lyons**\n\n**翻译｜<strong>张雪聃、林心宇**</strong>\n\n**OneFlow编译**\n\n**题图由****[SiliconCloud]()平台生成**\n\n**1**\n\n### AI加速器的背景知识\n\n从一个非常宏观的角度看，所有逻辑半导体都包含以下组成部分：\n\n1. 计算核心——执行实际的计算操作。\n\n2. 存储器——存储要传递给计算核心的数据。\n\n3. 缓存——临时存储可快速检索的数据。\n\n4. 控制单元——控制并管理其他组件的操作顺序。\n\n传统情况下，CPU是一种通用计算机，设计用于执行任何计算任务，包括复杂的多步流程。如下图所示，CPU具有更多的缓存、更强大的控制单元以及更小的计算核心（即CPU中的算术逻辑单元，ALU）。\n\n<img alt=\"fa75ae40cda07669b24796dfe1d94b38.png\" src=\"https://img-blog.csdnimg.cn/img_convert/fa75ae40cda07669b24796dfe1d94b38.png\" referrerpolicy=\"no-referrer\">\n\n\n\n另一方面，GPU专为处理大量小型计算任务或并行计算而设计。最初，GPU用于图形处理，需要同时进行大量的小型计算以生成显示内容。这种基础架构非常适合AI的工作负载。英伟达率先通过早期的GPU引入可编程着色器，并推出CUDA，使所有GPU都能成为可编程计算机。\n\n**为何GPU如此适合AI？**\n\n大多数AI模型的基本单元是神经网络，其结构由多层节点组成。这些节点通过加权处理，尽可能准确地表示训练数据的特性。\n\n当模型完成训练后，可以输入新的数据，模型则会预测输出结果（即推理）。\n\n这种“数据传递”涉及大量的小型计算，主要以矩阵乘法的形式实现：（某一层的节点与权重）×（另一层的节点与权重）。\n\n矩阵乘法是GPU的强项，因其具备出色的并行处理能力。\n\n<img alt=\"53ffab4623ec1f02a3bb9251a5213351.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/53ffab4623ec1f02a3bb9251a5213351.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n（Stephen Wolfram撰写了一篇详细解析ChatGPT工作原理的文章：https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/）\n\n#### 当今的GPU\n\nGPU的规模不断扩大，计算能力和内存也在持续增强，并且更加专注于适配矩阵乘法等工作负载。\n\n以英伟达的H100为例。它由CUDA和Tensor核心（基本处理器）、处理集群（由多个核心组成）以及高带宽内存组成。H100的设计目标是以尽可能高的数据流量处理尽可能多的计算。\n\n<img alt=\"71dd223c116502541895cde1866f9711.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/71dd223c116502541895cde1866f9711.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n提升性能的目标不仅限于芯片本身，而是聚焦于整个系统的优化。在芯片之外，通过连接GPU构建计算集群（https://www.nvidia.com/en-us/data-center/dgx-superpod/），将服务器设计为一体化的计算设备(https://www.nvidia.com/en-gb/data-center/dgx-h100/)，甚至从系统层面优化数据中心的设计(https://www.fabricatedknowledge.com/p/the-data-center-is-the-new-compute)。\n \n\n**1**\n\n### 训练与推理的背景知识\n\n要理解AI半导体的格局，需要先回顾一下AI架构的基础知识。\n\n训练指的是通过大规模数据集进行迭代，创建能够表示复杂场景的模型；而推理则是向该模型提供新的数据以生成预测。\n\n<img alt=\"f3964799113fd269107fcf1c47bd0182.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/f3964799113fd269107fcf1c47bd0182.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n以下是推理的几个关键特性：\n\n1. **时延和位置至关重要**——推理为终端用户运行工作负载，因此响应速度至关重要。这使得在边缘设备或边缘云环境中进行推理更具优势，而训练则可以在任何位置完成。\n\n**2. 可靠性稍显次要**——训练最前沿的模型可能需要数月时间，并需要庞大的训练集群。由于集群内部高度依赖，一个环节出错可能拖慢整个训练进程。而推理的工作负载更小且相互独立，即使发生错误，仅会影响单个请求，并能快速重新运行。\n\n**3. 硬件扩展性的重要性较低**——英伟达的核心优势之一是其通过软件和网络实现大规模系统扩展的能力。然而在推理中，这种扩展性的重要性较低。\n\n以上原因解释了为什么许多新兴半导体公司将重心放在推理上。与训练相比，推理的进入门槛更低。\n\n英伟达的网络和软件能够支持其构建更大、更高性能和更可靠的训练集群。英伟达在AI训练方面的竞争壁垒非常牢固，其他竞争者难以企及。\n\n接下来，我们来谈谈竞争格局。\n\n**3**\n\n### AI半导体行业概览\n\nAI半导体行业可以大致分为三个主要领域：\n    数据中心训练芯片        数据中心推理芯片        边缘推理芯片    \n下图是相关公司展示：\n\n<img alt=\"cafbd2b843418757819e4df5190e033e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/cafbd2b843418757819e4df5190e033e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n（AI半导体价值链的思维模型图；该图并未展示所有公司和细分领域）\n\n#### 数据中心半导体市场\n\n简单来说，英伟达主导了数据中心半导体市场，而AMD是唯一具有竞争力的通用替代方案。超大规模云公司（Hyperscalers）倾向于开发自研芯片，而大部分初创公司专注于推理或特定架构的专用硬件。\n\n预计英伟达将在2024年销售超过1000亿美元的AI系统，而AMD紧随其后，预计实现50亿美元的收入。\n\n以下是截至2023年底，数据中心处理器市场份额的分布：\n\n<img alt=\"d550ff283d396d39793b3ebe1ba55512.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/d550ff283d396d39793b3ebe1ba55512.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n（数据来源：https://www.crn.com/news/components-peripherals/2024/google-was-third-biggest-data-center-processor-supplier-last-year-research）\n\n在超大规模云公司中，Google提供了最先进的加速器，其TPU备受关注。据TechInsights估计，Google去年出货了200万个TPU，仅次于英伟达的AI加速器。\n\n亚马逊则开发了自研神经网络芯片（Nitro）、CPU（Graviton）、推理芯片（Inferentia）和训练芯片（Trainium）。TechInsights估计，亚马逊在2023年向客户“出租”了230万个此类芯片。\n\n微软最近推出了CPU（Cobalt）和GPU（Maia），但由于产品较新，目前尚无法评估其市场表现。\n\n最后值得一提的是，英特尔原本预计今年销售约5亿美元的Gaudi 3芯片，但在最近的财报中表示这一目标无法实现。\n\n尽管英伟达凭借其软件和网络功能在训练领域占据主导地位，但推理领域由于架构差异，竞争格局更加多样化。\n\n#### 推理领域：更具吸引力的市场！\n\nRunPod最近对Nvidia H100和AMD MI300X进行了有趣的对比研究，结果表明，MI300X在超大批处理和极小批处理的推理任务中更具成本优势。\n\n<img alt=\"e789403f7761f24d403432d0b312efdf.png\" src=\"https://img-blog.csdnimg.cn/img_convert/e789403f7761f24d403432d0b312efdf.png\" referrerpolicy=\"no-referrer\">\n\n\n\n（数据来源：https://blog.runpod.io/amd-mi300x-vs-nvidia-h100-sxm-performance-comparison-on-mixtral-8x7b-inference/）\n\n同时，许多硬件初创公司筹集了大量资金，试图抢占这一市场的一席之地：\n\n<img alt=\"6afc719c09ca3040f35e886f3f9db21d.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/6afc719c09ca3040f35e886f3f9db21d.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n（数据来源：https://www.chipstrat.com/p/etched-silicon-valleys-speedrun）\n\n一个有趣的趋势是，这些初创公司正向软件领域扩展。例如，[**Groq**]()、[**Cerebras**]()和SambaNova三家领先的初创公司都在提供推理软件服务。理论上，这种垂直整合能够为终端用户提供成本和性能上的双重优势。\n\nAI半导体市场的最后一块拼图是边缘AI，这也是业内广泛关注的热点话题。\n\n### **4**\n 边缘AI？\n\n训练最庞大且功能最强的AI模型需要高昂的成本，甚至可能需要整个数据中心的GPU资源支持。然而，模型训练完成后，可以运行在性能相对较低的硬件上。实际上，AI模型甚至可以在智能手机或笔记本电脑等“边缘”设备上运行。边缘设备通常由SoC（系统级芯片）提供支持，这种芯片包含CPU、GPU、内存，通常还集成了NPU（神经网络处理单元）。\n\n<img alt=\"db71451b5af5eba87dd9219e32c98c41.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/db71451b5af5eba87dd9219e32c98c41.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n例如，一个运行在智能手机上的AI模型必须足够小，以适应手机内存的限制。因此，与大型云端模型相比，边缘模型通常更小、更简单。然而，模型在本地运行能够安全地访问用户特定数据（如位置、短信），而无需将数据传输到云端。\n\n尽管AI模型可以技术上运行在手机的CPU上，但矩阵乘法更适合由GPU这样支持并行处理的硬件来完成。由于CPU优化用于顺序处理，在推理任务中可能表现较慢，即使是处理小型模型也是如此。作为一种替代方案，AI模型可以运行在手机的GPU上。\n\n然而，智能手机的GPU主要为图形处理而设计。在玩游戏时，用户期望GPU能全力支持平滑的画面效果。如果同时分配GPU资源给AI模型，可能会显著影响游戏体验。\n\n因此，NPU成为边缘设备AI推理的理想选择。智能手机可以利用NPU来运行AI任务，而不会增加GPU或CPU的负担。由于电池寿命对边缘设备至关重要，NPU经过优化以降低功耗。与GPU相比，NPU在执行AI任务时的功耗可能低至其1/5到1/10，大大延长了设备的电池续航时间。\n\n边缘推理不仅应用于消费类设备，还广泛用于工业和汽车领域。例如，自动驾驶汽车依靠边缘推理实时处理传感器数据，从而做出快速决策以确保行车安全。而在工业物联网中，传感器和摄像头数据的本地推理可以支持主动维护等措施。\n\n工业和汽车领域的设备通常具备比消费类设备更高的功率支持，因此可以部署高性能计算平台。例如，Nvidia的Orin平台就内置了与数据中心GPU类似的功能（详见https://www.nvidia.com/en-us/edge-computing/products/igx/）。需要远程硬件可编程性的场景，则可以使用例如Altera的FPGA（详见：https://www.intel.com/content/www/us/en/content-details/765466/altera-fpgas-and-socs-with-fpga-ai-suite-and-openvino-toolkit-drive-embedded-edge-ai-machine-learning-applications-white-paper.html)。\n\n**5**\n\n### 对市场的一些思考\n\n最后，我想分享几个我认为在AI半导体领域中最有趣的问题：\n\n**1. 英伟达的优势壁垒有多深厚？**\n\n多年来，英伟达在数据中心GPU市场上维持了90%以上的市场份额。作为一家极具前瞻性的公司，其在过去二十年内做出了正确的技术和战略决策。在这个领域中，经常有人问我有关英伟达的壁垒的问题。\n\n对此，我有两个看似矛盾的观点：首先，我认为英伟达在不断扩展自己的优势，从AI软件、基础设施到模型和云服务都在积极布局。他们还在投资网络和垂直整合。这家公司持续高效执行其战略，令人惊叹。\n\n其次，在收入上，每家公司都想与英伟达分一杯羹。如今，全球企业正投入数千亿美元争相开发通用人工智能（AGI）。英伟达最大的客户们正在斥资数十亿美元减少对其依赖，同时投资者也向竞争对手注入巨额资金，希望能瓜分英伟达的市场份额。\n\n总结来说：英伟达是目前全球AI领域最有优势的公司，同时也面临来自竞争者、客户和投资者的数百亿美元投入的挑战。\n\n**2. 初创公司的机会在哪里？**\n\n半导体初创公司面临着极其艰难的挑战，要建立可持续的商业模式难度很大。而当竞争对手是英伟达时，这种挑战更显艰巨。不过，市场总是在通用性与专业性之间进行权衡。如果公司能够有效地专注于足够大的细分市场，就有机会成长为一家规模庞大的企业。这包括专为推理设计的硬件和特定模型优化的硬件。\n\n我尤其对能够加速专用芯片开发的方案感兴趣。这类方法能够降低芯片开发的门槛，同时利用专业化带来性能优势。\n\n然而，半导体行业极其复杂，这类产品需要经过时间和多代技术的积累才能成熟。这些公司需要持续的资金支持，才能完成多个产品代际的研发。\n\n**3. 边缘AI会成为现实吗？**\n\n从历史来看，技术的颠覆往往发生在一种新产品以更低价格提供了较少的功能时，而现有企业难以与之竞争。例如，大型主机让位于小型计算机，小型计算机又被PC取代，PC之后是智能手机。\n\n这些颠覆的关键变量在于性能的“过剩供应”。顶级解决方案常常解决了大多数人非必需的问题。许多计算领域的颠覆来自于计算能力的去中心化，因为消费者并不需要额外的性能。\n\n在AI领域，我认为这种“性能过剩”还没有出现。ChatGPT很强大，但还称不上完美。一旦它变得足够优秀，边缘AI的时代才会到来。小型语言模型和NPU将推动这一时代的到来。届时，问题将不再是“AI是否会进入边缘设备”，而是“什么时候进入”。\n\n其他人都在看\n    [生成式AI推理技术、市场与未来]()        [比GPU快20倍？d-Matrix推理性价比分析]()        [Tenstorrent虫洞分析：挑战英伟达的新玩家]()        [Cerebras：全球最快AI推理芯片的“魔法”]()        [Groq：从头设计一个张量流式处理器架构]()        [LLM逻辑推演策略：推理时计算vs训练时计算]()        [1%预训练成本，实现最高20倍算力扩展效果]()    \n**让超级产品开发者实现“Token自由”**\n \n\n**<em>**[邀好友用SiliconCloud]()，****</em>**<em>**狂送2000万Token/人****</em>\n\n****[<em><strong>即刻体验****QwQ-32B-Preview]()</strong></em>\n****[siliconflow.cn/zh-cn/siliconcloud](http://siliconflow.cn/zh-cn/siliconcloud)****\n",
    "labels": [],
    "created_at": "2024-12-20T16:40:23Z"
  },
  {
    "id": 3,
    "title": "拦截烂SQL，解读GaussDB(DWS)查询过滤器过滤规则原理",
    "url": "https://www.cnblogs.com/huaweiyun/p/18619491",
    "content": "\n本文分享自华为云社区[《GaussDB(DWS)查询过滤器过滤规则原理与使用介绍》](https://bbs.huaweicloud.com/blogs/441767?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)，作者： 清道夫。\n\n# 1. 前言\n\n**适用版本：【9.1.0.100（及以上）】**\n\n查询过滤器在9.1.0.100之前就具备提供查询过滤功能的能力，但仅支持自动隔离反复查询被终止的查询，防止烂SQL再次执行。\n\n老版本主要面向异常熔断机制和紧急拦截场景，前者可以与异常规则联动，自动将触发异常规则的语句添加到黑名单中，后者是需要手动找到core或者引发hang的语句进行屏蔽。\n\n\n\n大家有兴趣可以翻一下之前的这篇文章[GaussDB(DWS)查询过滤器原理与应用](https://bbs.huaweicloud.com/blogs/401188)。\n\n9.1.0.100及9.1.0.200版本对查询过滤器做了功能的改进，可以通过多维度进行烂SQL识别，功能更丰富，配置更灵活。\n\n# 2. 原理介绍\n\n在原理介绍之前，先举个简单的例子。在业务开发过程中，要想禁止对2张以上的表进行关联查询，此时可以使用DDL语句创建过滤规则：\n\n```\nCREATE BLOCK RULE forbid_2_t_sel FOR SELECT FILTER BY  SQL('test_block_rule') with(table_num='2');\n```\n\ntable_num指的是一个语句中出现的表的个数，此时所有查询语句不能包含有两张表以上的查询。\n\n```\n--两张表直接关联查询，可以正常执行\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\n c1 | c2 | c1 | c2\n----+----+----+----\n(0 rows)\n\n--三张表直接关联查询，被拦截\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2 join test_block_rule3 t3 on t2.c1=t3.c1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 2(3))\n```\n\n说到这，整体逻辑就非常清楚了。用户可以提前识别烂SQL的特征，然后抽象出来，用DDL语句创建规则，后续会对查询的语句进行过滤，被规则筛选出来的便是烂SQL，执行前会报错，反之则可以正常执行。\n\n查询过滤器框架及功能原理概况：\n\n<img src=\"https://img2024.cnblogs.com/blog/2030258/202412/2030258-20241220160540342-411141587.png\" alt=\"\" referrerpolicy=\"no-referrer\">\n\n\n\n从图中可以看出，之前的查询过滤器的功能依然存在，可以保证与异常规则的联动，新版本的增强更注重规则的灵活性和功能的丰富性。\n\n# 3. 使用介绍\n\n## 3.1 查询过滤规则元数据管理\n\n查询过滤规则，可以通过DDL进行新增、删除或者修改，其语法如下：\n\n### （1）创建\n\n```\nCREATE BLOCK RULE [ IF NOT EXISTS ] block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE ] |\n    FILTER BY\n    { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) }\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n其中，\n\n   \n**block_name：**过滤规则的名称\n   \n   \n**user_name：**规则应用的对象用户\n   \n   \n**host：**是规则应用的客户端IP\n   \n   \n**FOR：**语句类型，支持对UPDATE/SELECT/INSERT/DELETE/MEGE INTO五种类型语句进行限制\n   \n   \n**FILTER BY：**过滤方法，包含两种形式\n   \n   \n**SQL：**根据关键词对语句进行正则匹配，例如表名，其长度不能超过1024，建议尽量精简\n   \n   \n**TEMPLATE：**\n   \n   \n**unique_sql_id：**归一化的64位哈希值，重复概率较sql_hash大一些\n   \n   \n**sql_hash：**归一化的哈希值（md5），一般不会重复，相较unique_sql_id更推荐使用\n   \n   \n**with_parameter：**查询过滤规则选项参数，可以附加多个条件，满足其一便会匹配过滤。\n   \n   \n**application_name：**客户端名称\n   \n   \n**query_band：**负载标识\n   \n   \n**table_num：**包含的基表个数\n   \n   \n**partition_num：**扫描分区的数量\n   \n   \n**estimate_row：**基表输出行数\n   \n   \n**resource_pool：**切换的目标资源池，仅适用于9.1.0.200及以上\n   \n   \n**max_active_num：**可并发执行的语句个数，仅适用于9.1.0.200及以上\n   \n   \n**is_warning：**改变拦截行为为告警，而非默认的报错，仅适用于9.1.0.200及以上\n   \n\n其中，user_name和FILTER BY是必选项，其他可以通过业务实际需要进行配置。\n\n### （2）修改\n\n```\nALTER BLOCK RULE block_name RENAME to new_block_name;\n```\n\n通过rename对查询过滤的规则进行重命名。\n\n```\nALTER BLOCK RULE block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] | [ TO DEFAULT ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE | DEFAULT ] |\n    [ [ FILTER BY ]\n    [ { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) } ] ]\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n所有选项均支持二次修改，如果需要去除部分字段的限制，可以指定default关键词，例如：\n\n```\n--修改为只能查询1张表\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num='1');\nALTER BLOCK RULE\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 1(2))\npostgres=# select * from test_block_rule1 t1;\n c1 | c2\n----+----\n(0 rows)\n\n--去除查询中表个数的限制\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num=default);\nALTER BLOCK RULE\n--再次查询报错拦截\npostgres=# select * from test_block_rule1 t1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule)\n```\n\n### （3）删除\n\n```\nDROP BLOCK RULE [ IF EXISTS ] block_name;\n```\n\n## 3.2 权限问题\n\n对于普通用户来讲是没有创建查询过滤规则权限的，需要管理员或者管理员将权限赋给某一普通用户才可以。\n\n```\n--切换至普通用户\npostgres=# set role jack password 'xxx';\nSET\n--创建查询过滤规则报错提示无权限\npostgres=> create block rule bl2 filter by sql('test');\nERROR:  CREATE/ALTER/DROP BLOCK RULE permission denied for current user\n--重置user\npostgres=> reset role;\nRESET\n--对普通用户进行授权\npostgres=# grant gs_role_block to jack;\nGRANT ROLE\n--切换普通用户\npostgres=# set role jack password 'xxx';\nSET\n--再次创建成功\npostgres=> create block rule bl2 filter by sql('test');\nCREATE BLOCK RULE\n```\n\n建议创建查询过滤规则时尽量缩小适用范围，避免误过滤，或者范围过大导致性能劣化。\n\n## 3.3 备份恢复\n\n对于查询过滤规则的备份或者恢复的权限与操作元数据的权限一致，需要管理员或者管理员讲权限赋值给某一普通用户才可以，用户可以通过gs_dump导出查询过滤规则定义。\n\n如果想查看或者导入查询过滤规则的定义，可以通过pg_get_blockruledef进行查询。\n\n```\npostgres=# select * from pg_get_blockruledef('test');\n                         pg_get_blockruledef\n----------------------------------------------------------------------\n CREATE BLOCK RULE test FILTER BY SQL('test') WITH(estimate_row='3');\n(1 row)\n```\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n## 3.4 使用举例\n\n### （1）使用关键词进行查询过滤\n\n```\nCREATE BLOCK RULE bl1\nTo block_user\nFOR SELECT\nFILTER BY SQL ('tt')\nWITH(partition_num='2',\n     table_num='1',\n     estimate_row='5'\n     );\n\npostgres=> select * from tt;\nERROR:  hit block rule bl1(user_name: block_user, block_type: SELECT, regexp_sql: tt, partition_num: 2(3), table_num: 1(1), estimate_row: 5(1))\n```\n\n从上面的查询可以看出，查询语句包含了tt关键字，并且扫描的分区个数超过了2，此时执行语句被过滤拦截。需要注意的是，**扫描分区的个数并不总是准确的**，仅能识别**静态**的分区剪枝个数，执行过程中的**动态剪枝**并不能被识别。\n\n\n\n**小技巧：**使用关键词过滤时可以先使用正则匹配符~*进行测试，正则匹配是忽略大小写的。\n\n另外，由于查询过滤器的规则直接作用在用户block_user上，因此在删除用户block_user时，会提示有依赖项，此时可以通过在语句最后加上cascade进行删除，此时作用在此用户上的查询过滤规则也会被一同删除。\n\n\n\n受限于篇幅，其他选项就不再一一列举。需要注意的是，过滤规则命中的依据是，with_parameter命中任意一项，且其他字段的特征也符合即会判定为符合查询过滤规则。\n\n\n\n特别注意，不同的计划，可能部分字段无法按照预期进行拦截，例如：\n\n```\npostgres=# create block rule test filter by sql('test')with(estimate_row='3');\nCREATE BLOCK RULE\npostgres=# select * from test;\n c1 | c2\n----+----\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n(5 rows)\n```\n\n此时，语句关键字是可以匹配上的，查询的行数也超过了3行的限制，那为什么无法拦截呢？\n\n```\npostgres=# explain verbose select * from test;\n                                          QUERY PLAN\n-----------------------------------------------------------------------------------------------\n  id |                  operation                   | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------------+--------+------------+---------+---------\n   1 | ->  Data Node Scan on \"__REMOTE_FQS_QUERY__\" |      0 |            |       0 | 0.00\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Data Node Scan on \"__REMOTE_FQS_QUERY__\"\n         Output: test.c1, test.c2\n         Node/s: All datanodes (node_group, bucket:16384)\n         Remote query: SELECT c1, c2 FROM public.test\n```\n\n通过计划可以看出，此时是FQS计划，导致没有估算信息。因此此时无法进行拦截，对于CN轻量化的计划也是一样的，如果我们让语句强制走stream计划，那么就可以拦截成功：\n\n```\npostgres=# set enable_stream_operator=on;\nSET\npostgres=# set enable_fast_query_shipping=off;\nSET\npostgres=# select * from test;\nERROR:  hit block rule test(regexp_sql: test, estimate_row: 3(5))\npostgres=#  explain verbose select * from test;\n                                       QUERY PLAN\n-----------------------------------------------------------------------------------------\n  id |               operation                | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------+--------+------------+---------+---------\n   1 | ->  Row Adapter                        |      5 |            |       8 | 69.00\n   2 |    ->  Vector Streaming (type: GATHER) |      5 |            |       8 | 69.00\n   3 |       ->  CStore Scan on public.test   |      5 |            |       8 | 59.01\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Row Adapter\n         Output: c1, c2\n   2 --Vector Streaming (type: GATHER)\n         Output: c1, c2\n         Node/s: All datanodes (node_group, bucket:16384)\n   3 --CStore Scan on public.test\n         Output: c1, c2\n         Distribute Key: c1\n```\n\n所以，如果估算信息不准确，也会导致误拦截或者漏拦截的情况，因为计划的信息是通过估算得到的，因此这种情况无法避免。\n\n### （2）使用语句归一化特征值进行查询过滤\n\n语句归一化的特征值，目前有两个，分别是unique_sql_id和sql_hash，两者均是对查询树进行哈希计算之后得出的，区别在于前者是64位哈希值，后者是md5值，因此前者的重复概率会大于后者，在使用时尽量使用sql_hash进行过滤。\n\n\n\n很多小伙伴会问，这两个值如何获取呢？两种方法：\n\n   \n查看explain结果\n   \n\n```\npostgres=> explain verbose select * from tt where a>1;\n                                              QUERY PLAN\n ----------------------------------------------------------------------------------------------------\n   id |                     operation                     | E-rows | E-distinct | E-width | E-costs\n  ----+---------------------------------------------------+--------+------------+---------+---------\n    1 | ->  Row Adapter                                   |      1 |            |       8 | 16.00\n    2 |    ->  Vector Streaming (type: GATHER)            |      1 |            |       8 | 16.00\n    3 |       ->  Vector Partition Iterator               |      1 |            |       8 | 6.00\n    4 |          ->  Partitioned CStore Scan on public.tt |      1 |            |       8 | 6.00\n\n    Predicate Information (identified by plan id)\n  -------------------------------------------------\n    3 --Vector Partition Iterator\n          Iterations: 3\n    4 --Partitioned CStore Scan on public.tt\n          Filter: (tt.a > 1)\n          Pushdown Predicate Filter: (tt.a > 1)\n          Partitions Selected by Static Prune: 1..3\n\n  Targetlist Information (identified by plan id)\n  ----------------------------------------------\n    1 --Row Adapter\n          Output: a, b\n    2 --Vector Streaming (type: GATHER)\n          Output: a, b\n          Node/s: datanode1\n    3 --Vector Partition Iterator\n          Output: a, b\n    4 --Partitioned CStore Scan on public.tt\n          Output: a, b\n\n               ====== Query Summary =====\n  -----------------------------------------------------\n  Parser runtime: 0.029 ms\n  Planner runtime: 0.286 ms\n  Unique SQL Id: 2229243778\n  Unique SQL Hash: sql_aae71adfaa3d91bfe75499d92ad969e8\n (34 rows)\n```\n\n   \n查看topsql记录\n   \n\n```\n queryid                     | 95701492082350773\n query                       | select * from tt where a>10;\n query_plan                  | 1 | Row Adapter  (cost=14.00..14.00 rows=1 width=8)\n                             | 2 |  ->Vector Streaming (type: GATHER)  (cost=0.06..14.00 rows=1 width=8)\n                             | 3 |   ->Vector Partition Iterator  (cost=0.00..4.00 rows=1 width=8)\n                             |   |     Iterations: 2\n                             | 4 |    ->Partitioned CStore Scan on public.tt  (cost=0.00..4.00 rows=1 width=8)\n                             |   |      Filter: (tt.a > 10)\n                             |   |      Pushdown Predicate Filter: (tt.a > 10)\n                             |   |      Partitions Selected by Static Prune: 2..3\n node_group                  | installation\n pid                         | 139803379566936\n lane                        | fast\n unique_sql_id               | 2229243778\n session_id                  | 1732413324.139803379566936.coordinator1\n min_read_bytes              | 0\n max_read_bytes              | 0\n average_read_bytes          | 0\n min_write_bytes             | 0\n max_write_bytes             | 0\n average_write_bytes         | 0\n recv_pkg                    | 2\n send_pkg                    | 2\n recv_bytes                  | 3297\n send_bytes                  | 57\n stmt_type                   | SELECT\n except_info                 |\n unique_plan_id              | 0\n sql_hash                    | sql_aae71adfaa3d91bfe75499d92ad969e8\n```\n\n可以看出两种方法都可以轻松获取这两个语句归一化的特征值，explain可以在事前提前获取，topsql可以在语句执行后进行获取。\n\n\n\n这个时候，可能很多小伙伴又会有疑问，语句中的条件有变化，是否会影响归一化的特征值呢？\n\n答案是不会，因为归一化过程中会去除常量的影响，上述的举例中两个语句条件中的常量值并不相同，但归一化的特征值确实一样的。\n\n### （3）查询过滤的性能\n\n由于语句的过滤，特别是关键词的正则匹配通常是比较耗时的，此时如果有过多的过滤规则，可能导致执行时间的劣化，特别是对于短查询可能影响更为明显。\n\n\n\n**本地实测：**正则匹配关键词长度1024，建立查询过滤规则1000条左右时，对于查询的影响在27.72ms左右，且如果考虑其他匹配项，可能影响会更大，所以，不建议添加太多的查询过滤规则。且业务稳定后可以只对特定开发或者新业务的用户创建查询过滤规则，此时查询过滤规则会优先通过绑定的用户跳过无效的过滤，减少对性能的性能的影响。\n\n### （4）过滤时间查看\n\n可以配置GUC参数analysis_options查看查询过滤规则对正常语句所消耗的时间。\n\n```\nset analysis_options='on(BLOCK_RULE)';\n\n-- explain performance + query\n\n                    User Define Profiling\n-----------------------------------------------------------------\nSegment Id: 3  Track name: Datanode build connection\n      datanode1 (time=0.288 total_calls=1 loops=1)\n      datanode2 (time=0.301 total_calls=1 loops=1)\n      datanode3 (time=0.321 total_calls=1 loops=1)\n      datanode4 (time=0.268 total_calls=1 loops=1)\nSegment Id: 3  Track name: Datanode wait connection\n      datanode1 (time=0.016 total_calls=1 loops=1)\n      datanode2 (time=0.038 total_calls=1 loops=1)\n      datanode3 (time=0.021 total_calls=1 loops=1)\n      datanode4 (time=0.017 total_calls=1 loops=1)\nSegment Id: 1  Track name: block rule check time\n      coordinator1 (time=0.028 total_calls=1 loops=1)\n```\n\n### （5）拦截记录\n\n**[仅适用于****9.1.0.200****及以上]**\n\n创建查询过滤规则后会拦截很多烂SQL，如何看拦截的语句有哪些呢？可以通过topsql进行查看，abort_info会记录拦截信息，也就是查询的报错信息。\n\n```\npostgres=# select abort_info,query from GS_WLM_SESSION_INFO where abort_info like '%hit block rule test%';\n                        abort_info                         |        query\n-----------------------------------------------------------+---------------------\n hit block rule test(regexp_sql: test, estimate_row: 3(5)) | select * from test;\n(1 rows)\n```\n\n# 4. 总结\n\n查询过滤器在9.1.0.100和9.1.0.200版本丰富了大量的功能，提高了烂SQL拦截的灵活性。\n\n管控面后续版本同样可以直接通过前端页面对查询过滤规则进行管理，大家敬请期待。\n\n有任何问题欢迎留言讨论，我们将不断丰富和完善查询过滤功能，让烂SQL无门可入。\n\n\n\n华为开发者空间，汇聚鸿蒙、昇腾、鲲鹏、GaussDB、欧拉等各项根技术的开发资源及工具，致力于为每位开发者提供一台云主机、一套开发工具及云上存储空间，让开发者基于华为根生态创新。[点击链接](https://developer.huaweicloud.com/space/devportal/desktop?utm_source=kfzwzdspace& utm_adplace=nrcbds)，免费领取您的专属云主机\n\n\n\n[**点击关注，第一时间了解华为云新鲜技术~**](https://bbs.huaweicloud.com/blogs?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)\n",
    "labels": [],
    "created_at": "2024-12-20T16:20:57Z"
  },
  {
    "id": 2,
    "title": "ChatGPT生成接口测试用例（一）",
    "url": "https://www.cnblogs.com/tester2test/p/18619460",
    "content": "\n<img src=\"https://img2024.cnblogs.com/blog/15184/202412/15184-20241220154829587-467838433.jpg\" width=\"300\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n　　接口测试在软件开发生命周期中扮演着至关重要的角色，有助于验证不同模块之间的交互是否正确。若协议消息被恶意修改，系统是否能够恰当处理，以确保系统的功能正常运行，不会出现宕机或者安全问题。\n\n## 5.1 ChatGPT在接口测试中的角色\n\n　　接口测试是确保系统各个模块协同工作的关键环节。传统的接口测试用例编写通常依赖于测试人员的经验、手动操作，这可能导致接口测试用例的不充分和测试内容的遗漏。ChatGPT能够理解和生成文本使其成为编写接口测试用例的理想工具。\n\n### 5.1.1 理解系统需求和接口规范\n\n　　ChatGPT可以用来解析系统需求规格说明书和接口文档。通过输入相关文档，ChatGPT可以生成摘要、要点和问题列表等，帮助测试人员更好地把握测试的范围和重点。\n\n　　假设我们正在测试一个电子商务网站的购物车接口。我们有系统需求文档和接口文档，但它们很冗长，需要花费大量的时间阅读、理解。但是有了ChatGPT以后，只需要将这些文档输入ChatGPT，它就可以快速给出摘要，列出所有与购物车相关的功能和要点等。这有助于快速地确定测试重点，确保接口测试用例的全面性。\n\n### 5.1.2 生成测试用例模板\n\n　　ChatGPT可以协助测试团队生成测试用例的模板，包括输入数据、操作步骤和预期结果等。测试人员可以向ChatGPT提供接口描述和测试目标，ChatGPT会自动生成基本的测试用例结构。测试用例模板化以后，既可以加快测试用例的编写速度，又能确保测试用例的一致性。\n\n　　对于一个社交媒体平台的接口测试，我们需要测试用户发布帖子的功能。通过描述接口规范和测试目标，ChatGPT可以自动生成测试用例模板，包括帖子内容、发布操作的步骤和预期的发布结果等。这样，测试人员只需填写具体的内容，而不必从头开始编写每个接口测试用例。\n\n例如，对于一个电子商务网站的购物车接口，ChatGPT生成的接口文档如下所示。\n\n**ChatGPT****生成的接口文档**\n\n> \n**接口名称：**add_to_cart\n**接口概述：**该接口用于将指定商品添加到购物车。\n**请求方式：**POST请求\n**请求****URL****：**https://example.com/api/add_to_cart\n**参数列表：**\n<ul>\n   item_id（商品ID）   \n</ul>\n参数描述：用于指定要添加到购物车的商品的唯一标识。\n参数类型：整数\n是否必填：是\n<ul>\n   num（数量）   \n</ul>\n参数描述：指定要添加的商品数量。\n参数类型：整数\n是否必填：是\n**请求示例：**\nPOST请求https://example.com/api/add_to_cart\n**请求体：**\n{\n\"item_id\": 12345，\n\"num\": 2\n}\n\n**响应示例：**\n{\n\"status\": 200，\n\"message\": \"添加商品到购物车成功\"，\n\"data\": {\n\"cart_total_items\": 5\n}\n}\n\n**响应字段说明：**\nstatus：标识业务是否成功的状态码。\n<ul>\n    200 表示请求成功，商品成功添加到购物车。   \n    其他状态码可根据具体情况自行定义。   \n</ul>\nmessage：状态信息，对状态码的详细描述。\ndata：包含响应数据。\n<ul>\n    cart_total_items：购物车中的总商品数量。   \n</ul>\n\n\n\n\n### 5.1.3 探索边界条件\n\n 在接口测试中，通常需要测试各种输入数据的边界条件和异常情况。ChatGPT可以帮助测试人员生成边界条件的接口测试用例，确保系统能够正确处理各种情况。\n\n在电子商务网站的购物车接口的测试中，我们需要确保它能够正确处理购买的商品数量的输入，包括正常的商品数量输入和异常地输入。异常输入可能包括带小数点的数字、字母、全角数字、中文字符以及货币符号等特殊字符。不同类型的数据库具有不同的数字上下限，例如，在MySQL中，SMALLINT类型占用2个字节，可以存储从-32768到32767的整数。因此，我们需要考虑商品数量超过上限、低于下限、等于上限、等于下限以及0作为特殊数字和正常数字等各种边界情况的测试用例。\n\n 很多情况下，通过UI无法提交的数字，若接口测试通过协议直接发送请求，在应用程序的后台没有进行校验并且数据库没有添加约束条件的情况下，仍然可以正常提交，这可能导致数据无法正常存储等严重问题。\n\n 例如，购物车中单个商品数量最大可以为9999，考虑边界值测试用例方法设计接口测试用例，则可以获得以下边界用例，ChatGPT生成的边界值接口测试用例如下所示。\n\n**ChatGPT****生成的边界值接口测试用例**\n\n> \n用例编号：TC001\n用例名称:添加数量为0的商品\n输入参数:\nitem_id: 123456\nnum: 0\n预期结果:\n添加失败，提示数量不能为0\n......\n用例编号：TC005\n用例名称:添加超过库存的商品\n输入参数:\nitem_id: 123456\nnum: 10000\n预期结果:\n添加失败，提示超过库存\n......\n\n\n\n",
    "labels": [],
    "created_at": "2024-12-20T16:17:55Z"
  },
  {
    "id": 1,
    "title": "LLM后训练绝招：1%预训练成本，实现最高20倍算力扩展效果",
    "url": "https://blog.csdn.net/OneFlow_Official/article/details/144124481",
    "content": "\n<img alt=\"d752e993e29569be524ff4dbc483663b.png\" src=\"https://img-blog.csdnimg.cn/img_convert/d752e993e29569be524ff4dbc483663b.png\" referrerpolicy=\"no-referrer\">\n\n\n\n根据规模定律，扩大训练计算规模可以提高大型语言模型（LLM）性能的关键，但调研机构Epoch AI的研究，LLM再训练无需高额费用，也能让AI能力获得显著提升。\n\n 在该研究中，他们引入了一个基本框架，用于量化后训练增强的收益和成本，特别是通过计算等效增益来衡量收益。他们将该框架应用于一系列具有代表性的后训练增强，并发现性能提升非常显著，但微调成本通常与预训练成本相比非常小，某些后训练增强技术可以在不到1%预训练成本的情况下，提供相当于增加5到20倍预训练计算资源获得的效果。\n\n （本文由OneFlow编译发布，转载请联系授权。原文：https://epochai.org/blog/ai-capabilities-can-be-significantly-improved-without-expensive-retraining）\n\n**<strong>作者**|</strong>**<strong>EPOCH AI**</strong>\n\n**OneFlow编译**\n\n<strong>翻译｜刘乾裕\n**题图由****[SiliconCloud]()平台生成**</strong>\n\n近年来，训练大语言模型和类似基础模型所需的高强度计算资源，已成为推动人工智能进步的主驱动力之一。这也让人们深刻认识到一个“惨痛教训”：更好利用计算资源的通用方法最终被证明是最有效的。如今，训练最前沿模型的成本已经高到只有少数参与者能够承担（https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems）。\n\n我们的研究探讨了在训练完成后提升模型性能的方法，这些方法无需依赖大量计算资源。我们将这些改进措施分为五类，详见下表。\n\n<img alt=\"9f94c575ad16d35fefad5a9b4e08391f.png\" src=\"https://img-blog.csdnimg.cn/img_convert/9f94c575ad16d35fefad5a9b4e08391f.png\" referrerpolicy=\"no-referrer\">\n\n\n\n后训练增强的类别及示例\n \n\n你可以在此处阅读完整论文（https://arxiv.org/abs/2312.07413）。本文由Epoch AI、Open Philanthropy、加州大学伯克利分校和ORCG合作完成。\n\n### **1**\n\n### **关键成果**\n\n**计算等效增益 (CEG）**：我们提出了计算等效增益这一概念，用于量化各类增强方法带来的性能提升。CEG被定义为在不采用增强的情况下，预训练计算量需要增加多少才能达到与增强方法相同的基准性能提升。我们开发了一种基于公开基准进行评估的估算方法，以此来计算CEG。\n\n<img alt=\"2e24ca31a86d2cbd9f8bc911a8e12cc4.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/2e24ca31a86d2cbd9f8bc911a8e12cc4.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n该示例中，CEG（计算等效增益）是5倍。同样的性能提升可以通过应用后训练增强（PTE）或将预训练计算规模扩大5倍来实现。\n\n**后训练增强效果调查（PTE）**：我们对PTE进行了研究，涵盖了五个方面即工具、提示、辅助结构（scaffolding）、解决方案选择和数据增强。他们的CEG估算值通常在相关基准数值的5到30倍之间。\n\n我们还对这些PTE的计算成本进行了估算，主要包括两类：一是使模型能够应用PTE所需的初始成本（例如，微调模型以学习使用某种工具）；二是推理过程中产生的持续成本（例如，增强方法需要生成多个样本并从中选取最佳结果）。对于我们评估的所有PTE，其初始成本通常低于预训练成本的10%，大部分甚至不到0.1%。尽管大多数情况下推理成本不会受到明显影响，但在个别情况下推理成本可能会增加至100倍左右。\n\n<img alt=\"45c9d55646bfa168b6cc2697f2759dc3.png\" src=\"https://img-blog.csdnimg.cn/img_convert/45c9d55646bfa168b6cc2697f2759dc3.png\" referrerpolicy=\"no-referrer\">\n\n\n\n结果概要：使用计算等效增益量化的技术所产生的改进。x轴展示了相关的一次性成本（左）和推理成本（右）。\n\n### **2**\n\n### **政策影响**\n\n随着后训练增强技术的不断发展和完善，已部署的大语言模型功能将会随着时间推移而增强。这一发现表明，安全策略（例如，负责任扩展策略，metr.org/blog/2023-09-26-rsp）应当包含一个“安全缓冲区”：限制那些在未来随着后训练增强，可能达到危险水平的模型功能。\n\n由于这些增强功能的计算成本较低，更多参与者能够加入开发，不再局限于那些具备大规模预训练能力的主体。这使得能力提升趋向民主化，但也为AI发展的监管带来新挑战，因为仅关注那些拥有大量计算资源的实体已不足以有效应对未来的风险。\n\n其他人都在看\n    [推算LLM训练的GPU内存需求]()        [用初中数学理解LLM工作原理]()        [企业AI调查:AI支出激增6倍，多模型部署盛行]()        [10倍工程师编码工具：Cursor x SiliconCloud]()        [强化学习之父Rich Sutton：AGI研究下一个范式]()        [大模型训练秘方:1000次超参数优化实验的发现]()        [LLM逻辑推演策略：推理时计算vs训练时计算]()    \n\n\n\n**让超级产品开发者实现“Token自由”**\n \n\n**<em>**[邀请好友体验SiliconCloud]()，****</em>**<em>**狂送2000万Token/人****</em>\n\n****邀请越多，Token奖励越多****\n****[siliconflow.cn/zh-cn/siliconcloud](http://siliconflow.cn/zh-cn/siliconcloud)****\n",
    "labels": [],
    "created_at": "2024-12-20T23:16:47Z"
  }
]