[
  {
    "id": 3,
    "title": "拦截烂SQL，解读GaussDB(DWS)查询过滤器过滤规则原理",
    "url": "https://www.cnblogs.com/huaweiyun/p/18619491",
    "content": "\n本文分享自华为云社区[《GaussDB(DWS)查询过滤器过滤规则原理与使用介绍》](https://bbs.huaweicloud.com/blogs/441767?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)，作者： 清道夫。\n\n# 1. 前言\n\n**适用版本：【9.1.0.100（及以上）】**\n\n查询过滤器在9.1.0.100之前就具备提供查询过滤功能的能力，但仅支持自动隔离反复查询被终止的查询，防止烂SQL再次执行。\n\n老版本主要面向异常熔断机制和紧急拦截场景，前者可以与异常规则联动，自动将触发异常规则的语句添加到黑名单中，后者是需要手动找到core或者引发hang的语句进行屏蔽。\n\n\n\n大家有兴趣可以翻一下之前的这篇文章[GaussDB(DWS)查询过滤器原理与应用](https://bbs.huaweicloud.com/blogs/401188)。\n\n9.1.0.100及9.1.0.200版本对查询过滤器做了功能的改进，可以通过多维度进行烂SQL识别，功能更丰富，配置更灵活。\n\n# 2. 原理介绍\n\n在原理介绍之前，先举个简单的例子。在业务开发过程中，要想禁止对2张以上的表进行关联查询，此时可以使用DDL语句创建过滤规则：\n\n```\nCREATE BLOCK RULE forbid_2_t_sel FOR SELECT FILTER BY  SQL('test_block_rule') with(table_num='2');\n```\n\ntable_num指的是一个语句中出现的表的个数，此时所有查询语句不能包含有两张表以上的查询。\n\n```\n--两张表直接关联查询，可以正常执行\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\n c1 | c2 | c1 | c2\n----+----+----+----\n(0 rows)\n\n--三张表直接关联查询，被拦截\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2 join test_block_rule3 t3 on t2.c1=t3.c1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 2(3))\n```\n\n说到这，整体逻辑就非常清楚了。用户可以提前识别烂SQL的特征，然后抽象出来，用DDL语句创建规则，后续会对查询的语句进行过滤，被规则筛选出来的便是烂SQL，执行前会报错，反之则可以正常执行。\n\n查询过滤器框架及功能原理概况：\n\n<img src=\"https://img2024.cnblogs.com/blog/2030258/202412/2030258-20241220160540342-411141587.png\" alt=\"\" referrerpolicy=\"no-referrer\">\n\n\n\n从图中可以看出，之前的查询过滤器的功能依然存在，可以保证与异常规则的联动，新版本的增强更注重规则的灵活性和功能的丰富性。\n\n# 3. 使用介绍\n\n## 3.1 查询过滤规则元数据管理\n\n查询过滤规则，可以通过DDL进行新增、删除或者修改，其语法如下：\n\n### （1）创建\n\n```\nCREATE BLOCK RULE [ IF NOT EXISTS ] block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE ] |\n    FILTER BY\n    { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) }\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n其中，\n\n   \n**block_name：**过滤规则的名称\n   \n   \n**user_name：**规则应用的对象用户\n   \n   \n**host：**是规则应用的客户端IP\n   \n   \n**FOR：**语句类型，支持对UPDATE/SELECT/INSERT/DELETE/MEGE INTO五种类型语句进行限制\n   \n   \n**FILTER BY：**过滤方法，包含两种形式\n   \n   \n**SQL：**根据关键词对语句进行正则匹配，例如表名，其长度不能超过1024，建议尽量精简\n   \n   \n**TEMPLATE：**\n   \n   \n**unique_sql_id：**归一化的64位哈希值，重复概率较sql_hash大一些\n   \n   \n**sql_hash：**归一化的哈希值（md5），一般不会重复，相较unique_sql_id更推荐使用\n   \n   \n**with_parameter：**查询过滤规则选项参数，可以附加多个条件，满足其一便会匹配过滤。\n   \n   \n**application_name：**客户端名称\n   \n   \n**query_band：**负载标识\n   \n   \n**table_num：**包含的基表个数\n   \n   \n**partition_num：**扫描分区的数量\n   \n   \n**estimate_row：**基表输出行数\n   \n   \n**resource_pool：**切换的目标资源池，仅适用于9.1.0.200及以上\n   \n   \n**max_active_num：**可并发执行的语句个数，仅适用于9.1.0.200及以上\n   \n   \n**is_warning：**改变拦截行为为告警，而非默认的报错，仅适用于9.1.0.200及以上\n   \n\n其中，user_name和FILTER BY是必选项，其他可以通过业务实际需要进行配置。\n\n### （2）修改\n\n```\nALTER BLOCK RULE block_name RENAME to new_block_name;\n```\n\n通过rename对查询过滤的规则进行重命名。\n\n```\nALTER BLOCK RULE block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] | [ TO DEFAULT ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE | DEFAULT ] |\n    [ [ FILTER BY ]\n    [ { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) } ] ]\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n所有选项均支持二次修改，如果需要去除部分字段的限制，可以指定default关键词，例如：\n\n```\n--修改为只能查询1张表\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num='1');\nALTER BLOCK RULE\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 1(2))\npostgres=# select * from test_block_rule1 t1;\n c1 | c2\n----+----\n(0 rows)\n\n--去除查询中表个数的限制\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num=default);\nALTER BLOCK RULE\n--再次查询报错拦截\npostgres=# select * from test_block_rule1 t1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule)\n```\n\n### （3）删除\n\n```\nDROP BLOCK RULE [ IF EXISTS ] block_name;\n```\n\n## 3.2 权限问题\n\n对于普通用户来讲是没有创建查询过滤规则权限的，需要管理员或者管理员将权限赋给某一普通用户才可以。\n\n```\n--切换至普通用户\npostgres=# set role jack password 'xxx';\nSET\n--创建查询过滤规则报错提示无权限\npostgres=> create block rule bl2 filter by sql('test');\nERROR:  CREATE/ALTER/DROP BLOCK RULE permission denied for current user\n--重置user\npostgres=> reset role;\nRESET\n--对普通用户进行授权\npostgres=# grant gs_role_block to jack;\nGRANT ROLE\n--切换普通用户\npostgres=# set role jack password 'xxx';\nSET\n--再次创建成功\npostgres=> create block rule bl2 filter by sql('test');\nCREATE BLOCK RULE\n```\n\n建议创建查询过滤规则时尽量缩小适用范围，避免误过滤，或者范围过大导致性能劣化。\n\n## 3.3 备份恢复\n\n对于查询过滤规则的备份或者恢复的权限与操作元数据的权限一致，需要管理员或者管理员讲权限赋值给某一普通用户才可以，用户可以通过gs_dump导出查询过滤规则定义。\n\n如果想查看或者导入查询过滤规则的定义，可以通过pg_get_blockruledef进行查询。\n\n```\npostgres=# select * from pg_get_blockruledef('test');\n                         pg_get_blockruledef\n----------------------------------------------------------------------\n CREATE BLOCK RULE test FILTER BY SQL('test') WITH(estimate_row='3');\n(1 row)\n```\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n## 3.4 使用举例\n\n### （1）使用关键词进行查询过滤\n\n```\nCREATE BLOCK RULE bl1\nTo block_user\nFOR SELECT\nFILTER BY SQL ('tt')\nWITH(partition_num='2',\n     table_num='1',\n     estimate_row='5'\n     );\n\npostgres=> select * from tt;\nERROR:  hit block rule bl1(user_name: block_user, block_type: SELECT, regexp_sql: tt, partition_num: 2(3), table_num: 1(1), estimate_row: 5(1))\n```\n\n从上面的查询可以看出，查询语句包含了tt关键字，并且扫描的分区个数超过了2，此时执行语句被过滤拦截。需要注意的是，**扫描分区的个数并不总是准确的**，仅能识别**静态**的分区剪枝个数，执行过程中的**动态剪枝**并不能被识别。\n\n\n\n**小技巧：**使用关键词过滤时可以先使用正则匹配符~*进行测试，正则匹配是忽略大小写的。\n\n另外，由于查询过滤器的规则直接作用在用户block_user上，因此在删除用户block_user时，会提示有依赖项，此时可以通过在语句最后加上cascade进行删除，此时作用在此用户上的查询过滤规则也会被一同删除。\n\n\n\n受限于篇幅，其他选项就不再一一列举。需要注意的是，过滤规则命中的依据是，with_parameter命中任意一项，且其他字段的特征也符合即会判定为符合查询过滤规则。\n\n\n\n特别注意，不同的计划，可能部分字段无法按照预期进行拦截，例如：\n\n```\npostgres=# create block rule test filter by sql('test')with(estimate_row='3');\nCREATE BLOCK RULE\npostgres=# select * from test;\n c1 | c2\n----+----\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n(5 rows)\n```\n\n此时，语句关键字是可以匹配上的，查询的行数也超过了3行的限制，那为什么无法拦截呢？\n\n```\npostgres=# explain verbose select * from test;\n                                          QUERY PLAN\n-----------------------------------------------------------------------------------------------\n  id |                  operation                   | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------------+--------+------------+---------+---------\n   1 | ->  Data Node Scan on \"__REMOTE_FQS_QUERY__\" |      0 |            |       0 | 0.00\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Data Node Scan on \"__REMOTE_FQS_QUERY__\"\n         Output: test.c1, test.c2\n         Node/s: All datanodes (node_group, bucket:16384)\n         Remote query: SELECT c1, c2 FROM public.test\n```\n\n通过计划可以看出，此时是FQS计划，导致没有估算信息。因此此时无法进行拦截，对于CN轻量化的计划也是一样的，如果我们让语句强制走stream计划，那么就可以拦截成功：\n\n```\npostgres=# set enable_stream_operator=on;\nSET\npostgres=# set enable_fast_query_shipping=off;\nSET\npostgres=# select * from test;\nERROR:  hit block rule test(regexp_sql: test, estimate_row: 3(5))\npostgres=#  explain verbose select * from test;\n                                       QUERY PLAN\n-----------------------------------------------------------------------------------------\n  id |               operation                | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------+--------+------------+---------+---------\n   1 | ->  Row Adapter                        |      5 |            |       8 | 69.00\n   2 |    ->  Vector Streaming (type: GATHER) |      5 |            |       8 | 69.00\n   3 |       ->  CStore Scan on public.test   |      5 |            |       8 | 59.01\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Row Adapter\n         Output: c1, c2\n   2 --Vector Streaming (type: GATHER)\n         Output: c1, c2\n         Node/s: All datanodes (node_group, bucket:16384)\n   3 --CStore Scan on public.test\n         Output: c1, c2\n         Distribute Key: c1\n```\n\n所以，如果估算信息不准确，也会导致误拦截或者漏拦截的情况，因为计划的信息是通过估算得到的，因此这种情况无法避免。\n\n### （2）使用语句归一化特征值进行查询过滤\n\n语句归一化的特征值，目前有两个，分别是unique_sql_id和sql_hash，两者均是对查询树进行哈希计算之后得出的，区别在于前者是64位哈希值，后者是md5值，因此前者的重复概率会大于后者，在使用时尽量使用sql_hash进行过滤。\n\n\n\n很多小伙伴会问，这两个值如何获取呢？两种方法：\n\n   \n查看explain结果\n   \n\n```\npostgres=> explain verbose select * from tt where a>1;\n                                              QUERY PLAN\n ----------------------------------------------------------------------------------------------------\n   id |                     operation                     | E-rows | E-distinct | E-width | E-costs\n  ----+---------------------------------------------------+--------+------------+---------+---------\n    1 | ->  Row Adapter                                   |      1 |            |       8 | 16.00\n    2 |    ->  Vector Streaming (type: GATHER)            |      1 |            |       8 | 16.00\n    3 |       ->  Vector Partition Iterator               |      1 |            |       8 | 6.00\n    4 |          ->  Partitioned CStore Scan on public.tt |      1 |            |       8 | 6.00\n\n    Predicate Information (identified by plan id)\n  -------------------------------------------------\n    3 --Vector Partition Iterator\n          Iterations: 3\n    4 --Partitioned CStore Scan on public.tt\n          Filter: (tt.a > 1)\n          Pushdown Predicate Filter: (tt.a > 1)\n          Partitions Selected by Static Prune: 1..3\n\n  Targetlist Information (identified by plan id)\n  ----------------------------------------------\n    1 --Row Adapter\n          Output: a, b\n    2 --Vector Streaming (type: GATHER)\n          Output: a, b\n          Node/s: datanode1\n    3 --Vector Partition Iterator\n          Output: a, b\n    4 --Partitioned CStore Scan on public.tt\n          Output: a, b\n\n               ====== Query Summary =====\n  -----------------------------------------------------\n  Parser runtime: 0.029 ms\n  Planner runtime: 0.286 ms\n  Unique SQL Id: 2229243778\n  Unique SQL Hash: sql_aae71adfaa3d91bfe75499d92ad969e8\n (34 rows)\n```\n\n   \n查看topsql记录\n   \n\n```\n queryid                     | 95701492082350773\n query                       | select * from tt where a>10;\n query_plan                  | 1 | Row Adapter  (cost=14.00..14.00 rows=1 width=8)\n                             | 2 |  ->Vector Streaming (type: GATHER)  (cost=0.06..14.00 rows=1 width=8)\n                             | 3 |   ->Vector Partition Iterator  (cost=0.00..4.00 rows=1 width=8)\n                             |   |     Iterations: 2\n                             | 4 |    ->Partitioned CStore Scan on public.tt  (cost=0.00..4.00 rows=1 width=8)\n                             |   |      Filter: (tt.a > 10)\n                             |   |      Pushdown Predicate Filter: (tt.a > 10)\n                             |   |      Partitions Selected by Static Prune: 2..3\n node_group                  | installation\n pid                         | 139803379566936\n lane                        | fast\n unique_sql_id               | 2229243778\n session_id                  | 1732413324.139803379566936.coordinator1\n min_read_bytes              | 0\n max_read_bytes              | 0\n average_read_bytes          | 0\n min_write_bytes             | 0\n max_write_bytes             | 0\n average_write_bytes         | 0\n recv_pkg                    | 2\n send_pkg                    | 2\n recv_bytes                  | 3297\n send_bytes                  | 57\n stmt_type                   | SELECT\n except_info                 |\n unique_plan_id              | 0\n sql_hash                    | sql_aae71adfaa3d91bfe75499d92ad969e8\n```\n\n可以看出两种方法都可以轻松获取这两个语句归一化的特征值，explain可以在事前提前获取，topsql可以在语句执行后进行获取。\n\n\n\n这个时候，可能很多小伙伴又会有疑问，语句中的条件有变化，是否会影响归一化的特征值呢？\n\n答案是不会，因为归一化过程中会去除常量的影响，上述的举例中两个语句条件中的常量值并不相同，但归一化的特征值确实一样的。\n\n### （3）查询过滤的性能\n\n由于语句的过滤，特别是关键词的正则匹配通常是比较耗时的，此时如果有过多的过滤规则，可能导致执行时间的劣化，特别是对于短查询可能影响更为明显。\n\n\n\n**本地实测：**正则匹配关键词长度1024，建立查询过滤规则1000条左右时，对于查询的影响在27.72ms左右，且如果考虑其他匹配项，可能影响会更大，所以，不建议添加太多的查询过滤规则。且业务稳定后可以只对特定开发或者新业务的用户创建查询过滤规则，此时查询过滤规则会优先通过绑定的用户跳过无效的过滤，减少对性能的性能的影响。\n\n### （4）过滤时间查看\n\n可以配置GUC参数analysis_options查看查询过滤规则对正常语句所消耗的时间。\n\n```\nset analysis_options='on(BLOCK_RULE)';\n\n-- explain performance + query\n\n                    User Define Profiling\n-----------------------------------------------------------------\nSegment Id: 3  Track name: Datanode build connection\n      datanode1 (time=0.288 total_calls=1 loops=1)\n      datanode2 (time=0.301 total_calls=1 loops=1)\n      datanode3 (time=0.321 total_calls=1 loops=1)\n      datanode4 (time=0.268 total_calls=1 loops=1)\nSegment Id: 3  Track name: Datanode wait connection\n      datanode1 (time=0.016 total_calls=1 loops=1)\n      datanode2 (time=0.038 total_calls=1 loops=1)\n      datanode3 (time=0.021 total_calls=1 loops=1)\n      datanode4 (time=0.017 total_calls=1 loops=1)\nSegment Id: 1  Track name: block rule check time\n      coordinator1 (time=0.028 total_calls=1 loops=1)\n```\n\n### （5）拦截记录\n\n**[仅适用于****9.1.0.200****及以上]**\n\n创建查询过滤规则后会拦截很多烂SQL，如何看拦截的语句有哪些呢？可以通过topsql进行查看，abort_info会记录拦截信息，也就是查询的报错信息。\n\n```\npostgres=# select abort_info,query from GS_WLM_SESSION_INFO where abort_info like '%hit block rule test%';\n                        abort_info                         |        query\n-----------------------------------------------------------+---------------------\n hit block rule test(regexp_sql: test, estimate_row: 3(5)) | select * from test;\n(1 rows)\n```\n\n# 4. 总结\n\n查询过滤器在9.1.0.100和9.1.0.200版本丰富了大量的功能，提高了烂SQL拦截的灵活性。\n\n管控面后续版本同样可以直接通过前端页面对查询过滤规则进行管理，大家敬请期待。\n\n有任何问题欢迎留言讨论，我们将不断丰富和完善查询过滤功能，让烂SQL无门可入。\n\n\n\n华为开发者空间，汇聚鸿蒙、昇腾、鲲鹏、GaussDB、欧拉等各项根技术的开发资源及工具，致力于为每位开发者提供一台云主机、一套开发工具及云上存储空间，让开发者基于华为根生态创新。[点击链接](https://developer.huaweicloud.com/space/devportal/desktop?utm_source=kfzwzdspace& utm_adplace=nrcbds)，免费领取您的专属云主机\n\n\n\n[**点击关注，第一时间了解华为云新鲜技术~**](https://bbs.huaweicloud.com/blogs?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)\n",
    "labels": [],
    "created_at": "2024-12-20T16:20:57Z"
  },
  {
    "id": 2,
    "title": "ChatGPT生成接口测试用例（一）",
    "url": "https://www.cnblogs.com/tester2test/p/18619460",
    "content": "\n<img src=\"https://img2024.cnblogs.com/blog/15184/202412/15184-20241220154829587-467838433.jpg\" width=\"300\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n　　接口测试在软件开发生命周期中扮演着至关重要的角色，有助于验证不同模块之间的交互是否正确。若协议消息被恶意修改，系统是否能够恰当处理，以确保系统的功能正常运行，不会出现宕机或者安全问题。\n\n## 5.1 ChatGPT在接口测试中的角色\n\n　　接口测试是确保系统各个模块协同工作的关键环节。传统的接口测试用例编写通常依赖于测试人员的经验、手动操作，这可能导致接口测试用例的不充分和测试内容的遗漏。ChatGPT能够理解和生成文本使其成为编写接口测试用例的理想工具。\n\n### 5.1.1 理解系统需求和接口规范\n\n　　ChatGPT可以用来解析系统需求规格说明书和接口文档。通过输入相关文档，ChatGPT可以生成摘要、要点和问题列表等，帮助测试人员更好地把握测试的范围和重点。\n\n　　假设我们正在测试一个电子商务网站的购物车接口。我们有系统需求文档和接口文档，但它们很冗长，需要花费大量的时间阅读、理解。但是有了ChatGPT以后，只需要将这些文档输入ChatGPT，它就可以快速给出摘要，列出所有与购物车相关的功能和要点等。这有助于快速地确定测试重点，确保接口测试用例的全面性。\n\n### 5.1.2 生成测试用例模板\n\n　　ChatGPT可以协助测试团队生成测试用例的模板，包括输入数据、操作步骤和预期结果等。测试人员可以向ChatGPT提供接口描述和测试目标，ChatGPT会自动生成基本的测试用例结构。测试用例模板化以后，既可以加快测试用例的编写速度，又能确保测试用例的一致性。\n\n　　对于一个社交媒体平台的接口测试，我们需要测试用户发布帖子的功能。通过描述接口规范和测试目标，ChatGPT可以自动生成测试用例模板，包括帖子内容、发布操作的步骤和预期的发布结果等。这样，测试人员只需填写具体的内容，而不必从头开始编写每个接口测试用例。\n\n例如，对于一个电子商务网站的购物车接口，ChatGPT生成的接口文档如下所示。\n\n**ChatGPT****生成的接口文档**\n\n> \n**接口名称：**add_to_cart\n**接口概述：**该接口用于将指定商品添加到购物车。\n**请求方式：**POST请求\n**请求****URL****：**https://example.com/api/add_to_cart\n**参数列表：**\n<ul>\n   item_id（商品ID）   \n</ul>\n参数描述：用于指定要添加到购物车的商品的唯一标识。\n参数类型：整数\n是否必填：是\n<ul>\n   num（数量）   \n</ul>\n参数描述：指定要添加的商品数量。\n参数类型：整数\n是否必填：是\n**请求示例：**\nPOST请求https://example.com/api/add_to_cart\n**请求体：**\n{\n\"item_id\": 12345，\n\"num\": 2\n}\n\n**响应示例：**\n{\n\"status\": 200，\n\"message\": \"添加商品到购物车成功\"，\n\"data\": {\n\"cart_total_items\": 5\n}\n}\n\n**响应字段说明：**\nstatus：标识业务是否成功的状态码。\n<ul>\n    200 表示请求成功，商品成功添加到购物车。   \n    其他状态码可根据具体情况自行定义。   \n</ul>\nmessage：状态信息，对状态码的详细描述。\ndata：包含响应数据。\n<ul>\n    cart_total_items：购物车中的总商品数量。   \n</ul>\n\n\n\n\n### 5.1.3 探索边界条件\n\n 在接口测试中，通常需要测试各种输入数据的边界条件和异常情况。ChatGPT可以帮助测试人员生成边界条件的接口测试用例，确保系统能够正确处理各种情况。\n\n在电子商务网站的购物车接口的测试中，我们需要确保它能够正确处理购买的商品数量的输入，包括正常的商品数量输入和异常地输入。异常输入可能包括带小数点的数字、字母、全角数字、中文字符以及货币符号等特殊字符。不同类型的数据库具有不同的数字上下限，例如，在MySQL中，SMALLINT类型占用2个字节，可以存储从-32768到32767的整数。因此，我们需要考虑商品数量超过上限、低于下限、等于上限、等于下限以及0作为特殊数字和正常数字等各种边界情况的测试用例。\n\n 很多情况下，通过UI无法提交的数字，若接口测试通过协议直接发送请求，在应用程序的后台没有进行校验并且数据库没有添加约束条件的情况下，仍然可以正常提交，这可能导致数据无法正常存储等严重问题。\n\n 例如，购物车中单个商品数量最大可以为9999，考虑边界值测试用例方法设计接口测试用例，则可以获得以下边界用例，ChatGPT生成的边界值接口测试用例如下所示。\n\n**ChatGPT****生成的边界值接口测试用例**\n\n> \n用例编号：TC001\n用例名称:添加数量为0的商品\n输入参数:\nitem_id: 123456\nnum: 0\n预期结果:\n添加失败，提示数量不能为0\n......\n用例编号：TC005\n用例名称:添加超过库存的商品\n输入参数:\nitem_id: 123456\nnum: 10000\n预期结果:\n添加失败，提示超过库存\n......\n\n\n\n",
    "labels": [],
    "created_at": "2024-12-20T16:17:55Z"
  },
  {
    "id": 1,
    "title": "LLM后训练绝招：1%预训练成本，实现最高20倍算力扩展效果",
    "url": "https://blog.csdn.net/OneFlow_Official/article/details/144124481",
    "content": "\n<img alt=\"d752e993e29569be524ff4dbc483663b.png\" src=\"https://img-blog.csdnimg.cn/img_convert/d752e993e29569be524ff4dbc483663b.png\" referrerpolicy=\"no-referrer\">\n\n\n\n根据规模定律，扩大训练计算规模可以提高大型语言模型（LLM）性能的关键，但调研机构Epoch AI的研究，LLM再训练无需高额费用，也能让AI能力获得显著提升。\n\n 在该研究中，他们引入了一个基本框架，用于量化后训练增强的收益和成本，特别是通过计算等效增益来衡量收益。他们将该框架应用于一系列具有代表性的后训练增强，并发现性能提升非常显著，但微调成本通常与预训练成本相比非常小，某些后训练增强技术可以在不到1%预训练成本的情况下，提供相当于增加5到20倍预训练计算资源获得的效果。\n\n （本文由OneFlow编译发布，转载请联系授权。原文：https://epochai.org/blog/ai-capabilities-can-be-significantly-improved-without-expensive-retraining）\n\n**<strong>作者**|</strong>**<strong>EPOCH AI**</strong>\n\n**OneFlow编译**\n\n<strong>翻译｜刘乾裕\n**题图由****[SiliconCloud]()平台生成**</strong>\n\n近年来，训练大语言模型和类似基础模型所需的高强度计算资源，已成为推动人工智能进步的主驱动力之一。这也让人们深刻认识到一个“惨痛教训”：更好利用计算资源的通用方法最终被证明是最有效的。如今，训练最前沿模型的成本已经高到只有少数参与者能够承担（https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems）。\n\n我们的研究探讨了在训练完成后提升模型性能的方法，这些方法无需依赖大量计算资源。我们将这些改进措施分为五类，详见下表。\n\n<img alt=\"9f94c575ad16d35fefad5a9b4e08391f.png\" src=\"https://img-blog.csdnimg.cn/img_convert/9f94c575ad16d35fefad5a9b4e08391f.png\" referrerpolicy=\"no-referrer\">\n\n\n\n后训练增强的类别及示例\n \n\n你可以在此处阅读完整论文（https://arxiv.org/abs/2312.07413）。本文由Epoch AI、Open Philanthropy、加州大学伯克利分校和ORCG合作完成。\n\n### **1**\n\n### **关键成果**\n\n**计算等效增益 (CEG）**：我们提出了计算等效增益这一概念，用于量化各类增强方法带来的性能提升。CEG被定义为在不采用增强的情况下，预训练计算量需要增加多少才能达到与增强方法相同的基准性能提升。我们开发了一种基于公开基准进行评估的估算方法，以此来计算CEG。\n\n<img alt=\"2e24ca31a86d2cbd9f8bc911a8e12cc4.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/2e24ca31a86d2cbd9f8bc911a8e12cc4.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n该示例中，CEG（计算等效增益）是5倍。同样的性能提升可以通过应用后训练增强（PTE）或将预训练计算规模扩大5倍来实现。\n\n**后训练增强效果调查（PTE）**：我们对PTE进行了研究，涵盖了五个方面即工具、提示、辅助结构（scaffolding）、解决方案选择和数据增强。他们的CEG估算值通常在相关基准数值的5到30倍之间。\n\n我们还对这些PTE的计算成本进行了估算，主要包括两类：一是使模型能够应用PTE所需的初始成本（例如，微调模型以学习使用某种工具）；二是推理过程中产生的持续成本（例如，增强方法需要生成多个样本并从中选取最佳结果）。对于我们评估的所有PTE，其初始成本通常低于预训练成本的10%，大部分甚至不到0.1%。尽管大多数情况下推理成本不会受到明显影响，但在个别情况下推理成本可能会增加至100倍左右。\n\n<img alt=\"45c9d55646bfa168b6cc2697f2759dc3.png\" src=\"https://img-blog.csdnimg.cn/img_convert/45c9d55646bfa168b6cc2697f2759dc3.png\" referrerpolicy=\"no-referrer\">\n\n\n\n结果概要：使用计算等效增益量化的技术所产生的改进。x轴展示了相关的一次性成本（左）和推理成本（右）。\n\n### **2**\n\n### **政策影响**\n\n随着后训练增强技术的不断发展和完善，已部署的大语言模型功能将会随着时间推移而增强。这一发现表明，安全策略（例如，负责任扩展策略，metr.org/blog/2023-09-26-rsp）应当包含一个“安全缓冲区”：限制那些在未来随着后训练增强，可能达到危险水平的模型功能。\n\n由于这些增强功能的计算成本较低，更多参与者能够加入开发，不再局限于那些具备大规模预训练能力的主体。这使得能力提升趋向民主化，但也为AI发展的监管带来新挑战，因为仅关注那些拥有大量计算资源的实体已不足以有效应对未来的风险。\n\n其他人都在看\n    [推算LLM训练的GPU内存需求]()        [用初中数学理解LLM工作原理]()        [企业AI调查:AI支出激增6倍，多模型部署盛行]()        [10倍工程师编码工具：Cursor x SiliconCloud]()        [强化学习之父Rich Sutton：AGI研究下一个范式]()        [大模型训练秘方:1000次超参数优化实验的发现]()        [LLM逻辑推演策略：推理时计算vs训练时计算]()    \n\n\n\n**让超级产品开发者实现“Token自由”**\n \n\n**<em>**[邀请好友体验SiliconCloud]()，****</em>**<em>**狂送2000万Token/人****</em>\n\n****邀请越多，Token奖励越多****\n****[siliconflow.cn/zh-cn/siliconcloud](http://siliconflow.cn/zh-cn/siliconcloud)****\n",
    "labels": [],
    "created_at": "2024-12-20T23:16:47Z"
  }
]