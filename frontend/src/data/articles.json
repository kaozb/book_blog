[
  {
    "id": 27,
    "title": "\n\n探索提升RAG系统问答质量的技术路线\n",
    "url": "https://mp.weixin.qq.com/s/VZdQyfaARBzzpHUVRm2yOg",
    "content": "\n<img class=\"rich_pages wxw-img js_insertlocalimg\" data-backh=\"415\" data-backw=\"548\" data-imgfileid=\"100011996\" data-ratio=\"0.7574074074074074\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7Bu8csC0RKMczR5tuOpePT13iaq8yjTDXyibdScVtibj3IFbzvoNgE5aR78iaPzfZCtvIz1fB1gJicxcTYJw/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\n\n\n\n\n\n\n**Query Translation**\n\n\n\n\n\n\n\n\n\n\n**Query Translation 将用户的自然语言查询转换为更适合检索和生成的形式**。在这个过程中，系统将原始查询转化成一种或多种可以提升信息检索效果的形式，确保系统能够更有效地从不同的数据源中提取相关信息。\n\n\n\n\n对于具有挑战性的检索任务，用户问题的措辞可能不太恰当。Query Translation****是指将用户的原始问题重新表达，使其更适合检索过程，提高检索的相关性和准确性。\n\n\n\n\n在 RAG 系统中，直接使用用户的原始问题进行检索可能会遇到以下问题：\n   **查询过于模糊或复杂**，数据库难以直接匹配相关内容。      **信息缺失**，例如用户只输入“DeepSeek-R1 的优势？”而没有明确上下文。      **不同数据库的适配性**，原始查询可能需要调整才能适应不同的数据源（如结构化数据、文档、向量数据库等）。   \n\n\n\nQuery Translation 包括以下几个技术或方法：\n\n**Multi-query**：通过生成多个查询，系统可以扩大检索范围并提高准确性。例如，针对同一个问题生成几个不同的查询，增加从多个角度检索到相关信息的可能性，提高召回率。\n   原始查询：“如何提高 DeepSeek-R1的推理能力？”      生成的多个查询：   <ol  class=\"list-paddingleft-1\">   “DeepSeek-R1 的推理能力受哪些因素影响？”      “如何优化提示（prompt）来增强 DeepSeek-R1 的推理能力？”      “有哪些方法可以提升 DeepSeek-R1 在数学推理任务上的表现？”   \n\n\n\n**RAG-Fusion**：指将多个查询的结果进行融合，选择最相关的信息。通过将多个查询结果合并，可以得到更加丰富和精确的答案。\n\n**Multi-query 主要是生成多个查询，而 RAG-Fusion 侧重于如何融合这些查询的检索结果（例如 Reciprocal Rank Fusion）。**\n\n\n\n\n**Decomposition**：将复杂问题拆解成多个简单问题，然后分别检索每个子问题的答案。这样可以避免处理复杂问题时可能遇到的信息检索困难。\n   原始查询：“比较 Transformer 和 RNN 在文本摘要任务上的优缺点。”      分解后的查询：      “Transformer 在文本摘要任务上的优点是什么？”      “RNN 在文本摘要任务上的优点是什么？”      “Transformer 和 RNN 在文本摘要任务上的对比研究有哪些？”   \n\n\n\n**Step-back**：如果当前查询无法获得足够相关的文档，系统可以先查询更高层次的信息，再细化检索。适用于探索性查询和不确定性较高的问题。\n   用户问题：“李白的诗风如何？”      可能的 Step-back 查询：      “李白的代表作品有哪些？”      “李白的诗风如何在《将进酒》中体现？”      “李白的诗风受哪些文学流派影响？”   \n\n\n\n**HyDE**：Hypothetical Document Embeddings，这是一种更为复杂的技术，根据用户查询生成假设性的文档形式，将其嵌入，并在检索中使用它们，从而提高检索效果。通过这种转换，系统可以更好地理解用户意图并找到最相关的信息。\n\n\n\n\n简而言之，**Query Translation****通过一系列技术方法将原始问题转化成更易于检索的查询形式，优化检索过程并提升最终的答案质量**。\n\n\n\n\n### **Query Translation 的优势**\n\n<strong>\n</strong>\n\n✅ **提高召回率**：通过 Multi-query、RAG-Fusion 等方法确保检索更全面。\n✅ **提升查询精度**：Step-back 和 HyDE 方法优化查询表达，提高相关性。\n✅ **增强复杂查询的可操作性**：查询分解可以让复杂问题更容易匹配数据库内容。\n\n\n\n\n**QA：****Multi-query****和****RAG-Fusion 都首先根据用户的原始查询生成多个相关的查询，那它们有什么区别呢？******\n\n\n\n\n\n**Multi-query和RAG-Fusion都是处理和扩展用户查询的技术，尽管它们看起来有些相似，但在具体实现和目标上有所不同。**\n\n### ****Multi-query****\n\n**Multi-query****的核心思想是根据用户的一个查询生成多个不同的查询。这些查询可以从不同角度、不同维度或者用不同的表达方式来探讨同一个问题。这样做的目的是扩展检索范围，增加找到相关信息的机会。**\n<li >**目标：扩大检索的覆盖面，确保系统在多个可能的查询方式下都能获取到相关信息。**   <li >**实现：系统会生成多个查询并分别发送到检索模块，获取每个查询的结果。**   <li >**举例：假设用户询问“如何提高机器学习模型的准确性？”系统可能生成以下几个查询：**   <ul class=\"list-paddingleft-1\" ><li >**“提高机器学习模型准确性的技巧”**   <li >**“机器学习精度优化方法”**   <li >**“如何训练更精确的机器学习模型”**   \n### ****RAG-Fusion****\n\n**RAG-Fusion****是一种将多个查询的结果融合在一起的技术。它不仅是生成多个查询，还涉及如何处理这些查询的结果，并将它们合并，以得到一个更为精确和丰富的答案。**\n<li >**目标：通过融合多个查询的结果，获取更加精确、全面的信息，避免单一查询可能带来的信息缺失。**   <li >**实现：多个查询的结果会被整合并过滤，以确保最终输出的结果更具相关性和准确性。RAG-Fusion 不仅关注检索，还关注如何对多个结果进行加权或排序，从中提取最有价值的信息。**   <li >**举例：基于上面的查询，系统可能会将不同查询得到的结果合并（例如从“提高机器学习模型准确性的技巧”中提取到的一条技巧与“机器学习精度优化方法”中提取到的优化方法合并），最后生成一个融合的、更加丰富的回答。**   \n### ****关键区别****\n<li >**Multi-query 主要关注生成多个不同的查询以扩大检索范围，而 RAG-Fusion 则关注如何将这些不同查询的结果有效地融合在一起，得到一个最终的答案。**   <li >**Multi-query是检索层面的扩展，RAG-Fusion 更多是在生成层面上的整合。**   \n\n\n\n**总结来说，Multi-query 着眼于如何增加查询的多样性，而 RAG-Fusion 则是在多个查询的结果合并之后，通过一定的技术手段融合它们，提供一个更准确、更具深度的回答。**\n\n\n\n\n**QA：****RAG-Fusion 中的文****档融合****方法：RRF**\n\n<strong >\n</strong>\n\n**RRF（Reciprocal Rank Fusion）是一种常用的融合方法，在信息检索领域尤其是在 RAG-Fusion 中被用于整合来自不同查询或信息源的检索结果。RRF 的基本思想是根据每个结果的排名（rank）来加权融合多个查询的结果。该方法尤其适用于处理多来源的检索结果，以提高最终答案的质量。**\n\n\n\n\n**Demo：**\n\nhttps://github.com/realyinchen/RAG/tree/main/QueryTranslation\n\n\n\n\n\n\n\n**Routing**\n\n\n\n\n\n\n\n****\n\n**Routing 是指根据用户的查询和相关信息，智能地选择合适的数据源或查询处理方法**。它决定了应该从哪一类数据库、向量存储或检索系统中获取信息。这一过程帮助系统更加高效和精准地定位相关数据源，从而提高最终结果的质量。\n\n\n\n\n### 它包括逻辑路由和语义路由两种方式，前者依赖查询的结构化特征，后者通过查询的语义理解来进行匹配。Routing 技术的引入，可以提升检索效率，确保系统能够精确地为用户提供最相关的信息。\n\n\n\n\n### **1. Logical Routing**\n\n\n\n\nLogical Routing 是基于预定义的规则或者查询的结构化特征来选择合适的数据源或处理路径。其基本思路是根据用户查询的类型或格式，自动决定使用哪个数据源。\n   工作原理：系统通过分析查询的类型或内容，推测查询所需的信息源。例如，如果查询涉及结构化数据，系统可能会选择从关系数据库（如 SQL 数据库）中获取数据；如果查询涉及图数据，系统可能会选择从图数据库（如 Neo4j）中获取数据。      示例：   <ul class=\"list-paddingleft-1\" >   如果用户查询“找出所有销售量最高的商品”，系统会判断这属于结构化查询，选择从关系数据库中获取数据。      如果查询是“查找与某个用户关系最密切的其他用户”，系统可能会选择图数据库，因为图数据库适合处理这类社交关系数据。   \n### \n\n\n### **2. Semantic Routing**\n\n\n\n\nSemantic Routing 则基于查询的语义或内容相似性来决定如何路由。也就是说，系统会分析用户查询的实际语义，并通过查询与预定义的“提示词”或嵌入（Embedding）相似度来选择处理路径。这种方法更多依赖自然语言处理（NLP）技术，通过嵌入向量的方式来实现高效的语义匹配。\n   工作原理：在语义路由中，查询会被转换为向量表示，系统将该向量与存储在数据库中的预定义提示词或模板进行比对。然后，系统根据语义相似度来确定最合适的处理路径或数据源。      示例：   <ul class=\"list-paddingleft-1\" >   如果用户的查询是“推荐给我一些科技书籍”，系统会通过语义理解，识别出这是与书籍推荐相关的查询，因此可以将其路由到包含书籍数据的向量数据库或推荐系统。      如果查询涉及情感分析，系统则可以将查询路由到专门处理情感分析的模块或模型。   \n\n\n\n### **Routing 的优势**\n\n\n\n\n✅ **智能分流，提高检索效率**：通过智能路由，系统可以避免无效的查询路径，直接跳转到最相关的数据源或模型，提高响应速度。\n\n✅**精准匹配**：根据查询的内容和类型，能够选择最匹配的处理方法，确保最终返回的结果具有较高的相关性和质量。\n\n✅ **适应多模态数据**：可以处理结构化数据、非结构化数据和语义搜索任务。\n\n### \n\n\n\n\n**Demo：**\n\nhttps://github.com/realyinchen/RAG/tree/main/Routing\n\n\n\n\n\n\n\n**Query Construction**\n\n\n\n\n\n\n\n\n\n**Query Construction 是指将用户的自然语言查询转换为适用于不同数据库（关系型数据库、图数据库、向量数据库）的结构化查询语句或带有适当过滤条件的查询表示**。这样，整个 RAG 体系能够更智能地处理复杂查询，并从不同类型的数据源中高效检索信息。\n\n\n\n\n### **1. ****关系型数据库（Relational DBs）******\n\n\n\n\n在查询关系型数据库时，Query Construction 需要将自然语言转换为 SQL 语句，以便查询结构化数据。\n\n对于用户问题：“列出销售量大于 1000 的产品。” 会生成以下SQL 查询：\n   \n```\nSELECT*FROMproductsWHEREsales>1000;\n```\n   \n```\nSELECT * FROM products ORDER BY embedding <-> '[query_embedding]' LIMIT 10;\n```\n\n### \n\n\n### **2.****图数据库（Graph DBs）**\n\n\n\n\n对于存储实体关系（如社交网络、知识图谱）的图数据库，Query Construction 需要将自然语言转换为 Cypher 查询（Neo4j 的查询语言）或其他图查询语言，如 Gremlin（适用于 JanusGraph 等）。\n\n\n\n\n对于用户问题：“查找与用户 123 关系最密切的用户。” 会生成以下Cypher 查询：\n               \n```\nMATCH (u:User)-[:FRIEND_WITH]->(f:User)\nWHERE u.id = 123\nRETURN f\nORDER BY f.interaction_score DESC\nLIMIT5;\n```\n   MATCH (u:User)-[:FRIEND_WITH]->(f:User)找到与用户 123 直接相连的用户\n      ORDER BY f.interaction_score DESC 按照交互得分降序排序，确保找到最相关的用户。\n   \n### <strong>\n</strong>\n\n### **3. ****向量数据库（Vector DBs）**\n\n\n\n\n向量数据库用于存储和检索高维向量（通常来自文本、图像或音频的嵌入向量）。在 Query Construction 过程中，自动生成元数据过滤条件（Metadata Filters）：结合用户查询内容，自动附加元数据过滤条件以减少无关的搜索结果，提高搜索精度。\n   纯粹的向量相似性搜索可能会返回一些无关的信息，比如书籍、博客文章等，而元数据过滤能保证只检索论文类的文档。      <p>过滤条件还能提高检索效率，因为数据库可以利用索引加速查询，而不需要对整个数据库进行向量计算。\n</p>   \n\n\n\n对于用户问题：“找一些关于 Transformer 论文的摘要。”，则常规向量搜索：\n   \n```\nvector_db.search(query_embedding, top_k=10)\n```\n\n这里 query_embedding 是通过 BERT 或 OpenAI Embeddings 等模型计算得到的向量表示。\n\n\n\n\n结合元数据过滤的查询：\n               \n```\nvector_db.search(\n    query_embedding, \n    top_k=10, \n    filters={\"category\": \"paper\", \"year\": {\"$gte\": 2020}}\n)\n```\n   只查找 category=paper 的文档，排除其他类别的内容；      仅搜索 2020 年及之后的论文，避免返回过时的信息。   \n<strong >\n</strong>\n\n******Query Construction 的优势**\n\n<strong>\n</strong>\n\n✅ **更精准的查询**：通过合适的查询构造方法，确保不同数据源返回高质量的结果。\n✅ **支持多种数据库类型**：能够适配结构化数据、关系型数据和向量数据。\n✅ **自动元数据过滤**：在向量数据库查询时自动附加适当的过滤条件，减少噪声，提高相关性。\n✅ **高效查询执行**：合理构造查询语句，使得数据库能更快地响应，提高检索效率。\n\n\n\n\n**Demo：**\n\nhttps://github.com/realyinchen/RAG/tree/main/QueryConstruction\n\n\n\n\n\n\n\n**Indexing**\n\n\n\n\n\n\n\n\n\n\nIndexing 是指将文档组织和优化，以提高检索效率和相关性。在 RAG 系统中，索引过程影响着数据的存储方式、查询匹配效果以及最终的答案质量。\n\n\n\n\nIndexing 主要解决数据组织和优化问题，提升 RAG 系统的查询效率和召回质量：\n   **Chunk Optimization**（语义切分）：优化查询匹配精度。      **Multi-representation Indexing**（多表征索引）：支持关键词+向量检索。      **Specialized Embeddings**（特定领域嵌入）：针对专业知识优化向量检索。      **Hierarchical Indexing**（层级索引）：提高大规模文档检索的效率。   \n## <strong>\n</strong>\n\n## **1. Chunk Optimization（文本切片优化）**\n\n<strong>\n</strong>\n\n文档通常是长篇文本，直接索引完整文档可能导致：\n   **查询匹配不精准**：一个 10 页的论文可能只含有 2 句话与查询相关，但向量检索可能会返回整个论文，使答案不够聚焦。      **计算开销大**：如果索引粒度太大，查询时需要处理的文本过多，影响检索速度。   \n### <strong>\n</strong>\n\n### 比如使用 Semantic Splitter（语义切分器）按照语义单元而不是固定字符数切分文本，使得每个切片（chunk）都包含完整的信息。从而可以：\n\n✅ **提高查询匹配精度**，避免语义断裂。\n✅ **降低计算开销**，减少不必要的文本传输。\n\n## <strong>\n</strong>\n\n## **2. Multi-representation Indexing（多表征索引）**\n\n### \n\n\n通过结合多种表示（例如，文档的摘要、块或完整内容），来在检索和生成过程中取得最佳效果。它通过使用不同层次和粒度的表示（简洁的摘要与详细的文档）来弥补单一表示可能存在的不足。\n\n\n\n\nProposition Indexing 是 Multi-representation 的一种实现方法：\n\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"231\" data-backw=\"548\" data-imgfileid=\"100012002\" data-ratio=\"0.4212962962962963\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7Bu96UsJibpEOeaSTVJGESVOhnEsTuIvug7Dul8u2EP9TiaLsGqicIAsxSibcnJ3Cg5WPnhDK4ficficGNdRQ/640?wx_fmt=other& from=appmsg& tp=webp& wxfrom=5& wx_lazy=1& wx_co=1\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n   我们使用 LLM 对原始文本进行总结形成摘要（Summary）；      对摘要进行嵌入处理，保存到 Vector Store 中；同时也将原始文本保存到 Doc Store 中；      检索时，根据用户的 Question 嵌入去 Vector Store 中检索对应的摘要，然后返回原始的文本；   \n\n\n\n✅**对长上下文的 LLM 来说更加友好**，因为甚至可以将整篇原始文档输入 LLM 进行回答。\n\n\n\n\n**3. Specialized Embeddings（特定领域嵌入）**\n\n\n\n\n通用向量嵌入（如 OpenAI Embeddings）在一些特定领域（如法律、医学）可能表现不佳，原因是：\n   预训练数据中**缺乏领域知识**，导致向量表达偏离专业概念。      语义相似度不一定适合特定任务，比如在法律文本中，“判例 A” 和 “判例 B” 可能需要特别区分，而普通 NLP 可能会认为它们相似。   \n\n\n\n我们可以使用领域数据对嵌入模型进行**Fine-tuning**（微调）或使用专门的检索优化模型（如 **ColBERT**）。从而可以：\n\n✅ **针对专业领域优化**，提高搜索精度。\n\n✅ **避免通用模型的语义偏差**，更适合专业知识库。\n\n\n\n\n到目前为止，我们所学的各种 RAG 技巧都是基于匹配文档（块）和问题的语义相似度来实现的，更具体地说：我们直接对整篇文档或者文档块与用户问题进行嵌入处理，比较处理之后的嵌入向量之间的相似程度。\n\n\n\n\n但是有人说用户提出的问题可能是各种各样的，如果仅仅将整篇文档或者文档块压缩成一个向量表示，那么这未免太过于笼统了。\n\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"316\" data-backw=\"548\" data-imgfileid=\"100012004\" data-ratio=\"0.5768518518518518\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7Bu8nKU3GjTh6GQJ7rGq1FpzSnCKRxiamy7JRibgWX738YnibjFdYDJ3ZIJfOyXic60bXt3dYIAF8dPBGDA/640?wx_fmt=other& from=appmsg& tp=webp& wxfrom=5& wx_lazy=1& wx_co=1\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\nColBERT（Contextualized Late Interaction over BERT），基于 BERT，实现了细粒度的 token 级嵌入相似性计算。\n\n\n\n\n以下是具体的步骤和原理：\n\n1. Token 级别相似度计算\n\n<img class=\"rich_pages wxw-img\" data-backh=\"200\" data-backw=\"548\" data-imgfileid=\"100012006\" data-ratio=\"0.36562203228869894\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7Bu8nKU3GjTh6GQJ7rGq1FpzSfrqhFN0KFJfTOYVV1a8ElJuGhbIvceeKiaicafVNNeKOJzBTx10lSSOw/640?wx_fmt=other& from=appmsg& tp=webp& wxfrom=5& wx_lazy=1& wx_co=1\" data-w=\"1053\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\n2. 最大池化（MaxSim）操作\n\n<img class=\"rich_pages wxw-img\" data-backh=\"265\" data-backw=\"548\" data-imgfileid=\"100012007\" data-ratio=\"0.48333333333333334\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7Bu8nKU3GjTh6GQJ7rGq1FpzSVicS2YxOpRukzGhrO4DSvUzzZgJ3SmkH8llVSYibMPxPQBGgBUsaQHVw/640?wx_fmt=other& from=appmsg& tp=webp& wxfrom=5& wx_lazy=1& wx_co=1\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\n3. 选择最相关的文档\n\n<img class=\"rich_pages wxw-img\" data-backh=\"54\" data-backw=\"548\" data-imgfileid=\"100012009\" data-ratio=\"0.09814814814814815\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7Bu8nKU3GjTh6GQJ7rGq1FpzSqN6gZSorlsawfcFKL4pEnZgndM9HBADVe5jypKz9Sj3ibjId6wLHZKA/640?wx_fmt=other& from=appmsg& tp=webp& wxfrom=5& wx_lazy=1& wx_co=1\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\n**ColBERT 的开源实现：**\n\nhttps://github.com/stanford-futuredata/ColBERT\n\n\n****\n\n**关于具体的代码案例，可以参考：**\n\nhttps://python.langchain.com/docs/integrations/retrievers/ragatouille\n\n## <strong>\n</strong>\n\n## **4. Hierarchical Indexing（层级索引）**\n\n\n\n\n对于大型文档（如书籍、论文集合），直接索引所有文本会导致：\n   **检索结果过多**，难以找到真正相关的信息。      **计算成本高**，每次查询都要处理大量数据。   \n\n\n\nRAPTOR（Recursive Abstractive Processing for Tree-Organized Retrieval）是一种将文档组织成分层结构的索引技术，它能够有效地实现递归式的信息抽象和检索。这种方法以“树”的形式组织文档数据，叶节点代表原始文档或者文档块，树的高层节点则代表这些文档经过抽象后的总结。RAPTOR 的目标是通过分层的方式，提升信息检索的效率，同时能够适应不同粒度的查询需求。\n\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"421\" data-backw=\"548\" data-imgfileid=\"100012010\" data-ratio=\"0.7688888888888888\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7BuicCkF5byH5GDibdC9BYwf0tWDhUgY9UdZCxJBdx9y3ZPzYfK5naqvAIvRsNYib8OeFn993VNE1FaHVg/640?wx_fmt=other& from=appmsg& tp=webp& wxfrom=5& wx_lazy=1& wx_co=1\" data-w=\"900\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\n****\n\n**Demo：**\n\nhttps://github.com/realyinchen/RAG/tree/main/Indexing\n\n\n\n\n\n\n\n**Retrieval&  Generation**\n\n\n\n\n\n\n\n## \n\n## \n\n## **🔍 Retrieval &  Generation —— RAG 的双引擎 🚀**\n\n\n\n\nRetrieval 和 Generation 是互补的两个核心环节，共同决定最终答案的质量。一个好的 RAG 系统，既要确保检索到**最相关的信息**，又要让生成模型能够充分利用这些信息，生成**高质量、可信的回答**。\n\n\n\n\n### **📌 Retrieval &  Generation 的核心目标**\n\n✅ **Retrieval**：找到最相关的外部信息，为 LLM 生成答案提供支持。\n✅ **Generation**：基于检索到的内容，生成连贯、准确、有事实依据的回答。\n\n# <strong>\n</strong>\n\n# **🟢 Retrieval：找到最相关的信息**\n\nRetrieval 负责从外部知识库（如 VectorDB、SQL 数据库、文件系统）中检索到**与用户查询最相关的信息**，并将其提供给 LLM 进行生成。\n\n\n\n\n📌 **核心目标**：\n   **高召回率（Recall）**：确保不会遗漏重要信息。      **高精准度（Precision）**：减少无关或冗余的内容。      **可扩展性（Scalability）**：支持大规模数据检索，提高系统效率。   \n### <strong>\n</strong>\n\n### **🔹 核心技术**\n\n\n\n\n🔹 **1. RAG Fusion**\n\n> 通过不同角度生成多个查询，提高召回率，并使用 Reciprocal Rank Fusion（RRF）融合检索结果。✅ **优势**：\n   适用于复杂问题，避免遗漏关键信息。      结合多个查询方式，提高搜索的全面性。   \n\n\n\n🔹 **2. Re-Rank（重排序）**\n\n> <p>召回初步结果后，使用更强的排序方法（如 Cross-Encoder、RankGPT）进行二次排序，确保最相关的文档排在前面。\n✅ **优势**：</p>\n   提高文档相关性，减少 LLM 处理无关信息的负担。      适用于开放领域问答，提高答案质量。   \n\n\n\n🔹 **3. Hybrid Retrieval（混合检索）**\n\n> <p>结合**向量搜索（Vector Search）、关键字搜索（BM25）、知识图谱（Knowledge Graph）**，以多模态方式找到最佳结果。\n✅ **优势**：</p>\n   兼顾语义匹配和关键字匹配，减少错误召回。      适用于结构化 + 非结构化数据的场景。   \n\n\n\n🔹 **4. Active Retrieval（主动检索）**\n\n> <p>让 LLM 评估初步检索结果，决定是否需要额外查询，以提高答案的完整性。\n✅ **优势**：</p>\n   避免因信息不足导致的错误回答。      适用于需要多轮查询的复杂任务。   \n# <strong>\n</strong>\n\n# **🟠 Generation：基于检索信息生成高质量答案**\n\nGeneration 负责利用 Retrieval 提供的信息，生成准确、连贯、有事实依据的回答。\n\n\n\n\n📌 **核心目标**：\n   **增强事实性（Fact-enhancement）**：减少幻觉，提高答案可信度。      **提高推理能力（Reasoning）**：让 LLM 具备跨文档、多步骤推理能力。      **提高可控性（Controllability）**：确保答案符合特定格式或标准。   \n### <strong>\n</strong>\n\n### **🔹 核心技术**\n\n\n\n\n🔹 **1. <strong >CRAG**（**Corrective RAG**，纠正型 RAG）</strong>\n\n<strong>\n</strong>\n\n<strong><img class=\"rich_pages wxw-img\" data-backh=\"235\" data-backw=\"548\" data-imgfileid=\"100012011\" data-ratio=\"0.42777777777777776\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7BuibYWhC3zNnO2U60tKPmrvVV0klVAt4yKv33rDibia8eDCqZaJWyVsddxEfWpwvDUz97G0pf3BUI8BhQ/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n</strong>\n\n> <p>LLM 评估检索到的文档是否相关，并根据需要触发额外检索来补充检索数据。\n✅ **优势**：</p>\n   减少错误答案，提高答案质量。      适用于法律、医学、金融等领域，确保答案符合行业标准。   \n\n\n\n🔹 **2. Self-RAG（<strong >Self-Reflective RAG，**自我反思 RAG）</strong>\n\n<strong>\n</strong>\n\n<strong><img class=\"rich_pages wxw-img\" data-backh=\"203\" data-backw=\"548\" data-imgfileid=\"100012012\" data-ratio=\"0.3712962962962963\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7BuibYWhC3zNnO2U60tKPmrvVVlicKVnDTFRHuwTVK9eWytbibaUMdSOYrrA7O2icKNITSnUHAoR4SyCR3A/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n</strong>\n\n> <p>让 LLM 生成初步答案后，进行自我检查，并决定是否需要额外检索或调整答案结构。\n✅ **优势**：</p>\n   提高生成的连贯性和合理性。      适用于开放领域问答，提高 LLM 的“自我意识”。   \n\n\n\n**🔹 3. Adaptive-RAG（自适应 RAG）**\n\n<strong >\n</strong>\n\n<strong ><img class=\"rich_pages wxw-img\" data-backh=\"287\" data-backw=\"548\" data-imgfileid=\"100012014\" data-ratio=\"0.524074074074074\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/SaeK9tW7BuibYWhC3zNnO2U60tKPmrvVVHiabYPvYaNd7sRib40tHUMeIC8nib47ia9HLM6U0kyzxWicrV6jckLvYZ4Q/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n</strong>\n\n<strong >\n</strong>\n\n> <p>系统根据查询的复杂度和要求自适应地选择检索和生成的方式。\n✅**优势**：</p>\n   **提高效率**：通过自适应地选择检索和生成的策略，减少不必要的计算，提升处理速度。      **增强准确性**：动态评估查询的需求，避免生成无关或低质量的答案。      **灵活应对多样化查询**：适应不同领域和复杂度的查询，确保答案的质量和适用性。      **减少计算开销**：针对简单问题，直接检索而不进行多轮生成，减少不必要的计算资源消耗。   \n<strong >\n</strong>\n\n# <strong>\n</strong>\n\n# **📢 未来趋势**\n\n1️⃣ **混合检索（Hybrid Retrieval）**：结合向量搜索 + 关键字搜索 + 知识图谱，提高召回质量\n2️⃣ **自适应（Adaptive RAG）**：LLM 根据需求动态决定是否检索，提高系统效率\n3️⃣ **端到端优化（End-to-End RAG）**：将检索与生成集成训练，提高整体效果\n4️⃣ **增强事实性（Fact-Enhanced Generation）**：结合检索结果进行交叉验证，减少幻觉\n\n\n\n\n****\n\n**Demo：**\n\nhttps://github.com/realyinchen/RAG/tree/main/AgenticRAG\n\n\n\n\n\n\n\n\n\n\n\n\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2025-02-10T08:51:37Z"
  },
  {
    "id": 26,
    "title": "\n\nRAG应用在得物开放平台的智能答疑的探索\n",
    "url": "https://mp.weixin.qq.com/s/6yhYLKfNrumSMs7ELvktjg",
    "content": "\n<img class=\"rich_pages wxw-img\" data-imgfileid=\"100040703\" data-ratio=\"0.18232558139534882\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_gif/AAQtmjCc74DZeqm2Rc4qc7ocVLZVd8FOASKicbMfKsaziasqIDXGPt8yR8anxPO3NCF4a4DkYCACam4oNAOBmSbA/640?wx_fmt=gif\" data-type=\"gif\" data-w=\"1075\"  referrerpolicy=\"no-referrer\">\n\n\n\n**目录**\n\n一、背景\n\n二、简介\n\n  1. 什么是RAG\n\n  2. RAG应用的可落地场景\n\n  3. RAG应用的主要组成部分\n\n  4.RAG应用的核心流程\n\n三、实现目标\n\n四、整体流程\n\n  1. 技术选型\n\n  2. 准确性思考\n\n  3. 用户提问结构化\n\n  4.数据预处理与向量库的准备工作\n\n  5.CO-STAR结构\n\n  6.相似性搜索\n\n  7. 用户提问解答\n\n  8.Runnable的结合\n\n  9. 流程串联\n\n五、应用测试\n\n六、未来展望\n\n\n\n\n**一**\n\n**背景**\n\n得物开放平台是一个把得物能力进行开放，同时提供给开发者提供 公告、应用控制台、权限包申请、业务文档等功能的平台。\n   面向商家：通过接入商家自研系统。可以实现自动化库存、订单、对账等管理。      面向ISV ：接入得物开放平台，能为其产品提供更完善的全平台支持。      面向内部应用：提供安全、可控的、快速支持的跨主体通讯。   \n\n\n\n得物开放平台目前提供了一系列的文档以及工具去辅助开发者在实际调用API之前进行基础的引导和查询。\n\n\n\n\n但目前的文档搜索功能仅可以按照接口路径，接口名称去搜索，至于涉及到实际开发中遇到的接口前置检查，部分字段描述不清等实际问题，且由于信息的离散性，用户想要获得一个问题的答案需要在多个页面来回检索，造成用户焦虑，进而增大TS的答疑可能性。\n\n\n\n\n随着这几年AI大模型的发展，针对离散信息进行聚合分析且精准回答的能力变成了可能。而RAG应用的出现，解决了基础问答类AI应用容易产生幻觉现象的问题，达到了可以解决实际应用内问题的目标。\n\n\n\n\n**二**\n\n**简介**\n\n**什么是RAG**\n\nRAG（检索增强生成）指Retrieval Augmented Generation。\n\n\n\n\n这是一种通过从外部来源获取知识来提高生成性人工智能模型准确性和可靠性的技术。通过RAG，用户实际上可以与任何数据存储库进行对话，这种对话可视为“开卷考试”，即让大模型在回答问题之前先检索相关信息。\n\n\n\n\n**RAG应用的可落地场景**\n\nRAG应用的根本是依赖一份可靠的外部数据，根据提问检索并交给大模型回答，任何基于可靠外部数据的场景均是RAG的发力点。\n\n\n\n\n**RAG应用的主要组成部分**\n   外部知识库：问题对应的相关领域知识，该知识库的质量将直接影响最终回答的效果。      Embedding模型：用于将外部文档和用户的提问转换成Embedding向量。      向量数据库：将外部信息转化为Embedding向量后进行存储。      检索器：该组件负责从向量数据库中识别最相关的信息。检索器将用户问题转换为Embedding向量后执行相似性检索，以找到与用户查询相关的Top-K文档（最相似的K个文档）。      生成器（大语言模型LLM）：一旦检索到相关文档，生成器将用户查询和检索到的文档结合起来，生成连贯且相关的响应。      提示词工程（Prompt Engineering）：这项技术用于将用户的问题与检索到的上下文有效组合，形成大模型的输入。   \n\n\n\n**RAG应用的核心流程**\n\n以下为一个标准RAG应用的基础流程：\n   将查询转换为向量      在文档集合中进行语义搜索      将检索到的文档传递给大语言模型生成答案      从生成的文本中提取最终答案   \n\n\n\n但在实际生产中，为了确保系统的全面性、准确性以及处理效率，还有许多因素需要加以考虑和处理。\n\n\n\n\n下面我将基于答疑助手在开放平台的落地，具体介绍每个步骤的详细流程。\n\n\n\n\n**三**\n\n**实现目标**\n\n鉴于目前得物开放平台的人工答疑数量相对较高，用户在开放平台查询未果就会直接进入到人工答疑阶段。正如上文所说，RAG擅长依赖一份可靠的知识库作出相应回答，构建一个基于开放平台文档知识库的RAG应用再合适不过，同时可以一定程度降低用户对于人工答疑的依赖性，做到问题前置解决。\n\n\n\n\n**四**\n\n**整体流程**\n\n\n\n\n**技术选型**\n   大模型：https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/      Embedding模型：https://platform.openai.com/docs/guides/embeddings      向量数据库：https://milvus.io/      框架：https://js.langchain.com/v0.2/docs/introduction/LangChain.js是LangChain的JavaScript版本，专门用于开发LLM相关的交互应用程序，其Runnable设计在开放平台答疑助手中广泛应用，在拓展性、可移植性上相当强大。   \n\n\n\n**准确性思考**\n\n问答的准确性会直接反馈到用户的使用体验，当一个问题的回答是不准确的，会导致用户根据不准确的信息进一步犯错，导致人工客服介入，耐心丧失直至投诉。\n\n\n\n\n所以在实际构建基于开放平台文档的答疑助手之前，首先考虑到的是问答的准确性，主要包括以下2点：\n   **首要解决答疑助手针对非开放平台提问的屏蔽**      **寻找可能导致答非所问的时机以及相应的解决方案**   \n\n\n\n**屏蔽非相关问题**\n\n为了屏蔽AI在回答时可能会回答一些非平台相关问题，我们首先要做的是让AI明确我们的目标（即问答上下文），且告诉他什么样的问题可以回答，什么问题不可以回答。\n\n\n\n\n在这一点上，常用的手段为告知其什么是开放平台以及其负责的范畴。\n\n\n\n\n**例如：得物的开放平台是一个包含着 API 文档，解决方案文档的平台，商家可以通过这个平台获取到得物的各种接口，以及解决方案，帮助商家更好的使用得物的服务。现在需要做一个智能答疑助手，你是其中的一部分。**\n\n\n\n\n在这一段描述中，我们告知了答疑助手，开放平台包含着API文档，包含着解决方案，同时包含接口信息，同时会有商家等之类的字眼。大模型在收到这段上下文后，将会对其基础回答进行判断。\n\n\n\n\n同时，我们可以通过让答疑助手二选一的方式进行回答，即平台相关问题与非平台相关问题。我们可以让大模型返回特定的数据枚举，且限定枚举范围，例如：开放平台通用问题、开放平台API答疑问题，未知问题。\n\n\n\n\n**借助Json类型的输出 + JSON Schema，我们可通过Prompt描述来限定其返回，从而在进入实际问答前做到事前屏蔽。**\n\n\n\n\n**寻找可能导致答非所问的时机**\n\n当问题被收拢到开放平台这个主题之后，剩余的部分就是将用户提问与上下文进行结合，再交由大模型回答处理。在这过程中，可能存在的答非所问的时机有：不够明确的Prompt说明、上下文信息过于碎片化以及上下文信息的连接性不足三种。\n\n\n\n   **不够明确的Prompt说明：**Prompt本身描述缺少限定条件，导致大模型回答轻易超出我们给予的要求，从而导致答非所问。      **上下文信息过于碎片化：**上下文信息可能被分割成N多份，这个N值过大或者过小，都会导致单个信息过大导致缺乏联想性、单个信息过小导致回答时不够聚焦。      **上下文信息连接性不够：**若信息之间被随意切割，且缺少相关元数据连接，交给大模型的上下文将会是丧失实际意义的文本片段，导致无法提取出有用信息，从而答非所问。   \n\n\n\n为了解决以上问题，在设计初期，开放平台答疑助手设定了以下策略来前置解决准确性问题：\n   用户提问的结构化      向量的分割界限以及元信息处理      CO-STAR Prompt结构      相似性搜索的K值探索   \n\n\n\n**用户提问结构化**\n\n目标：通过大模型将用户提问的结构化，将用户提问分类并提取出精确的内容，便于提前引导、终止以及提取相关信息。\n\n\n\n\n**例如，用户提问今天天气怎么样，结构化Runnable会将用户问题进行初次判断。**\n\n\n\n\n一个相对简单的Prompt实现如下：\n                                                                                    \n```\n# CONTEXT\n得物的开放平台是一个包含着 API 文档，解决方案文档的平台，商家可以通过这个平台获取到得物的各种接口，以及解决方案，帮助商家更好的使用得物的服务。现在需要做一个智能答疑助手，你是其中的一部分。\n\n\n# OBJECTIVE\n你现在扮演一名客服。请将每个客户问题分类到固定的类别中。\n你只接受有关开放平台接口的相关问答，不接受其余任何问题。\n具体的类别我会在提供给你的JSON Schema中进行说明。\n\n\n# STYLE\n\n\n你需要把你的回答以特定的 JSON 格式返回\n\n\n# TONE\n\n\n你给我的内容里，只能包含特定 JSON 结构的数据，不可以返回给我任何额外的信息。\n\n\n# AUDIENCE\n\n\n你的回答是给机器看的，所以不需要考虑任何人类的感受。\n\n\n# RESPONSE\n\n\n你返回的数据结构必须符合我提供的 JSON Schema 规范，我给你的 Schema 将会使用\\`<json-schema></json-schema>\\`标签包裹.\n每个字段的描述，都是你推算出该字段值的依据，请仔细阅读。\n\n\n<json-schema>\n  {schema}\n</json-schema>\n```\n\n\n\n\nJson Schema的结构通过zod描述如下：\n                                                                              \n```\n\n\nconst zApiCallMeta = z\n  .object({\n    type: z\n      .enum(['api_call'， 'unknown', 'general'])\n      .describe('当前问题的二级类目, api_call为API调用类问题，unknown为非开放平台相关问题, general为通用类开放平台问题'),\n    apiName: z\n      .string()\n      .describe(\n        '接口的名称。接口名称为中文，若用户未给出明确的API中文名称，不要随意推测，将当前字段置为空字符串',\n      ),\n    apiUrl: z.string().describe('接口的具体路径, 一般以/开头'),\n    requestParam: z.unknown().default({}).describe('接口的请求参数'),\n    response: z\n      .object({})\n      .or(z.null())\n      .default({})\n      .describe('接口的返回值，若未提供则返回null'),\n    error: z\n      .object({\n        traceId: z.string(),\n      })\n      .optional()\n      .describe('接口调用的错误信息，若接口调用失败，则提取traceId并返回'),\n  })\n  .describe('当二级类目为api_call时，使用这个数据结构');\n```\n\n\n\n\n以上结构，将会对用户的问题输入进行结构化解析。同时给出相应JSON数据结构。\n\n\n\n\n将以上结构化信息结合，可实现一个基于LangChain.js的结构化Runnable，在代码结构设计上，所有的Runnable将会使用$作为变量前缀，用于区分Runnable与普通函数。\n                                                                                                                  \n```\nimport { ChatOpenAI } from '@langchain/openai';\nimport { StringOutputParser } from '@langchain/core/output_parsers';\nimport { RunnableSequence, RunnableMap } from '@langchain/core/runnables';\nimport { $getPrompt } from './$prompt';\nimport { zSchema, StructuredInputType } from './schema';\nimport { n } from 'src/utils/llm/gen-runnable-name';\nimport { getLLMConfig } from 'src/utils/llm/get-llm-config';\nimport { getStringifiedJsonSchema } from 'src/utils/llm/get-stringified-json-schema';\n\n\nconst b = n('$structured-input');\n\n\nconst $getStructuredInput = () => {\n  const $model = new ChatOpenAI(getLLMConfig().ChatOpenAIConfig).bind({\n    response_format: {\n      type: 'json_object',\n    },\n  });\n\n\n  const $input = RunnableMap.from<{ question: string }>({\n    schema: () => getStringifiedJsonSchema(zSchema),\n    question: (input) => input.question,\n  }).bind({ runName: b('map') });\n\n\n  const $prompt = $getPrompt();\n  const $parser = new StringOutputParser();\n\n\n  return RunnableSequence.from<{ question: string }, string>([\n    $input.bind({ runName: b('map') }),\n    $prompt.bind({ runName: b('prompt') }),\n    $model,\n    $parser.bind({ runName: b('parser') }),\n  ]).bind({\n    runName: b('chain'),\n  });\n};\n\n\nexport { $getStructuredInput, type StructuredInputType };\n\n\n```\n\n\n\n\n鉴于CO-STAR以及JSONSchema的提供的解析稳定性，此Runnable甚至具备了可单测的能力。\n                                                                                                         \n```\nimport dotenv from 'dotenv';\ndotenv.config();\nimport { describe, expect, it } from 'vitest';\nimport { zSchema } from '../runnables/$structured-input/schema';\nimport { $getStructuredInput } from '../runnables/$structured-input';\n\n\nconst call = async (question: string) => {\n  return zSchema.safeParse(\n    JSON.parse(await $getStructuredInput().invoke({ question })),\n  );\n};\n\n\ndescribe('The LLM should accept user input as string, and output as structured data', () => {\n  it('should return correct type', { timeout: 10 * 10000 }, async () => {\n    const r1 = await call('今天天气怎么样');\n    expect(r1.data?.type).toBe('unknown');\n    const r2 = await call('1 + 1');\n    expect(r2.data?.type).toBe('unknown');\n    const r3 = await call('trace: 1231231231231231313');\n    expect(r3.data?.type).toBe('api_call');\n    const r4 = await call('快递面单提示错误');\n    expect(r4.data?.type).toBe('api_call');\n    const r5 = await call('发货接口是哪个');\n    expect(r5.data?.type).toBe('api_call');\n    const r6 = await call('怎么发货');\n    expect(r6.data?.type).toBe('general');\n    const r7 = await call('获取商品详情');\n    expect(r7.data?.type).toBe('api_call');\n    const r8 = await call('dop/api/v1/invoice/cancel_pick_up');\n    expect(r8.data?.type).toBe('api_call');\n    const r9 = await call('开票处理');\n    expect(r9.data?.type).toBe('api_call');\n    const r10 = await call('权限包');\n    expect(r10.data?.type).toBe('api_call');\n  });\n```\n\n\n\n\n**数据预处理与向量库的准备工作**\n\nRAG应用的知识库准备是实施过程中的关键环节，涉及多个步骤和技术。以下是知识库准备的主要过程：\n\n\n\n   **知识库选择：**【全面性与质量】数据源的信息准确性在RAG应用中最为重要，基于错误的信息将无法获得正确的回答。      **知识库收集：**【多类目数据】数据收集通常涉及从多个来源提取信息，包括不同的渠道，不同的格式等。如何确保数据最终可以形成统一的结构并被统一消费至关重要。      **数据清理：**【降低额外干扰】原始数据往往包含不相关的信息或重复内容。      **知识库分割：**【降低成本与噪音】将文档内容进行分块，以便更好地进行向量化处理。每个文本块应适当大小，并加以关联，以确保在检索时能够提供准确的信息，同时避免生成噪声。      **向量化存储：**【Embedding生成】使用Embedding模型将文本块转换为向量表示，这些向量随后被存储在向量数据库中，以支持快速检索。      **检索接口构建：**【提高信息准确性】构建检索模块，使其能够根据用户查询从向量数据库中检索相关文档。   \n\n\n\n**知识库拆分**\n\n知识库文档的拆分颗粒度（Split Chunk Size) 是影响RAG应用准确性的重要指标：\n   拆分颗粒度过大可能导致检索到的文本块包含大量不相关信息，从而降低检索的准确性。      拆分颗粒度过小则可能导致必要的上下文信息丢失，使得生成的回答缺乏连贯性和深度。      在实际应用中，需要不断进行实验以确定最佳分块大小。通常情况下，128字节大小的分块是一个合适的分割大小。      同时还要考虑LLM的输入长度带来的成本问题。   \n\n\n\n下图为得物开放平台【开票取消预约上门取件】接口的接口文档：\n\n**开票取消预约上门取件接口信息**\n\n\n\n\n**拆分逻辑分析（**根据理论提供128字节大小）\n\n\n\n\n在成功获取到对应文本数据后，我们需要在数据的预处理阶段，将文档根据分类进行切分。这一步将会将一份文档拆分为多份文档。\n\n\n\n\n由上图中信息可见，一个文档的基础结构是由一级、二级标题进行分割分类的。一个基本的接口信息包括：基础信息、请求地址、公共参数、请求入参、请求出参、返回参数以及错误码信息组成。\n\n\n\n\n**拆分方式**\n\n<strong>\n</strong>\n\n拆分的实现一般有2种，**一是根据固定的文档大小进行拆分（128字节）二是根据实际文档结构自己做原子化拆分。**\n\n\n\n\n直接根据文档大小拆分的优点当然是文档的拆分处理逻辑会直接且简单粗暴，缺点就是因为是完全根据字节数进行分割，一段完整的句子或者段落会被拆分成2半从而丢失语义（但可通过页码进行链接解决）。\n\n\n\n\n根据文档做结构化拆分的优点是上下文结构容易连接，单个原子文档依旧具备语义化，检索时可以有效提取到信息，缺点是拆分逻辑复杂具备定制性，拆分逻辑难以与其他知识库复用，且多个文档之间缺乏一定的关联性（但可通过元信息关联解决）。\n\n\n\n\n在得物开放平台的场景中，**因为文档数据大多以json为主（例如api表格中每个字段的名称、默认值、描述等），将这些json根据大小做暴力切分丢失了绝大部分的语义，难以让LLM理解。**所以，我们选择了第二种拆分方式。\n\n\n\n\n**拆分实现**\n\n<strong>\n</strong>\n\n在文档分割层面，Markdown作为一种LLM可识别且可承载文档元信息的文本格式，作为向量数据的基础元子单位最为合适。\n\n基础的文档单元根据大标题进行文档分割，同时提供frontmatter作为多个向量之间连接的媒介。\n\n\n\n\n正文层面，开放平台的API文档很适合使用Markdown Table来做内容承接，且Table对于大模型更便于理解。\n\n\n\n\n根据以上这种结构，我们可得到以下拆分流程：\n\n\n\n\n代码实现：\n                                                                                                                        \n```\n const hbsTemplate = `\n---\n服务ID (serviceId): {{ service.id }}\n接口ID (apiId): {{ apiId }}\n接口名称 (apiName): {{ apiName }}\n接口地址 (apiUrl): {{ apiUrl }}\n页面地址 (pageUrl): {{ pageUrl }}\n---\n\n\n# {{ title }}\n\n\n{{ paragraph }}\n`;\nexport const processIntoEmbeddings = (data: CombinedApiDoc) => {\n  const template = baseTemplate(data);\n\n\n  const texts = [\n    template(requestHeader(data)),\n    template(requestUrl(data)),\n    template(publicRequestParam(data)),\n    template(requestParam(data)),\n    template(responseParam(data)),\n    template(errorCodes(data)),\n    template(authPackage(data)),\n  ].filter(Boolean) as string[][];\n\n\n  return flattenDeep(texts).map((content) => {\n    return new Document<MetaData>({\n      // id: toString(data.apiId!),\n      metadata: {\n        serviceId: data.service.id,\n        apiId: data.apiId!,\n        apiName: data.apiName!,\n        apiUrl: data.apiUrl!,\n        pageUrl: data.pageUrl!,\n      },\n      pageContent: content!,\n    });\n  });\n};\n```\n\n\n\n\n**知识库导入**\n\n通过建立定时任务（DJOB），使用MILVUS sdk将以上拆分后的文档导入对应数据集中。\n\n\n\n\n**CO-STAR结构**\n\n在上文中的Prompt，使用了一种名为CO-STAR的结构化模板，该框架由新加坡政府科技局的数据科学与AI团队创立。**CO-STAR框架是一种用于设计Prompt的结构化模板，旨在提高大型语言模型（LLM）响应的相关性和有效性，考虑了多种影响LLM输出的关键因素。**\n\n\n\n\n结构：\n   上下文（Context）：提供与任务相关的背景信息，帮助LLM理解讨论的具体场景，确保其响应具有相关性。      目标（Objective）：明确你希望LLM执行的具体任务。清晰的目标有助于模型聚焦于完成特定的请求，从而提高输出的准确性。      风格（Style）：指定希望LLM采用的写作风格。这可以是某位名人的风格或特定职业专家的表达方式，甚至要求LLM不返回任何语气相关文字，确保输出符合要求。      语气（Tone）：设定返回的情感或态度，例如正式、幽默或友善。这一部分确保模型输出在情感上与用户期望相符。      受众（Audience）：确定响应的目标受众。根据受众的不同背景和知识水平调整LLM的输出，使其更加适合特定人群。      响应（Response）：规定输出格式，以确保LLM生成符合后续使用需求的数据格式，如列表、JSON或专业报告等。这有助于在实际应用中更好地处理LLM的输出。   \n\n\n\n在上文结构化的实现中，演示了如何使用CO-STAR结构的Prompt，要求大模型“冰冷的”对用户提问进行的解析，当然CO-STAR也适用于直接面向用户的问答，例如：\n                                                   \n```\n## Context\n我是一名正在寻找酒店信息的旅行者，计划在即将到来的假期前往某个城市。我希望了解关于酒店的设施、价格和预订流程等信息。\n\n\n## Objective\n请提供我所需的酒店信息，包括房间类型、价格范围、可用设施以及如何进行预订。\n\n\n## Style\n请以简洁明了的方式回答，确保信息易于理解。\n\n\n## Tone\n使用友好和热情的语气，给人一种欢迎的感觉。\n\n\n## Audience\n目标受众是普通旅行者，他们可能对酒店行业不太熟悉。\n\n\n## Response\n请以列表形式呈现每个酒店的信息，包括名称、地址、房间类型、价格和联系方式。每个酒店的信息应简短且直接，便于快速浏览。\n```\n\n\n\n\n**相似性搜索**\n\n当我们使用了问题结构化Runnable后，非开放平台类问题将会提前终止，告知用户无法解答相关问题，其他有效回答将会进入相似性搜索环节。\n\n\n\n\n相似性搜索基于数据之间的相似性度量，通过计算数据项之间的相似度来实现检索。在答疑助手的相似性实现是通过余弦相似度来进行相似性判断的。\n\n\n\n\n我们将用户的提问，与向量数据库中数据进行余弦相似度匹配。取K为5获取最相似的五条记录。\n\n\n\n\n**注意：此K值是经过一系列的推断最终决定的，可根据实际情况调整。**\n                                                                  \n```\nimport { Milvus } from '@langchain/community/vectorstores/milvus';\nimport { OpenAIEmbeddings } from '@langchain/openai';\nimport { RunnableSequence } from '@langchain/core/runnables';\nimport { getLLMConfig } from 'src/utils/llm/get-llm-config';\n\n\nexport const $getContext = async () => {\n  const embeddings = new OpenAIEmbeddings(\n    getLLMConfig().OpenAIEmbeddingsConfig,\n  );\n\n\n  const vectorStore = await Milvus.fromExistingCollection(embeddings, {\n    collectionName: 'open_rag',\n  });\n\n\n  return RunnableSequence.from([\n    (input) => {\n      return input.question;\n    },\n    vectorStore.asRetriever(5),\n  ]);\n};\n\n\n```\n\n\n\n\n此Runnable会将搜索结果组成一大段可参考数据集，用于后续用户提问。\n\n\n\n\n**用户提问解答**\n\n用户提问的解答同样通过Runnable的方式来承接，通过用户提问、结构化数据、提取的相似性上下文进行结合，最终得到问题的解答。\n\n\n\n\n我们先将上下文进行格式化整理：\n                                                                           \n```\nimport { RunnablePassthrough, RunnablePick } from '@langchain/core/runnables';\nimport { Document } from 'langchain/document';\nimport { PromptTemplate } from '@langchain/core/prompts';\nimport { MetaData } from 'src/types';\n\n\nconst $formatRetrieverOutput = async (documents: Document<MetaData>[]) => {\n  const strings = documents.map(async (o) => {\n    const a = await PromptTemplate.fromTemplate(`{pageContent}`).format({\n      pageContent: o.pageContent,\n    });\n\n\n    return a;\n  });\n\n\n  const context = (await Promise.all(strings)).join('\\n');\n\n\n  return context;\n};\n\n\nexport const $contextAssignRunnable = () => {\n  return RunnablePassthrough.assign({\n    context: new RunnablePick('context').pipe($formatRetrieverOutput),\n  });\n};\n\n\n```\n\n\n\n\n问答整体Prompt实现：\n                                                                                                                                                                     \n```\nexport const promptTemplateMarkdown = () => {\n  return `\n# CONTEXT\n\n\n得物的开放平台是一个包含着 API 文档，解决方案文档的平台，商家可以通过这个平台获取到得物的各种接口，以及解决方案，帮助商家更好的使用得物的服务。\n现在得物开放平台的人工答疑率相当高，原因可能是文档的信息藏的较深，我希望做一个人工智能答疑助手，通过分析开放平台的各种文档，来回答用户的问题，最终让用户不进入人工答疑阶段。\n我们只讨论[开放平台接口]的相关问题,不要谈及其他内容。\n\n\n# OBJECTIVE\n你需要根据用户的输入，以及提供的得物开放平台的文档上下文，进行答疑。\n你只接受有关[开放平台接口]的相关问答，不接受其余任何问题。\n\n\n## 关于用户的输入：\n\n\n1. 你会得到一份符合 JSONSchema 结构的结构化数据，这份数据我会使用\\`<structured-input></structured-input>\\`包裹。\n   这份结构化数据是通过实际的用户提问进行了二次分析而得出的。结构化数据里也会包含用户的最初始的问题供你参考（最初始的问题会放在 question 字段里）\n\n\n## 关于上下文\n\n\n1.  我已经提前准备好了你需要参考的资料，作为你回答问题的上下文，上下文是由许多篇 Markdown 文档组成的。这些 Markdown 的文档大标题代表了这个片段的模块名，例如 \\`# 接口入参\\`就代表这部分是文档的接口入参部分， \\`# 接口返回\\`就代表这部分是文档的接口返回部分，\n2.  上下文中的主要信息部分我会使用 Markdown Table 的结构提供给你。\n3.  每个上下文的开头，我都会给你一些关于这份上下文的元信息（使用 FrontMatter 结构），这个元信息代表了这份文档的基础信息，例如文档的页面地址，接口的名称等等。\n\n\n以下是我提供的结构化输入，我会使用\\`<structured-input></structured-input>\\`标签做包裹\n<structured-input>\n{structuredInput}\n</structured-input>\n\n\n以下是我为你提供的参考资料，我会使用\\`<context></context>\\`标签包裹起来：\n<context>\n{context}\n</context>\n\n\n# STYLE\n\n\n你需要把你的回答以特定的 JSON 格式返回\n\n\n# TONE\n\n\n你是一个人工智能答疑助手，你的回答需要温柔甜美，但又不失严谨。对用户充满了敬畏之心，服务态度要好。在你回答问题之前，需要简单介绍一下自己，例如“您好，很高兴为您服务。已经收到您的问题。”\n\n\n# AUDIENCE\n\n\n你的用户是得物开放平台的开发者们，他们是你要服务的对象。\n\n\n# RESPONSE\n\n\n你返回的数据结构必须符合我提供的 JSON Schema 规范，我给你的 Schema 将会使用\\`<structured-output-schema></structured-output-schema>\\`标签包裹.\n\n\n<structured-output-schema>\n  {strcuturedOutputSchema}\n</structured-output-schema>\n`;\n};\n\n\n```\n\n\n\n\n以上问答通过CO-STAR结构，从6个方面完全限定了答疑助手的回答腔调以及问答范畴，我们现在只需要准备相应的数据结构提供给这份Prompt模板。\n\n\n\n\n**问答结果结构化**\n\n在开放平台答疑助手的场景下，我们不仅要正面回答用户的问题，同时还需要给出相应的可阅读链接。结构如下：\n                                                                                                      \n```\nimport { z } from 'zod';\n\n\nconst zOutputSchema = z\n  .object({\n    question: z\n      .string()\n      .describe(\n        '提炼后的用户提问。此处的问题指的是除去用户提供的接口信息外的问题。尽量多的引用用户的提问',\n      ),\n    introduction: z\n      .string()\n      .describe('开放平台智能答疑助手对用户的问候以及自我介绍'),\n    answer: z\n      .array(z.string())\n      .describe(\n        '开放平台智能答疑助手的回答，需将问题按步骤拆分，形成数组结构，回答拆分尽量步骤越少越好。如果回答的问题涉及到具体的页面地址引用，则将页面地址放在relatedUrl字段里。不需要在answer里给出具体的页面地址',\n      ),\n    relatedUrl: z\n      .array(z.string())\n      .describe(\n        '页面的链接地址，取自上下文的pageUrl字段，若涉及多个文档，则给出所有的pageUrl，若没有pageUrl，则不要返回',\n      )\n      .optional(),\n  })\n  .required({\n    question: true,\n    introduction: true,\n    answer: true,\n  });\n\n\ntype OpenRagOutputType = z.infer<typeof zOutputSchema>;\n\n\nexport { zOutputSchema, type OpenRagOutputType };\n\n\n```\n\n\n\n\n在我们之前的设计中，我们的每一份向量数据的头部，均带有相应的文档meta信息，通过这种向量设计，我们可以很容易的推算出可阅读链接。同时，我们在这份zod schema中提供了很详细的description，来限定机器人的回答可以有效的提取相应信息。\n\n\n\n\n**Runnable的结合**\n\n在用户提问解答这个Runnable中，我们需要结合Retriever, 上下文，用户提问，用户输出限定这几部分进行组合。\n                                                                                                                                       \n```\nimport { ChatOpenAI } from '@langchain/openai';\nimport { $getPrompt } from './prompt/index';\nimport { JsonOutputParser } from '@langchain/core/output_parsers';\nimport { RunnableSequence, RunnableMap } from '@langchain/core/runnables';\nimport { zOutputSchema } from './schema';\nimport { $getContext } from './retriever/index';\nimport { getLLMConfig } from 'src/utils/llm/get-llm-config';\nimport { getStringifiedJsonSchema } from 'src/utils/llm/get-stringified-json-schema';\nimport { n } from 'src/utils/llm/gen-runnable-name';\n\n\nconst b = n('$open-rag');\n\n\ntype OpenRagInput = {\n  structuredInput: string;\n  question: string;\n};\n\n\nconst $getOpenRag = async () => {\n  const $model = new ChatOpenAI(getLLMConfig().ChatOpenAIConfig).bind({\n    response_format: {\n      type: 'json_object',\n    },\n  });\n\n\n  const chain = RunnableSequence.from([\n    RunnableMap.from<OpenRagInput>({\n      // 问答上下文\n      context: await $getContext(),\n      // 结构化输入\n      structuredInput: (input) => input.structuredInput,\n      // 用户提问\n      question: (input) => input.question,\n      // 输出结构\n      strcuturedOutputSchema: () => getStringifiedJsonSchema(zOutputSchema),\n    }).bind({ runName: b('runnable-map') }),\n    $getPrompt().bind({ runName: b('prompt') }),\n    $model,\n    new JsonOutputParser(),\n  ]).bind({ runName: b('chain') });\n\n\n  return chain;\n};\n\n\nexport { $getOpenRag };\n\n\n```\n\n\n\n\n\n\n\n**流程串联**\n\n通过上文的几大部分，我们已经将 用户提问、结构化解析、相似性搜索、问答上下文，问答Prompt模板以及结构化输出这几部分实现完成。\n\n\n\n\n现在可以通过LangChain的Runnable能力进行流程串联。\n                                       \n```\nconst structure = safeJsonParse<StructuredInputType>(\n  await $getStructuredInput().invoke({ question }),\n  null,\n)\nconst mainChain = RunnableSequence.from<{ question: string }, OpenRagOutputType>([\n  RunnablePassthrough.assign({\n    structuredInput: () => structure,\n  }),\n  await $getOpenRag(),\n]);\nconst response = await mainChain.invoke({\n  question: question,\n});\n```\n\n\n\n\n串联流程如下：\n\n\n\n\n返回结果示例：\n\n\n\n\n输入：\n         \n```\n{\n  \"question\": \"正式环境调用：直发订单获取打印面单v2【品牌直发/众筹直发/拍卖直发】接口返回报错（https://openapi.dewu.com/dop/api/v2/order/brand_deliver/express_sheet）。请求参数：{\\\"order_no\\\":\\\"110152014363632294\\\",\\\"app_key\\\":\\\"953a43bec40943c98a5cdbc63333242f;\\\",\\\"sign\\\":\\\"AD968ED6A359D144B61F1FEF6BB380ED\\\",\\\"timestamp\\\":1720430064169}\"\n}\n```\n\n\n\n\n输出：\n                                             \n```\n{\n  \"question\": \"我的问题是：正式环境调用：直发订单获取打印面单v2【品牌直发/众筹直发/拍卖直发】接口返回报错。\",\n  \"introduction\": \"您好，很高兴为您服务。已经收到您的问题。\",\n  \"answer\": [\n    \"根据您提供的请求参数信息，出现错误可能是由于接口调用参数不正确导致的，接下来我们一步步排查问题。\",\n    \"首先，您提供的请求参数中的订单号为\\\"110152014363632294\\\"，请确认该订单号的状态是否为待物流揽收或待买家收货状态。\",\n    \"其次，检查您所使用的app_key是否正确，确保该app_key授权了调用该接口的权限。\",\n    \"再次，请确认传入的签名（sign）是否正确，可能需要重新生成签名以保证准确性。\",\n    \"最后，检查时间戳（timestamp）是否正确，并且处于合理的时间范围内。\",\n    \"如果以上步骤都没有解决问题，建议您查看详细的接口文档以进一步调试和解决问题。\"\n  ],\n  \"relatedUrl\": [\n    \"https://open.dewu.com/#/api?apiId=1174\"\n  ]\n}\n```\n\n\n\n\n**五**\n\n**应用调试**\n\n基于大模型应用可能设计到多个Runnable的多次调用，借用LangSmith的trace功能，我们可以对每一个Runnable进行出入参的debug。\n\n\n\n\n关于LangSmith的接入：\n\n\n\n\n**六**\n\n**未来展望**\n\nRAG在减少模型幻觉，无需模型训练就可享受内容时效性的特点在此类答疑应用中展露无遗，RAG应用开放平台落地从一定程度上验证了依赖可靠知识库的答疑场景具备可执行性，还为内部系统的应用提供了有力的参考。在实际应用中，除了直接解决用户的提问外，通过回放用户提问的过程，可以为产品和业务的发展提供重要的洞察。\n\n\n\n\n面向未来，是否可以尝试将答疑助手的形式在内部系统落地，在内部建立知识库体系，将部分问题前置给大模型处理，降低TS和开发介入答疑的成本。\n\n\n\n\n**往期回顾**\n\n\n\n\n1.[得物基于AIGC生成测试用例的探索与实践](https://mp.weixin.qq.com/s?__biz=MzkxNTE3ODU0NA==& mid=2247536606& idx=1& sn=14fe70f4a2b609e83466edc0f181230f& scene=21#wechat_redirect)\n\n2.[软件开发领域变革前夕-AI智能编码的发展｜得物技术](https://mp.weixin.qq.com/s?__biz=MzkxNTE3ODU0NA==& mid=2247536557& idx=1& sn=c410b8acf90ae7e9db5a46ce0ee26278& scene=21#wechat_redirect)\n\n3.[Java程序中的潜在危机: 深入探讨NullPointerException｜得物技术](https://mp.weixin.qq.com/s?__biz=MzkxNTE3ODU0NA==& mid=2247536496& idx=1& sn=bf324ef495d3ce9cf6b65b9bfa9115f8& scene=21#wechat_redirect)\n\n4.[基于RocksDB编写一个简单的SQL数据库｜得物技术](https://mp.weixin.qq.com/s?__biz=MzkxNTE3ODU0NA==& mid=2247536482& idx=1& sn=b477a5b9fe8be4b28a1d43bba26aeb84& scene=21#wechat_redirect)\n\n5.[站外商详的重构与优化｜得物技术](https://mp.weixin.qq.com/s?__biz=MzkxNTE3ODU0NA==& mid=2247536457& idx=1& sn=6be880754503f6bc11f5135308080b91& scene=21#wechat_redirect)\n\n\n\n\n文 / 惑普\n\n\n\n\n关注得物技术，每周一、三更新技术干货\n\n要是觉得文章对你有帮助的话，欢迎评论转发点赞～\n\n未经得物技术许可严禁转载，否则依法追究法律责任。\n\n“\n\n**扫码添加小助手微信**<strong>\n</strong>\n\n如有任何疑问，或想要了解更多技术资讯，请添加小助手微信：\n\n\n\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2025-01-22T03:04:36Z"
  },
  {
    "id": 18,
    "title": "Pytho在数据科学中的应用",
    "url": "https://blog.51cto.com/u_15916160/12897075",
    "content": "\n随着数据科学的兴起，Python已经成为数据科学家和分析师的首选编程语言。Python的简洁语法、丰富的库以及强大的社区支持，使其在数据科学领域具有广泛的应用。本文将探讨Python在数据科学中的一些主要应用，包括数据处理、数据分析、机器学习等方面。\n\n#### 一、数据处理\n\n##### 1. Pandas\n\nPandas是Python中用于数据处理和分析的强大库。它提供了灵活的数据结构（如DataFrame），使得数据的读取、清洗、转换和聚合变得非常方便。Pandas还支持多种数据格式，包括CSV、Excel、SQL数据库等，使得数据的导入和导出变得简单。\n\n```\nimport pandas as pd\n\n# 读取CSV文件\ndf = pd.read_csv('data.csv')\n\n# 查看数据\nprint(df.head())\n\n# 数据清洗：删除缺失值\ndf.dropna(inplace=True)\n\n# 数据转换：创建新列\ndf['new_column'] = df['existing_column'] * 2\n\n# 数据聚合：计算平均值\nmean_value = df['numeric_column'].mean()\nprint(\"Mean value:\", mean_value)\n```\n\n##### 2. NumPy\n\nNumPy是Python中用于数值计算的基础库。它提供了高效的多维数组对象和各种数学函数，使得数值计算变得更加高效。NumPy通常与Pandas一起使用，以处理大规模的数据集。\n\n```\nimport numpy as np\n\n# 创建一个NumPy数组\narr = np.array([1, 2, 3, 4, 5])\n\n# 进行数值计算\nmean_value = np.mean(arr)\nprint(\"Mean value:\", mean_value)\n\n# 矩阵运算\nmatrix_a = np.array([[1, 2], [3, 4]])\nmatrix_b = np.array([[5, 6], [7, 8]])\nresult = np.dot(matrix_a, matrix_b)\nprint(\"Matrix multiplication result:\n\", result)\n```\n\n#### 二、数据分析\n\n##### 1. Matplotlib和Seaborn\n\nMatplotlib和Seaborn是Python中用于数据可视化的两个常用库。它们可以帮助我们创建各种图表，如折线图、柱状图、散点图等，以便更好地理解数据。\n\n```\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# 使用Matplotlib创建折线图\nplt.plot([1, 2, 3, 4], [10, 20, 25, 30])\nplt.xlabel('X轴标签')\nplt.ylabel('Y轴标签')\nplt.title('折线图示例')\nplt.show()\n\n# 使用Seaborn创建热力图\ndata = np.random.rand(10, 12)\nsns.heatmap(data)\nplt.show()\n```\n\n##### 2. SciPy\n\nSciPy是基于NumPy的另一个库，主要用于科学计算。它提供了许多用于统计分析、优化、积分等功能的工具。SciPy与NumPy和Pandas结合使用，可以大大简化数据分析的过程。\n\n```\nfrom scipy import stats\n\n# 生成正态分布数据\ndata = np.random.normal(loc=0, scale=1, size=1000)\n\n# 计算均值和标准差\nmean_value, std_dev = np.mean(data), np.std(data)\nprint(\"Mean:\", mean_value, \"Standard Deviation:\", std_dev)\n\n# 正态性检验\nk2, p = stats.normaltest(data)\nprint(\"K-squared:\", k2, \"P-value:\", p)\n```\n\n#### 三、机器学习\n\n##### 1. Scikit-learn\n\nScikit-learn是Python中最常用的机器学习库之一。它提供了各种监督学习和非监督学习的算法，如线性回归、逻辑回归、支持向量机、K均值聚类等。Scikit-learn还提供了模型选择、评估和交叉验证等功能，使得机器学习的流程变得更加简单。\n\n```\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# 假设我们有一个数据集df，包含特征和目标变量\nX = df[['feature1', 'feature2']]\ny = df['target']\n\n# 划分训练集和测试集\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 训练线性回归模型\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# 预测\npredictions = model.predict(X_test)\n\n# 评估模型\nmse = mean_squared_error(y_test, predictions)\nprint(\"Mean Squared Error:\", mse)\n```\n\n##### 2. TensorFlow和Keras\n\nTensorFlow是一个开源的深度学习框架，而Keras是一个高层神经网络API，能够运行在TensorFlow之上。Keras使得构建和训练深度学习模型变得更加简单和直观。\n\n```\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\n# 构建一个简单的神经网络模型\nmodel = Sequential([\n    Dense(64, activation='relu', input_shape=(input_dim,)),\n    Dense(64, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\n\n# 编译模型\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# 训练模型\nmodel.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n# 评估模型\nloss, accuracy = model.evaluate(X_test, y_test)\nprint(\"Test Accuracy:\", accuracy)\n```\n\n#### 四、大数据处理\n\n##### 1. PySpark\n\n对于大规模数据集，单机处理可能效率低下甚至不可行。PySpark是基于Apache Spark的大数据处理框架的Python API，可以有效地处理大规模数据。PySpark支持分布式数据处理，使得在大数据集上的操作变得更加高效。\n\n```\nfrom pyspark.sql import SparkSession\n\n# 初始化SparkSession\nspark = SparkSession.builder.appName('example').getOrCreate()\n\n# 读取数据\ndf = spark.read.csv('large_dataset.csv', header=True, inferSchema=True)\n\n# 数据处理\ndf = df.filter(df['column'] > threshold)\ndf = df.groupBy('another_column').agg({'target_column': 'mean'})\n\n# 保存结果\ndf.write.csv('processed_data.csv', header=True)\n```\n\n#### 五、Web爬虫和数据抓取\n\n##### 1. BeautifulSoup和Requests\n\n网络爬虫是数据科学中获取数据的重要手段之一。BeautifulSoup和Requests是Python中常用的两个库，用于从网页中抓取数据。BeautifulSoup用于解析HTML和XML文档，而Requests则用于发送HTTP请求。\n\n```\nimport requests\nfrom bs4 import BeautifulSoup\n\n# 发送HTTP请求\nurl = 'http://example.com'\nresponse = requests.get(url)\nhtml_content = response.text\n\n# 解析HTML内容\nsoup = BeautifulSoup(html_content, 'html.parser')\n\n# 提取数据\ntitle = soup.find('title').text\nprint(\"Page title:\", title)\n```\n\n#### 六、总结\n\nPython在数据科学中的应用非常广泛，涵盖了从数据处理、数据分析到机器学习和大数据处理的各个方面。其简洁的语法和丰富的库使得数据科学工作变得更加高效和便捷。未来，随着技术的不断发展，Python在数据科学领域的应用将会更加深入和广泛。\n",
    "labels": [],
    "created_at": "2025-01-10T01:50:36Z"
  },
  {
    "id": 19,
    "title": "liux服务器下模拟网络延迟",
    "url": "https://blog.51cto.com/u_1969518/12890849",
    "content": "\n## 1.背景\n\n为了测试程序的健壮性以及在弱网环境下程序的表现，通常需要创造一个“不那么稳定”的网络环境，但这种模拟十分不好控制变量，比如希望控制网络延迟在500ms时，现实环境则是难以实现的，那有什么解决的办法呢？\n\n答案是，可以在Linux下使用tc命令来模拟延迟。\n\n## 2.安装\n\n在不同的发行版中使用不同的安装命令：\n\n**Ubuntu/Debian**：\n\n```\nsudo apt-get install iproute2\n```\n\n**CentOS/RHEL**：\n\n```\nsudo yum install iproute-tc\n```\n\n<img src=\"https://s2.51cto.com/images/blog/202412/20173005_6765391ddf69225544.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184\" alt=\"linux 服务器下模拟网络延迟_控制变量\" title=\"图片\"  referrerpolicy=\"no-referrer\">\n\n\n\nimg\n\n## 3.使用准备\n\n首先ping一下baidu.com看一下当前的网络延迟情况，执行命令：\n\n```\nping baidu.com\n```\n\n可以看到延迟在135ms左右(一台国外虚拟机)\n\n<img src=\"https://s2.51cto.com/images/blog/202412/20173005_6765391de7d4362771.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184\" alt=\"linux 服务器下模拟网络延迟_控制变量_02\" title=\"图片\"  referrerpolicy=\"no-referrer\">\n\n\n\nimg\n\n判断一下当前连接外网所用的网卡，使用以下命令：\n\n```\nifconfig\n```\n\n我这里是使用的eth0的网卡，大家可以根据自己主机的情况灵活判断。\n\n<img src=\"https://s2.51cto.com/images/blog/202412/20173006_6765391e144d264759.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184\" alt=\"linux 服务器下模拟网络延迟_控制变量_03\" title=\"图片\"  referrerpolicy=\"no-referrer\">\n\n\n\n获得以上的延迟数据和网卡信息后，准备实验。\n\n## 4.应用测试\n\n### 4.1 tc 命令格式\n\n```\ntc [ OPTIONS ] OBJECT { COMMAND | help }\n```\n\n其中，OPTIONS 是可选的命令行选项，OBJECT 是要操作的对象，COMMAND 是要执行的命令，help 用于显示帮助信息。\n\n### 4.2 增加延迟\n\n下面举几个常用的例子：**增加500ms的网络延迟**\n\n```\n# 其中eth0为需要增加延迟的网卡\ntc qdisc add dev eth0 root netem delay 500ms\n```\n   tc qdisc add：表示添加一个排队规则（queue discipline）。      dev eth0：指定要操作的网络设备为 eth0。      root：表示在该设备的根（最顶层）处添加。      netem：表示使用网络仿真（Network Emulator）排队规则。      delay 500ms：表示设置延迟为 500 毫秒。   \n如果大家执行命令时出现以下的报错，那么需要执行以下命令。\n\n<img src=\"https://s2.51cto.com/images/blog/202412/20173005_6765391dc993970346.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184\" alt=\"linux 服务器下模拟网络延迟_ci_04\" title=\"图片\"  referrerpolicy=\"no-referrer\">\n\n\n\nqdisc作为内核模块在名为kernel-modules-extra的包中提供，需要安装对应内核版本的kernel-modules-extra才会有qdisc模块\n\n```\nyum -y install kernel-modules-extra.x86_64\n```\n\n完成之后，再次执行之前增加延迟的命令，发现可以正常执行了，同时明显地感受到终端的响应速度变慢。\n\n<img src=\"https://s2.51cto.com/images/blog/202412/20173006_6765391e28b0285418.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184\" alt=\"linux 服务器下模拟网络延迟_ci_05\" title=\"图片\"  referrerpolicy=\"no-referrer\">\n\n\n\n当前延迟在535毫秒，相较于之前135ms，延迟成功增加了500ms。\n\n<img src=\"https://s2.51cto.com/images/blog/202412/20173006_6765391e011ba52531.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184\" alt=\"linux 服务器下模拟网络延迟_控制变量_06\" title=\"图片\"  referrerpolicy=\"no-referrer\">\n\n\n\n如果要取消延迟，将add命令，修改为delete命令\n\n```\ntc qdisc delete dev eth0 root netem delay 500ms\n```\n\n<img src=\"https://s2.51cto.com/images/blog/202412/20173006_6765391e234db26967.png?x-oss-process=image/watermark,size_14,text_QDUxQ1RP5Y2a5a6i,color_FFFFFF,t_30,g_se,x_10,y_10,shadow_20,type_ZmFuZ3poZW5naGVpdGk=,x-oss-process=image/resize,m_fixed,w_1184\" alt=\"linux 服务器下模拟网络延迟_模拟网络延迟_07\" title=\"图片\"  referrerpolicy=\"no-referrer\">\n\n\n\n### 4.3 限制带宽\n\n```\nsudo tc qdisc add dev eth0 root handle 1:0 htb default 10\n```\n\n设置带宽限制：使用 tc class add 命令设置带宽限制。例如，以下命令将创建一个名为 1:1 的类，限速为 1Mbps：\n\n```\nsudo tc class add dev eth0 parent 1:0 classid 1:1 htb rate 1mbit\n```\n\n（可选）设置特定流量的带宽限制：可以根据需要进一步设置特定流量的带宽限制。例如，以下命令将限制源 IP 为 192.168.0.10 的流量为 500Kbps：\n\n```\nsudo tc class add dev eth0 parent 1:1 classid 1:10 htb rate 500kbit ceil 1mbit\nsudo tc filter add dev eth0 protocol ip parent 1:0 prio 1 u32 match ip src 192.168.0.10 flowid 1:10\n```\n\n验证和测试：可以使用网络测试工具（如 iperf）来验证带宽限制是否生效。\n\n当然取消限制也是和前面的方法类似，各位朋友可自行测试效果！\n",
    "labels": [],
    "created_at": "2024-12-31T20:23:24Z"
  },
  {
    "id": 25,
    "title": "软件项目工作量评估方法COSMIC重点笔记",
    "url": "https://blog.csdn.net/qq_41587612/article/details/115013034",
    "content": "\n<img alt=\"\" height=\"230\" src=\"https://i-blog.csdnimg.cn/blog_migrate/e6295a28df00f4f3a435f3249c48edb8.png\" width=\"262\" referrerpolicy=\"no-referrer\">\n\n若本文对你有帮助，记得点赞、关注我哟！\n\nCosmic（Common Software Measurement International Consortium通用软件度量国际联盟）是PMO（项目管理办公室，是不同于PM项目经理的一种岗位）需要掌握的一个基础软件规模度量方法——估算功能点。\n\n优点：简单易用；原理科学；适用范围广，应用软件与嵌入式都可；客观准确；易于扩充；适用于软件开发的各阶段。\n\n适用于：业务应用软件；实施软件；平台软件；一些科学/工程软件。\n\n我自学过一阵子COSMIC，学习成本很底，大概只需一周就能掌握，实际应用时还是得公司统一 一套本地化规则，COSMIC官方手册并未阐述所有应用场景的评估方案，而且初学不建议直接看官方手册，理由是它把很简单的概念说得很复杂，学完后再去看官方手册就轻松多了。\n\n初学推荐：麦哲思科技（北京）有限公司官方出品的“快速学习COSMIC手册”\n\n你可以在官网下载[http://www.measures.net.cn/d/news/detail?id=5cad572ac6983442f2abf105](http://www.measures.net.cn/d/news/detail?id=5cad572ac6983442f2abf105)\n\n也可在CSDN里收藏[https://blog.csdn.net/dylanren/article/details/78614073?locationNum=10& fps=1](https://blog.csdn.net/dylanren/article/details/78614073?locationNum=10& fps=1)\n\n我个人觉得第5~15章是最实用的，所以放在收藏夹里，脑袋不好使了就翻来看看(o°ω°o)\n\n<img alt=\"\" src=\"https://i-blog.csdnimg.cn/blog_migrate/d09cfe8308abb2416aca040d87143056.png\" referrerpolicy=\"no-referrer\">\n\n\n\n所以我就不赘述COSMIC的入门笔记了，在此只展示我在学习过程中觉得比较重要的内容。\n\n<img alt=\"\" src=\"https://i-blog.csdnimg.cn/blog_migrate/0be9e63a2dc253ce594e47240fe925e7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这是我自己画的，方便理解！\n\n<img alt=\"\" src=\"https://i-blog.csdnimg.cn/blog_migrate/0143c1f884f0acb3f39347204e4635fe.png\" referrerpolicy=\"no-referrer\">\n\n\n\n\n\n<img alt=\"\" src=\"https://i-blog.csdnimg.cn/blog_migrate/bf41b42a5c75a1aa3685c2061bc6fb1e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n<img alt=\"\" src=\"https://img-blog.csdnimg.cn/20210319231935174.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzQxNTg3NjEy,size_16,color_FFFFFF,t_70\" referrerpolicy=\"no-referrer\">\n\n\n\n可复用功能处理的度量：\n\n<img alt=\"\" height=\"198\" src=\"https://i-blog.csdnimg.cn/blog_migrate/72bf86dd4f643422aae5ad8f1edf1811.png\" width=\"778\" referrerpolicy=\"no-referrer\">\n\n\n\n    当一个FUR在软件里被实现时，任何“功能共性”可能开发为可复用软件，也可能不会。当度量软件规模时，必须忽略所有包括实际的或潜在的软件复用的实现决策。但是，当使用功能规模度量法进行项目工作量分析或估算时，也许需要考虑复用。\n\n<img alt=\"\" height=\"66\" src=\"https://i-blog.csdnimg.cn/blog_migrate/1c600d07017b78bd2396411ae15a6e1b.png\" width=\"598\" referrerpolicy=\"no-referrer\">\n\n\n\n<img alt=\"\" src=\"https://i-blog.csdnimg.cn/blog_migrate/860a5ab5b043308f8a6787014b2696f7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n<img alt=\"\" height=\"165\" src=\"https://i-blog.csdnimg.cn/blog_migrate/7ada13666fc39ff3e81604978213ead5.png\" width=\"333\" referrerpolicy=\"no-referrer\">\n\n\n\n<img alt=\"\" height=\"116\" src=\"https://i-blog.csdnimg.cn/blog_migrate/800ce0f9d4ade3fdff8b0d0325799713.png\" width=\"399\" referrerpolicy=\"no-referrer\">\n\n\n\n<img alt=\"\" height=\"240\" src=\"https://i-blog.csdnimg.cn/blog_migrate/4363f3ded718c7d5a70b6c9cfabb532b.png\" width=\"273\" referrerpolicy=\"no-referrer\">\n\n\n",
    "labels": [],
    "created_at": "2024-12-31T08:40:22Z"
  },
  {
    "id": 24,
    "title": "\n\nLlamaFactory：解锁LLM微调效率的终极神器！\n",
    "url": "https://mp.weixin.qq.com/s/7Z1FfLrIRKceLN8jD7ecEQ",
    "content": "\nLLaMA Factory 是一个简单易用且高效的大型语言模型（Large Language Model）训练与微调平台。通过 LLaMA Factory，可以在无需编写任何代码的前提下，在本地完成上百种预训练模型的微调，框架特性包括：\n<li >模型种类：LLaMA、LLaVA、Mistral、Mixtral-MoE、Qwen、Yi、Gemma、Baichuan、ChatGLM、Phi 等等。   <li >训练算法：（增量）预训练、（多模态）指令监督微调、奖励模型训练、PPO 训练、DPO 训练、KTO 训练、ORPO 训练等等。   <li >运算精度：16 比特全参数微调、冻结微调、LoRA 微调和基于 AQLM/AWQ/GPTQ/LLM.int8/HQQ/EETQ 的 2/3/4/5/6/8 比特 QLoRA 微调。   <li >优化算法：GaLore、BAdam、DoRA、LongLoRA、LLaMA Pro、Mixture-of-Depths、LoRA+、LoftQ 和 PiSSA。   <li >加速算子：FlashAttention-2 和 Unsloth。   <li >推理引擎：Transformers 和 vLLM。   <li >实验面板：LlamaBoard、TensorBoard、Wandb、MLflow 等等。   \n在安装 LLaMA-Factory 之前，请确保您安装了下列依赖:\n\n运行以下指令以安装 LLaMA-Factory 及其依赖:\n         \n```\ngitclone--depth 1 https://github.com/hiyouga/LLaMA-Factory.git\ncdLLaMA-Factory\npip install -e\".[torch,metrics]\"\n```\n   \n```\nllamafactory-cli webui\n```\n\nWebUI 主要分为四个界面：训练、评估与预测、对话、导出。\n                  \n```\n\"alpaca_zh_demo.json\"\n{\n\"instruction\":\"计算这些物品的总费用。 \",\n\"input\":\"输入：汽车 -$3000，衣服 -$100，书 -$20。\",\n\"output\":\"汽车、衣服和书的总费用为$3000+$100+$20=$3120。\"\n},\n```\n   \n```\nllamafactory-cli train examples/train_lora/llama3_lora_sft.yaml\n```\n                                                                                       \n```\n### examples/train_lora/llama3_lora_sft.yaml\nmodel_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\nstage: sft\ndo_train:true\nfinetuning_type: lora\nlora_target:all\ndataset:identity,alpaca_en_demo\ntemplate: llama3\ncutoff_len:1024\nmax_samples:1000\noverwrite_cache:true\npreprocessing_num_workers:16\noutput_dir: saves/llama3-8b/lora/sft\nlogging_steps:10\nsave_steps:500\nplot_loss:true\noverwrite_output_dir:true\nper_device_train_batch_size:1\ngradient_accumulation_steps:8\nlearning_rate:1.0e-4\nnum_train_epochs:3.0\nlr_scheduler_type: cosine\nwarmup_ratio:0.1\nbf16:true\nddp_timeout:180000000\nval_size:0.1\nper_device_eval_batch_size:1\neval_strategy: steps\neval_steps:500\n```\n\n当我们基于预训练模型训练好 LoRA 适配器后，我们不希望在每次推理的时候分别加载预训练模型和 LoRA 适配器，因此我们需要将预训练模型和 LoRA 适配器合并导出成一个模型，并根据需要选择是否量化。根据是否量化以及量化算法的不同，导出的配置文件也有所区别。\n\n您可以通过`llamafactory-cliexportmerge_config.yaml`指令来合并模型。其中`merge_config.yaml`需要您根据不同情况进行配置。\n\n在完成模型合并并获得完整模型后，为了优化部署效果，人们通常会基于显存占用、使用成本和推理速度等因素，选择通过量化技术对模型进行压缩，从而实现更高效的部署。\n\n量化（Quantization）通过数据精度压缩有效地减少了显存使用并加速推理。LLaMA-Factory 支持多种量化方法，包括:\n<li >AQLM   <li >AWQ   <li >GPTQ   <li >QLoRA   <li >…   \nGPTQ 等后训练量化方法(Post Training Quantization)是一种在训练后对预训练模型进行量化的方法。我们通过量化技术将高精度表示的预训练模型转换为低精度的模型，从而在避免过多损失模型性能的情况下减少显存占用并加速推理，我们希望低精度数据类型在有限的表示范围内尽可能地接近高精度数据类型的表示，因此我们需要指定量化位数`export_quantization_bit`以及校准数据集`export_quantization_dataset`。\n                                 \n```\n### examples/merge_lora/llama3_gptq.yaml\n### model\nmodel_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\ntemplate: llama3\n### export\nexport_dir: models/llama3_gptq\nexport_quantization_bit:4\nexport_quantization_dataset: data/c4_demo.json\nexport_size:2\nexport_device: cpu\nexport_legacy_format:false\n```\n\nLLaMA-Factory 支持多种推理方式。\n\n您可以使用`llamafactory-clichatinference_config.yaml`或`llamafactory-cliwebchatinference_config.yaml`进行推理与模型对话。对话时配置文件只需指定原始模型`model_name_or_path`和`template`，并根据是否是微调模型指定`adapter_name_or_path`和`finetuning_type`。\n\n如果您希望向模型输入大量数据集并保存推理结果，您可以启动vllm推理引擎对大量数据集进行快速的批量推理。您也可以通过部署 api服务的形式通过 api 调用来进行批量推理。\n\n默认情况下，模型推理将使用 Huggingface 引擎。 您也可以指定`infer_backend:vllm`以使用 vllm 推理引擎以获得更快的推理速度。\n\n在完成模型训练后，您可以通过`llamafactory-clievalexamples/train_lora/llama3_lora_eval.yaml`来评估模型效果。\n\n配置示例文件`examples/train_lora/llama3_lora_eval.yaml`具体如下：\n                                             \n```\n### examples/train_lora/llama3_lora_eval.yaml\n### model\nmodel_name_or_path: meta-llama/Meta-Llama-3-8B-Instruct\nadapter_name_or_path: saves/llama3-8b/lora/sft # 可选项\n### method\nfinetuning_type: lora\n### dataset\ntask: mmlu_test # mmlu_test, ceval_validation, cmmlu_test\ntemplate: fewshot\nlang: en\nn_shot:5\n### output\nsave_dir: saves/llama3-8b/lora/eval\n### eval\nbatch_size:4\n```\n\nLLaMA-Factory 支持单机多卡和多机多卡分布式训练。同时也支持DDP,DeepSpeed和 FSDP 三种分布式引擎。\n\nDDP(DistributedDataParallel) 通过实现模型并行和数据并行实现训练加速。 使用 DDP 的程序需要生成多个进程并且为每个进程创建一个 DDP 实例，他们之间通过`torch.distributed`库同步。\n\nDeepSpeed是微软开发的分布式训练引擎，并提供ZeRO（Zero Redundancy Optimizer）、offload、Sparse Attention、1 bit Adam、流水线并行等优化技术。 您可以根据任务需求与设备选择使用。\n\nFSDP通过全切片数据并行技术（Fully Sharded Data Parallel）来处理更多更大的模型。在 DDP 中，每张 GPU 都各自保留了一份完整的模型参数和优化器参数。而 FSDP 切分了模型参数、梯度与优化器参数，使得每张 GPU 只保留这些参数的一部分。 除了并行技术之外，FSDP 还支持将模型参数卸载至CPU，从而进一步降低显存需求。\n\n随着语言模型规模的不断增大，其训练的难度和成本已成为共识。 而随着用户数量的增加，模型推理的成本也在不断攀升，甚至可能成为限制模型部署的首要因素。 因此，我们需要对模型进行压缩以加速推理过程，而模型量化是其中一种有效的方法。\n\n大语言模型的参数通常以高精度浮点数存储，这导致模型推理需要大量计算资源。 量化技术通过将高精度数据类型存储的参数转换为低精度数据类型存储， 可以在不改变模型参数量和架构的前提下加速推理过程。这种方法使得模型的部署更加经济高效，也更具可行性。\n\n浮点数一般由3部分组成：符号位、指数位和尾数位。指数位越大，可表示的数字范围越大。尾数位越大、数字的精度越高。\n\n量化可以根据何时量化分为：后训练量化和训练感知量化，也可以根据量化参数的确定方式分为：静态量化和动态量化。\n\n为了解决大语言模型的遗忘问题， LLaMA Pro 通过在原有模型上增加新模块以适应新的任务，使其在多个新任务上的表现均优于原始模型。 LLaMA-Factory 支持 LLaMA Pro 的使用。 您可以使用运行`expand.sh`将`Meta-Llama-3-8B-Instruct`扩展为`llama3-8b-instruct-pro`。\n\n对于 LLaMA Pro 模型进行训练时，您需要指定`use_llama_pro`为`true`。\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-26T06:56:56Z"
  },
  {
    "id": 20,
    "title": "\n内网穿透及常见工具\n",
    "url": "https://juejin.cn/post/7439646231943266354",
    "content": "\n## 什么是内网穿透\n\n在了解内网穿透原理之前，我们先说什么是内网穿透。内网，就是在公司或者家庭内部，建立的局域网络或者是办公网络，可以实现多台电脑之间的资源共享，包括设备、资料、数据等。而外网则是通过一个网关与其它的网络系统连接，相对于内网而言，这种网络系统称之为外部网络，常见的就是我们日常使用的互联网。\n\n一般而言，在没有固定公网IP的情况下，外网设备无法直接访问内网设备。而内网穿透技术，顾名思义就是能让外网的设备找到处于内网的设备，从而实现数据通信。\n\n## 内网穿透的原理\n\n内网穿透，又称为NAT穿透。NAT背后的设备，它们的主要特点是 ，可以访问外网，但不能被外网设备有效访问。基于这一特点，NAT穿透技术是让NAT背后的设备，先访问指定的外网服务器，由指定的外网服务器搭建桥梁，打通内、外网设备的访问通道，实现外网设备访问到内网设备。\n\n该技术除了可以访问隐藏在NAT后的设备，同样可以穿透防火墙。这是因为防火墙一般只拦截了入站没有拦截出站，所以也可以让防火墙内的设备对外提供服务。\n\n由于内网设备并不是与外网设备直接相连，所以在安全性上是毋庸置疑的，内网穿透可以说是安全与效率兼得。\n\n<img src=\"https://p3-xtjj-sign.byteimg.com/tos-cn-i-73owjymdk6/13f1bf1c27b44123ab864c5ea0de3887~tplv-73owjymdk6-jj-mark-v1:0:0:0:0:5o6Y6YeR5oqA5pyv56S-5Yy6IEAgd2VrZw==:q75.awebp?rk3s=f64ab15b& x-expires=1735283991& x-signature=TQn0LxXaDEkAHkRBXGXeuvXlCEI%3D\" alt=\"image.png\" loading=\"lazy\" referrerpolicy=\"no-referrer\">\n\n\n\n## 如何实现内网穿透\n\n### 常见工具\n\n|名称|说明|复杂程度|下载地址\n|------|------\n|花生壳|免费的工具，安装就可以实现内网穿透功能，免费端口数量限制，不能使用80，所有数据都从他们服务端过|简单|[hsk.oray.com/](https://link.juejin.cn?target=https%3A%2F%2Fhsk.oray.com%2F)\n|nat123|免费的工具，安装就可以实现内网穿透功能，免费端口限制，所有数据都从他们服务端过|简单|[www.nat123.com/](https://link.juejin.cn?target=http%3A%2F%2Fwww.nat123.com%2F)\n|Frp|frp 是一个专注于内网穿透的高性能的反向代理应用，支持 TCP、UDP、HTTP、HTTPS 等多种协议。可以将内网服务以安全、便捷的方式通过具有公网 IP 节点的中转暴露到公网。|复杂|[github.com/fatedier/fr…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Ffatedier%2Ffrp)\n|Ngrok|ngrok是一个反向代理，它创建一个从公共端点到本地运行的web服务的安全隧道。ngrok捕获并分析隧道内的所有流量，以备日后检查和回放。注册后提供免费的公共服务器|复杂|[github.com/inconshreve…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Finconshreveable%2Fngrok)\n|lanproxy|lanproxy是一个将局域网个人电脑、服务器代理到公网的内网穿透工具，支持tcp流量转发，可支持任何tcp上层协议（访问内网网站、本地支付接口调试、ssh访问、远程桌面、http代理、https代理、socks5代理.|复杂|[github.com/ffay/lanpro…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fffay%2Flanproxy)\n|goproxy|Proxy是golang实现的高性能http,https,websocket,tcp,socks5代理服务器,支持内网穿透,链式代理,通讯加密,智能HTTP,SOCKS5代理,黑白名单,限速,限流量,限连接数,跨平台,KCP支持,认证API。|复杂|[github.com/snail007/go…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fsnail007%2Fgoproxy)\n|nps|一款轻量级、高性能、功能强大的内网穿透代理服务器。支持tcp、udp、socks5、http等几乎所有流量转发，可用来访问内网网站、本地支付接口调试、ssh访问、远程桌面，内网dns解析、内网socks5代理等等……，并带有功能强大的web管理端。|复杂|[github.com/cnlh/nps](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Fcnlh%2Fnps)\n\n### 搭建示例\n\n#### Frp的搭建（windows版本）\n\nfrp 主要由 客户端(frpc) 和 服务端(frps) 组成，服务端通常部署在具有公网 IP 的机器上，客户端通常部署在需要穿透的内网服务所在的机器上。\n\n内网服务由于没有公网 IP，不能被非局域网内的其他用户访问。\n\n用户通过访问服务端的 frps，由 frp 负责根据请求的端口或其他信息将请求路由到对应的内网机器，从而实现通信。\n\n##### 1.下载Frp的程序\n\n可以在github上下载release版本：[github.com/fatedier/fr…](https://link.juejin.cn?target=https%3A%2F%2Fgithub.com%2Ffatedier%2Ffrp%2Freleases%2Fdownload%2Fv0.40.0%2Ffrp_0.40.0_windows_amd64.zip)\n或者在本文的附件中下载frp程序\n\n##### 2.服务端安装\n\n需要在云服务上，有固定公网ip地址的服务器上安装服务端，并且设置的端口需要可以访问。\n把安装包复制到云服务器上，修改frps.ini的配置。\n服务端的全部配置的详细说明如下：\n**基础配置**\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|bind_addr|string|服务端监听地址|0.0.0.0||\n|bind_port|int|服务端监听端口|7000||接收 frpc 的连接\n|bind_udp_port|int|服务端监听 UDP 端口|0||用于辅助创建 P2P 连接\n|kcp_bind_port|int|服务端监听 KCP 协议端口|0||用于接收采用 KCP 连接的 frpc\n|proxy_bind_addr|string|代理监听地址|同 bind_addr||可以使代理监听在不同的网卡地址\n|log_file|string|日志文件地址|./frps.log||如果设置为 console，会将日志打印在标准输出中\n|log_level|string|日志等级|info|trace, debug, info, warn, error|\n|log_max_days|int|日志文件保留天数|3||\n|disable_log_color|bool|禁用标准输出中的日志颜色|false||\n|detailed_errors_to_client|bool|服务端返回详细错误信息给客户端|true||\n|tcp_mux_keepalive_interval|int|tcp_mux|的心跳检查间隔时间\t60||单位：秒\n|heartbeat_timeout|int|服务端和客户端心跳连接的超时时间|90||单位：秒\n|user_conn_timeout|int|用户建立连接后等待客户端响应的超时时间|10||单位：秒\n|udp_packet_size|int|代理 UDP 服务时支持的最大包长度|1500||服务端和客户端的值需要一致\n|tls_cert_file|string|TLS 服务端证书文件路径|||\n|tls_key_file|string|TLS 服务端密钥文件路径|||\n|tls_trusted_ca_fil|e\tstring|TLS CA 证书路径|||\n|**权限验证**|||||\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|authentication_method|string|鉴权方式|token|token, oidc|\n|authenticate_heartbeats|bool|开启心跳消息鉴权|false||\n|authenticate_new_work_conns|bool|开启建立工作连接的鉴权|false||\n|token|string|鉴权使用的 token 值|||客户端需要设置一样的值才能鉴权通过\n|oidc_issuer|string|oidc_issuer|||\n|oidc_audience|string|oidc_audience|||\n|oidc_skip_expiry_check|bool|oidc_skip_expiry_check|||\n|oidc_skip_issuer_check|bool|oidc_skip_issuer_check|||\n\n**管理配置**\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|allow_ports|string|允许代理绑定的服务端端口|||格式为 1000-2000,2001,3000-4000\n|max_pool_count|int|最大连接池大小|5||\n|max_ports_per_client|int|限制单个客户端最大同时存在的代理数|0||0 表示没有限制\n|tls_only|bool|只接受启用了 TLS 的客户端连接|false||\n\n**Dashboard, 监控**\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|dashboard_addr|string|启用 Dashboard 监听的本地地址|0.0.0.0||\n|dashboard_port|int|启用 Dashboard 监听的本地端口|0||\n|dashboard_user|string|HTTP BasicAuth 用户名|||\n|dashboard_pwd|string|HTTP BasicAuth 密码|||\n|enable_prometheus|bool|是否提供 Prometheus 监控接口|false||需要同时启用了 Dashboard 才会生效\n|asserts_dir|string|静态资源目录|||Dashboard 使用的资源默认打包在二进制文件中，通过指定此参数使用自定义的静态资源\n|**HTTP &  HTTPS**|||||\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|vhost_http_port|int|为 HTTP 类型代理监听的端口|0||启用后才支持 HTTP 类型的代理，默认不启用\n|vhost_https_port|int|为 HTTPS 类型代理监听的端口|0||启用后才支持 HTTPS 类型的代理，默认不启用\n|vhost_http_timeout|int|HTTP 类型代理在服务端的 ResponseHeader 超时时间|60||\n|subdomain_host|string|二级域名后缀|||\n|custom_404_page|string|自定义 404 错误页面地址|||\n\n**TCPMUX**\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|tcpmux_httpconnect_port|int|为 TCPMUX 类型代理监听的端口|0||启用后才支持 TCPMUX 类型的代理，默认不启用\n\n我们这边只配置最简单的方式，修改frps.ini的配置文件为：\n\n```\n[common]\nbind_port = 22354\ntoken = woxuwireless\n\n```\n\n其中`bind_port`基于自己服务器的实际情况配置范围  `1~65535`，配置的端口服务器必须要要开放。\n`token`就是自己设置的认证信息，所有的客户端都要保持一致，自己定义\n打开 cmd 终端，在cmd终端中执行命令，终端必须在frp根目录， 在执行命令 `./frps -c ./frps.ini` 启动服务端\n\n##### 3.客户端安装\n\n把安装包复制到内网需要开放的服务器上（也可以复制到其他电脑上，其他电脑必须可以访问需要开发的服务器）。\n修改frpc.ini配置，详细的frpc配置介绍如下：\n\n**基础配置**\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|server_addr|string|连接服务端的地址|0.0.0.0||\n|server_port|int|连接服务端的端口|7000||\n|connect_server_local_ip|string|连接服务端时所绑定的本地 IP|||\n|dial_server_timeout|int|连接服务端的超时时间|10||\n|http_proxy|string|连接服务端使用的代理地址|||格式为 {protocol}://user:passwd@192.168.1.128:8080 protocol 目前支持 http、socks5、ntlm\n|log_file|string|日志文件地址|./frpc.log||如果设置为 console，会将日志打印在标准输出中\n|log_level|string|日志等级|info|trace, debug, info, warn, error|\n|log_max_days|int|日志文件保留天数|3||\n|disable_log_color|bool|禁用标准输出中的日志颜色|false||\n|pool_count|int|连接池大小|0||\n|user|string|用户名|||设置此参数后，代理名称会被修改为 {user}.{proxyName}，避免代理名称和其他用户冲突\n|dns_server|string|使用 DNS 服务器地址|||默认使用系统配置的 DNS 服务器，指定此参数可以强制替换为自定义的 DNS 服务器地址\n|login_fail_exit|bool|第一次登陆失败后是否退出|true||\n|protocol|string|连接服务端的通信协议|tcp|tcp, kcp, websocket|\n|tls_enable|bool|启用 TLS 协议加密连接|false||\n|tls_cert_file|string|TLS 客户端证书文件路径|||\n|tls_key_file|string|TLS 客户端密钥文件路径|||\n|tls_trusted_ca_file|string|TLS CA 证书路径|||\n|tls_server_name|string|TLS Server 名称|||为空则使用 server_addr\n|disable_custom_tls_first_byte|bool|TLS 不发送 0x17|false||当为 true 时，不能端口复用\n|tcp_mux_keepalive_interval|int|tcp_mux 的心跳检查间隔时间|60||单位：秒\n|heartbeat_interval|int|向服务端发送心跳包的间隔时间|30||建议启用 tcp_mux_keepalive_interval，将此值设置为 -1\n|heartbeat_timeout|int|和服务端心跳的超时时间|90||\n|udp_packet_size|int|代理 UDP 服务时支持的最大包长度|1500||服务端和客户端的值需要一致\n|start|string|指定启用部分代理|||当配置了较多代理，但是只希望启用其中部分时可以通过此参数指定，默认为全部启用\n|meta_xxx|map|附加元数据|||会传递给服务端插件，提供附加能力\n\n**权限验证**\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|authentication_method|string|鉴权方式|token|token, oidc|需要和服务端一致\n|authenticate_heartbeats|bool|开启心跳消息鉴权|false||需要和服务端一致\n|authenticate_new_work_conns|bool|开启建立工作连接的鉴权|false||需要和服务端一致\n|token|string|鉴权使用的 token 值|||需要和服务端设置一样的值才能鉴权通过\n|oidc_client_id|string|oidc_client_id|||\n|oidc_client_secret|string|oidc_client_secret|||\n|oidc_audience|string|oidc_audience|||\n|oidc_token_endpoint_url|string|oidc_token_endpoint_url|||\n|oidc_additional_xxx|map|OIDC 附加参数|||map 结构，key 需要以 oidc_additional_ 开头\n|**UI**|||||\n\n|参数|类型|说明|默认值|可选值|备注\n|------|------\n|admin_addr|string|启用 AdminUI 监听的本地地址|0.0.0.0||\n|admin_port|int|启用 AdminUI 监听的本地端口|0||\n|admin_user|string|HTTP BasicAuth 用户名|||\n|admin_pwd|string|HTTP BasicAuth 密码|||\n|asserts_dir|string|静态资源目录|||AdminUI 使用的资源默认打包在二进制文件中，通过指定此参数使用自定义的静态资源\n\n**代理基础配置**\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|type|string|代理类型|是|tcp|tcp, udp, http, https, stcp, sudp, xtcp, tcpmux|\n|use_encryption|bool|是否启用加密功能|否|false||启用后该代理和服务端之间的通信内容都会被加密传输\n|use_compression|bool|是否启用压缩功能|否|false||启用后该代理和服务端之间的通信内容都会被压缩传输\n|proxy_protocol_version|string|启用|proxy protocol 协议的版本|否|v1, v2|如果启用，则 frpc 和本地服务建立连接后会发送 proxy protocol 的协议，包含了原请求的 IP 地址和端口等内容\n|bandwidth_limit|string|设置单个 proxy 的带宽限流|否|||单位为 MB 或 KB，0 表示不限制，如果启用，会作用于对应的 frpc\n\n**本地服务配置**\nlocal_ip 和 plugin 的配置必须配置一个，且只能生效一个，如果配置了 plugin，则 local_ip 配置无效。\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|local_ip|string|本地服务 IP|是|127.0.0.1||需要被代理的本地服务的 IP 地址，可以为所在 frpc 能访问到的任意 IP 地址\n|local_port|int|本地服务端口|是|||配合 local_ip\n|plugin|string|客户端插件名称|否||见客户端插件的功能说明|用于扩展 frpc 的能力，能够提供一些简单的本地服务，如果配置了 plugin，则 local_ip 和 local_port 无效，两者只能配置一个\n|plugin_params|map|客户端插件参数|否|||map 结构，key 需要都以 “plugin_” 开头，每一个 plugin 需要的参数也不一样，具体见客户端插件参数中的内容\n|**负载均衡和健康检查**||||||\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|group|string|负载均衡分组名称|否|||用户请求会以轮询的方式发送给同一个 group 中的代理\n|group_key|string|负载均衡分组密钥|否|||用于对负载均衡分组进行鉴权，group_key 相同的代理才会被加入到同一个分组中\n|health_check_type|string|健康检查类型|否||tcp,http|配置后启用健康检查功能，tcp 是连接成功则认为服务健康，http 要求接口返回 2xx 的状态码则认为服务健康\n|health_check_timeout_s|int|健康检查超时时间(秒)|否|3||执行检查任务的超时时间\n|health_check_max_failed|int|健康检查连续错误次数|否|1||连续检查错误多少次认为服务不健康\n|health_check_interval_s|int|健康检查周期(秒)|否|10||每隔多长时间进行一次健康检查\n|health_check_url|string|健康检查的 HTTP 接口|否|||如果 health_check_type 类型是 http，则需要配置此参数，指定发送 http 请求的 url，例如 “/health”\n\n**TCP**\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|remote_port|int|服务端绑定的端口|是|||用户访问此端口的请求会被转发到 local_ip:local_port\n\n**UDP**\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|remote_port|int|服务端绑定的端口|是||用户访问此端口的请求会被转发到 local_ip:local_port|\n\n**HTTP**\ncustom_domains 和 subdomain 必须要配置其中一个，两者可以同时生效。\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|custom_domains|[]string|v服务器绑定自定义域名|是(和 subdomain 两者必须配置一个)|||用户通过 vhost_http_port 访问的 HTTP 请求如果 Host 在 custom_domains 配置的域名中，则会被路由到此代理配置的本地服务\n|subdomain|string|自定义子域名|是(和 custom_domains 两者必须配置一个)|||和 custom_domains 作用相同，但是只需要指定子域名前缀，会结合服务端的 subdomain_host 生成最终绑定的域名\n|locations|[]string|URL 路由配置|否|||采用最大前缀匹配的规则，用户请求匹配响应的 location 配置，则会被路由到此代理\n|http_user|string|用户名|否|||如果配置此参数，暴露出去的 HTTP 服务需要采用 Basic Auth 的鉴权才能访问\n|http_pwd|string|密码|否|||结合 http_user 使用\n|host_header_rewrite|string|替换 Host header|否|||替换发送到本地服务 HTTP 请求中的 Host 字段\n|headers|map|替换 header|否|||map 中的 key 是要替换的 header 的 key，value 是替换后的内容\n\n**HTTPS**\ncustom_domains 和 subdomain 必须要配置其中一个，两者可以同时生效。\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|custom_domains|[]string|服务器绑定自定义域名|是(和 subdomain 两者必须配置一个)|||用户通过 vhost_http_port 访问的 HTTP 请求如果 Host 在 custom_domains 配置的域名中，则会被路由到此代理配置的本地服务\n|subdomain|string|自定义子域名|是(和 custom_domains 两者必须配置一个)|||和 custom_domains 作用相同，但是只需要指定子域名前缀，会结合服务端的 subdomain_host 生成最终绑定的域名\n|**STCP**||||||\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|role|string|角色|是|server|server,visitor|server 表示服务端，visitor 表示访问端\n|sk|string|密钥|是|||服务端和访问端的密钥需要一致，访问端才能访问到服务端\n|**SUDP**||||||\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|role|string|角色|是|server|server,visitor|server 表示服务端，visitor 表示访问端\n|sk|string|密钥|是|||服务端和访问端的密钥需要一致，访问端才能访问到服务端\n|**XTCP**||||||\n\n|参数|类型|说明|是否必须|默认值|可选值|备注\n|------|------\n|role|string|角色|是|server|server,visitor|server 表示服务端，visitor 表示访问端\n|sk|string|密钥|是|||服务端和访问端的密钥需要一致，访问端才能访问到服务端\n\n我们这边只配置最简单的方式，修改frpc.ini的配置文件为：\n\n```\n[common]\nserver_addr = 47.62.44.12\nserver_port = 22354\ntoken = {{token}}\n\n[nat1]\ntype = tcp\nlocal_ip = 192.168.1.100\nlocal_port = 8001\nremote_port = 1801\n\n\n[nat2]\ntype = tcp\nlocal_ip = 192.168.1.101\nlocal_port = 8080\nremote_port = 1802\n\n```\n\n`[common]`节点下面为客户端配置\n`[nat1]`和`[nat2]`为代理配置，详细的参数请参考上面的参数说明。\n\n运行前请确保服务端已经启动成功，打开 cmd 终端，在cmd终端中执行命令，终端必须在frp根目录， 在执行命令 `./frpc -c ./frpc.ini` 启动客户端\n\n上面示例的效果是：\n访问 `47.62.44.12:1801`就可以访问到 内网的`192.168.1.100:8001`\n访问 `47.62.44.12:1802`就可以访问到 内网的`192.168.1.102:8080`\n更多用法，请参考frp官方文档\n",
    "labels": [],
    "created_at": "2024-12-23T20:23:51Z"
  },
  {
    "id": 23,
    "title": "\n\n2024年AIAgets技术栈全面解析\n",
    "url": "https://mp.weixin.qq.com/s/0MosTOqNIQbmJIS_6J-u5w",
    "content": "\n**Letta 是一家专注于 AI agents 的公司，提供开源工具和云服务，帮助开发者构建、部署和管理具有记忆能力和工具调用能力的 AI agents。**\n\n原文：https://www.letta.com/blog/ai-agents-stack\n\n（由ChatGPT翻译）\n\n**理解AI Agents 的生态版图**\n\n尽管市面上已经有许多关于代理技术堆栈和代理市场的地图，但我们往往对其分类方式持不同意见，并发现它们很少真实反映开发者实际使用的工具和技术。在过去的几个月中，随着记忆机制、工具使用、安全执行以及部署等方面的进展，代理软件生态系统取得了显著发展。因此，我们决定基于我们一年多的开源AI开发经验和七年以上的AI研究积累，分享我们自己的“代理技术堆栈”。\n\n在基于封闭 API 的模型推理提供商中，OpenAI 和 Anthropic 凭借私有的前沿模型处于领先地位。Together.AI、Fireworks 和 Groq 是一些流行的选项，它们通过收费 API 提供开放权重模型（如 Llama 3）的服务。在本地模型推理提供商中，vLLM 是生产级 GPU 服务负载的领先者，而 SGLang 则是一个面向类似开发者群体的新兴项目。 \n\n对于爱好者（“AI 爱好者”），Ollama 和 LM Studio 是两种运行本地模型的热门选择，特别适合在个人电脑（例如 M 系列的苹果 MacBook）上运行模型。\n\n存储\n\n存储是构建具备状态的 agents 的基础模块之一。agents 的定义依赖于持久化的状态，例如其对话历史、记忆，以及用于 RAG（检索增强生成）的外部数据源。 \n\n向量数据库在存储 agents 的“外部记忆”方面非常流行，例如 Chroma、Weaviate、Pinecone、Qdrant 和 Milvus。它们允许 agents 利用无法放入上下文窗口的大量数据源和对话历史。此外，自上世纪 80 年代起便存在的传统数据库 Postgres，现在也通过 pgvector 扩展支持向量搜索。 \n\n一些基于 Postgres 的公司也在为 agents 提供嵌入式搜索与存储功能，例如 Neon（无服务器 Postgres）和 Supabase，它们同样是这一领域的重要玩家。\n\n工具& 库\n\n标准 AI 聊天机器人与 AI agents 之间的主要区别之一在于 agents 能够调用“工具”（或称“函数”）。在大多数情况下，这种调用机制是由 LLM 生成结构化输出（例如 JSON 对象），指定要调用的函数及其参数来实现的。 \n\n关于 agent 工具执行的一个常见误解是，工具的实际执行并非由 LLM 提供商完成。LLM 的职责仅限于选择调用哪个工具以及提供哪些参数。支持任意工具或参数的 agent 服务必须使用沙盒环境（如 Modal、E2B）来确保安全执行。 \n\n所有 agents 都通过 OpenAI 定义的 JSON 架构调用工具，这意味着不同框架下的 agents 和工具可以实现兼容。例如，Letta agents 能够调用 LangChain、CrewAI 和 Composio 的工具，因为它们遵循相同的架构。正因如此，一个为常用工具服务的生态系统正在快速成长。 \n\nComposio 是一个流行的通用工具库，同时负责授权管理；Browserbase 是用于网页浏览的专用工具；而 Exa 则提供了专注于网络搜索的工具。随着更多 agents 的开发，我们预计工具生态系统将进一步扩大，并引入新功能，例如针对 agents 的认证与访问控制服务。\n\nAgent框架\n\nAgent 框架：编排 LLM 调用与管理 agent 状态\n\n目前，大多数 agent 框架设计的 agents 仅限于存在于编写它们的 Python 脚本或 Jupyter notebook 中。然而，我们认为 agents 的未来应是作为服务存在，部署在本地或云基础设施中，并通过 REST API 提供访问。正如 OpenAI 的 ChatCompletion API 已成为与 LLM 服务交互的行业标准，我们预期未来也会出现一个统一的 Agents API 标准，但这一“赢家”尚未出现。 \n\n将 agents 部署为服务比将 LLM 部署为服务复杂得多，这主要归因于状态管理和安全工具执行的问题。工具及其所需的依赖和环境必须显式存储在数据库中，因为服务需要重新创建运行工具的环境（而当工具与 agents 在同一脚本中运行时，这并不是问题）。 \n\n此外，应用可能需要运行数百万个 agents，每个 agent 的对话历史不断增长。从原型开发到生产阶段，不可避免地需要对 agent 状态进行数据规范化处理，同时通过 REST API 定义 agent 的交互方式。目前，这一过程通常由开发者自行编写 FastAPI 和数据库代码来完成，但我们预计，随着 agents 的成熟，这些功能将更深地集成到框架中。 \n\n---\n\n结论\n\nAgent 技术栈仍处于非常早期的发展阶段，但我们对其生态系统未来的扩展和演变充满期待。\n\n\n\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-23T03:07:29Z"
  },
  {
    "id": 22,
    "title": "\n\nRAG：困境求生\n",
    "url": "https://mp.weixin.qq.com/s/g7qE2pOifIMj21W3qJFzNQ",
    "content": "   2023年进入RAG萌芽期，摸索前进；      2024上半年RAG到达狂热顶点；\n      2024下半年实际落地情况并不理想，渐入困境：一周出demo，半年不交付。   \n**Hype Cycle（技术成熟度曲线），当前RAG逐渐进入狂热之后的低谷期**\n\n展望2025年：RAG回归理性。\n   找到最小业务切入点，明确需求，定好指标      做好RAG落地标杆，毕竟谁都不愿意当小白鼠      <p >仍然关注新技术<img class=\"rich_pages wxw-img\" data-ratio=\"1\" src=\"https://res.wx.qq.com/t/wx_fed/we-emoji/res/v1.3.10/assets/newemoji/Yellowdog.png\" data-w=\"128\"  referrerpolicy=\"no-referrer\">\n\no3都发了，GPT5不远了<img src=\"https://res.wx.qq.com/t/wx_fed/we-emoji/res/v1.3.10/assets/newemoji/Yellowdog.png\" data-ratio=\"1\" data-w=\"128\"  referrerpolicy=\"no-referrer\">\n\n</p>   \n这两年，大模型RAG技术侧也经历了快速发展：\n   2023年RAG模块迭代：朴素RAG->高级RAG->模块化RAG      2024年RAG框架演化：RAG-> MM-RAG/GraphRAG->Agentic RAG   \n**7种大模型RAG技术架构**\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"100007619\" data-ratio=\"1.2453703703703705\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/AE74ia62XricEcicXHc33kGnU6C22AibibSwZ0KLhS5ws14vOELpuibwjMmw5GbjRKkc08nrjbSj6ThP2oyuyUJRpuPw/640?wx_fmt=png& from=appmsg\" data-type=\"jpeg\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n# Naive RAG框架\n\n# Retrieve-and-rerank框架\n\n# MM-RAG框架（多模态RAG）\n\n# Graph RAG框架\n\n<img class=\"rich_pages wxw-img\" data-imgfileid=\"100007805\" data-ratio=\"0.6274256144890039\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/AE74ia62XricFxO0piabBA9K2DATmuP5MPuAI38XBRRiaOxpLpiagmYoxRxDJ9xeh1OzoltEIh4olLO73g1ic00ia4P5Q/640?wx_fmt=other& from=appmsg& tp=webp& wxfrom=5& wx_lazy=1& wx_co=1\" data-w=\"773\" referrerpolicy=\"no-referrer\">\n\n\n\n# Hybrid RAG框架\n\n# Agentic RAG框架\n\n# Multi-Agent RAG框架\n            \n```\nVanilla RAG: https://weaviate.io/blog/introduction-to-rag\nAdvanced RAG: https://weaviate.io/blog/advanced-rag\nMultimodal RAG: https://weaviate.io/blog/multimodal-rag\nAgenticRAG:https://weaviate.io/blog/what-is-agentic-rag\n```\n\n推荐阅读\n<li >•[对齐LLM偏好的直接偏好优化方法：DPO、IPO、KTO](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==& mid=2247484447& idx=1& sn=f01188d29e2c5133addbd67229db4ee7& chksm=c2ce3e6ef5b9b77874aa250e55522bbbaf214df817ad5f5f1ff98135255863522daeebdf2d3b& scene=21#wechat_redirect)   <li >•[2024：ToB、Agent、多模态](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==& mid=2247483838& idx=1& sn=547104d60f0e4620417adf503eaaab9d& chksm=c2ce3bcff5b9b2d97bb4835bac4d00217f07603e64db9546c611dad53305dac4b32c6923f071& scene=21#wechat_redirect)   <li >•[RAG全景图：从RAG启蒙到高级RAG之36技，再到终章Agentic RAG！](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==& mid=2247487375& idx=1& sn=e16bc2fdaac04e58e99cfd2a1dc0b0cb& chksm=c2ce35fef5b9bce80dcf0a70b753707036fe7f962d7888d60396490841a538b0f46e952f53f0& scene=21#wechat_redirect)   <li >•[Agent到多模态Agent再到多模态Multi-Agents系统的发展与案例讲解（1.2万字，20+文献，27张图）](http://mp.weixin.qq.com/s?__biz=Mzk0MTYzMzMxMA==& mid=2247485322& idx=1& sn=71ffb345fca514aa5ce2848cb2c9f071& chksm=c2ce3dfbf5b9b4edd5b98e45c6179890bdea748fb5220636d25f42006954ea5c81afa8735725& scene=21#wechat_redirect)   \n欢迎关注我的公众号“**PaperAgent**”，每天一篇大模型（LLM）文章来锻炼我们的思维，简单的例子，不简单的方法，提升自己。\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-23T02:58:53Z"
  },
  {
    "id": 21,
    "title": "\n\n都在讲的AI产品架构，具体是个啥丨机器坏人\n",
    "url": "https://mp.weixin.qq.com/s/osQqdVrgX3wPNABUlta6dQ",
    "content": "\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021837\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyDDbs8ic7hicIickCC9Pgjw2Z5A6ZpBu3sQc7ialwiauQpIibbPy0Ha4r6Q3rg/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021843\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyD57ibhBCjiadSs2oMwGosd8eWeWgEJmBmpeZY3zibpZLTj01IqZcMa7YBQ/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021838\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyD2g1Upmhg7ZrMviayjvZu5qZEMEUKZicToZdiaJlT5bosdyFH9yplDSicBQ/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021836\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyDbTpw5I7U0Pd5HBBJsGjYNy02qF5EI9R2rMoRt6mOmuydtHBB9pzOhg/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021835\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyD0iboicpgEgfzcJPrDJp6XOT6brDa6XkInSKoI1ZWCZfI59rtSg8pDPCg/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021839\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyDZOTE3UddoWlxHxsxVbiaGdibGJ8kmjgV7JoZ3ahKDAPsOLLdiboqbLnOg/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021840\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyDaCWhrYHyeE9IhbOqQicrX4Kpn00wjNBbWtbTdEmP5rwPfRm566Kv6vQ/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021844\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyDa1zUywK7o2LxSCOruOfvicJZQdstGggsmGWYHNDicBn4iaCmfUp34JQ3Q/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<img class=\"rich_pages wxw-img\" data-backh=\"578\" data-backw=\"578\" data-galleryid=\"\" data-imgfileid=\"100021841\" data-ratio=\"1\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/sz_mmbiz_png/0v4p9yzE7NJG6ocicsZEzMM7FtnQsGCyDFTZbFa6iaicROmvvTt6JTDXxDhDy0dY8togjsCPtqqMDiaCUol1ndNSyw/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-23T02:49:52Z"
  },
  {
    "id": 15,
    "title": "使用ComposeDesktop开发一款桌面端多功能APK工具",
    "url": "https://www.jianshu.com/p/3890895ab180",
    "content": "\n# 前言\n\n终于算是忙完了一个阶段！！！从4月份开始，工作内容以及职务上都进行了较大的变动，最直接的就是从海外项目组调到了国内项目组。国内项目组目前有两个应用在同时跑着，而且还有几个马甲包也要维护，不知道大家发版的时候复杂不复杂，反正我们每次发版的时候都需要经历--打包、加固、对齐、重签名、打渠道包、上传云存储、生成渠道推广链接、生成内更SQL、上传Mapping文件等等步骤(xN)，简直是折磨人啊。\n\n所以首要任务就是做出一套自动化的基础设施来，最初直接考虑到的方案是【**Jenkins+Docker+360命令行加固+VasDolly+Bugly等**】的方案（下一篇文章会给大家分享该方案），整个过程下来基本能达到自动化的目的。就这么稳定的跑了一个多月，然而，在5月下旬的时候360加固发布了一个通知，大致内容就是免费版用户无法使用命令行的加固方式了，只能手动用工具加固。这就导致最初的方案直接垮掉，我花费了个把月学习Linux，Pipeline，Docker，还制作了各种镜像，结果突然不能用了，心塞。然而路还是要继续走下去的，在尽量不花钱的前提下，想到了开发桌面端工具的方案。\n\n# 功能一览\n\n接下来先给大家一览下桌面端工具的基本功能，我的电脑是Windows的，所以都是基于Windows平台下的build-tools相关工具进行开发的。首先大部分的功能都是基于jar或exe文件，那么在Java（Kotlin）中我们可以通过如下方式来调用这些外部程序，exec其实最终也是调用了ProcessBuilder，整体的原理就是如此：\n\n```\n//方式1\nRuntime.getRuntime().exec(cmd)\n\n//方式2\nProcessBuilder(cmd)\n\n```\n\n## 多渠道打包\n\n这是该工具最基本的功能，使用[**VasDolly**](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3Dhttps%253A%252F%252Fgithub.com%252FTencent%252FVasDolly)方案对APK文件进行多渠道打包（当然该APK文件需要是签名好的）。\n\n多渠道包命令行工具即 VasDolly.jar，该文件可以在上述GitHub仓库中找到，常用的命令如下：\n\n```\n// 获取指定APK的签名方式\njava -jar VasDolly.jar get -s [源apk地址]\n\n// 获取指定APK的渠道信息\njava -jar VasDolly.jar get -c [源apk地址]\n\n// 删除指定APK的渠道信息\njava -jar VasDolly.jar remove -c [源apk地址]\n\n// 通过指定渠道字符串添加渠道信息\njava -jar VasDolly.jar put -c \"channel1,channel2\" [源apk地址] [apk输出目录]\n\n// 通过指定某个渠道字符串添加渠道信息到目标APK\njava -jar VasDolly.jar put -c \"channel1\" [源apk地址] [输出apk地址]\n\n// 通过指定渠道文件添加渠道信息\njava -jar VasDolly.jar put -c channel.txt [源apk地址] [apk输出目录]\n\n// 提供了FastMode，生成渠道包时不进行强校验，速度可提升10倍以上\njava -jar VasDolly.jar put -c channel.txt -f [源apk地址] [apk输出目录]\n\n\n```\n\n## 对齐和签名\n\n上传应用市场前，APK文件大部分会被市场要求进行加固，无论是使用腾讯乐固还是360加固等方式，加固后APK的签名信息总会被破坏，所以我们需要对加固后的APK文件重新进行签名。\n\n### 配置签名\n\n首先我们需要准备好应用的签名信息，该工具支持导入签名文件，并保存相应的StorePass、KeyAlias、KeyPass信息，如下：\n\n当选择APK后，程序会判断选择的APK是否进行了签名，如果没有签名，那么就会弹窗提醒用户选择配置好的签名文件进行签名，签名之后才可进行多渠道打包的过程。\n\n注：该功能现已升级，添加签名文件的时候绑定包名，选择apk后会自动获取到包名然后查找到对应的签名文件自动对齐签名处理，无需手动进行选择了。\n\n### 对齐\n\n签名的过程则需要用到Android SDK中的两个文件，以Windows系统为例，一个是处理对齐的【build-tools\\版本号\\zipalign.exe】文件，另一个则是用来签名的【build-tools\\版本号\\lib\\apksigner.jar】文件。\n\n我们先看下zipalign工具的官方说明：\n\n> \nzipalign is a zip archive alignment tool. It ensures that all uncompressed files in the archive are aligned relative to the start of the file. This allows those files to be accessed directly via [mmap(2)](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3Dhttps%253A%252F%252Fman7.org%252Flinux%252Fman-pages%252Fman2%252Fmmap.2.html), removing the need to copy this data in RAM and reducing your app's memory usage. zipalign是一种zip归档对齐工具。它确保存档中所有未压缩的文件都与文件的开头对齐。这允许通过mmap直接访问这些文件，无需将这些数据复制到RAM中，并减少应用程序的内存使用。\nzipalign should be used to optimize your APK file before distributing it to end-users. This is done automatically if you build with Android Studio. This documentation is for maintainers of custom build systems. 在将APK文件分发给用户之前，应使用zipalign优化APK文件。如果您使用Android Studio进行构建，这将自动完成。本文档面向定制构建系统的维护人员。\n\n\nGoogle官方现在要求在使用apksigner对APK文件进行签名前需要先使用zipalign来优化APK文件，具体命令如下，以Windows下的zipalign.exe文件为例：\n\n```\n//对齐APK\nzipalign.exe -p -f -v 4 [源apk路径] [输出apk路径]\n\n//验证APK是否对齐\nzipalign.exe -c -v 4 [源apk路径]\n\n```\n\n其他相关的内容可以参阅官网 [zipalign](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3Dhttps%253A%252F%252Fdeveloper.android.google.cn%252Fstudio%252Fcommand-line%252Fzipalign) 。\n\n### 签名\n\n当APK文件对齐后，就可以给对齐后的APK进行签名操作了，签名的方法有两种，我们这里单说使用--ks选项指定密钥库的方式，具体命令如下：\n\n```\njava -jar apksigner.jar sign \n    --verbose \n    --ks [KeyStore文件路径] \n    --ks-pass pass:[KeyStorePass]\n    --ks-key-alias [KeyAlias]\n    --key-pass pass:[KeyPass]\n    --out [输出apk路径]\n    [源apk路径]\n\n```\n\n命令本身很简单，别搞错参数就好，尤其是两个密码的参数，后面需要使用【pass:密码】。输入密码这里还支持其他格式，如果有需要请参阅官网 [apksigner](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3Dhttps%253A%252F%252Fdeveloper.android.google.cn%252Fstudio%252Fcommand-line%252Fapksigner%253Fhl%253Den) 。\n\n加固、对齐、重签名后，这个apk就可以进行多渠道打包的处理了，然后即可发布到相关市场和渠道。\n\n## 其他内容\n\n在项目中还有很多其他的相关配置，比如发版的时候需要对APP进行应用内的更新通知。那么就需要我们填写发版的相关信息，版本名、版本号、更新日志等等内容都需要完善（可根据APK文件的命名来获取部分信息），然后通过这些信息生成应用内部更新的SQL语句，发送钉钉通知给相关后台人员处理。通知这一步又用到了钉钉的SDK，该工具支持配置钉钉机器人Webhook地址以及需要艾特的人员信息。\n\n打出来的这些包都需要统一上传到云存储上面，这一步使用了AWS的云存储SDK，可以配置云存储桶地址等信息，免去人工手动上传apk的烦恼。上传完毕后会根据文件名生成相应的下载链接并通知到钉钉群，以便市场人员获取到渠道最新的推广链接等。\n\n# 桌面端开发\n\n接下来就说下桌面端的开发过程，至于Compose MultiPlatform的介绍，请参阅[官网地址](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3Dhttps%253A%252F%252Fwww.jetbrains.com%252Flp%252Fcompose-mpp%252F)。本文主要就描述下一些针对桌面端的相关需求。\n\n## 弹窗\n\n关于弹窗，ComposeDesktop同样提供了Dialog可组合函数：\n\n```\n@Composable \npublic fun Dialog(\n    onCloseRequest: () -> kotlin.Unit, \n    state: androidx.compose.ui.window.DialogState, \n    visible: kotlin.Boolean, \n    title: kotlin.String, \n    icon: androidx.compose.ui.graphics.painter.Painter?, \n    undecorated: kotlin.Boolean, \n    transparent: kotlin.Boolean, \n    resizable: kotlin.Boolean, \n    enabled: kotlin.Boolean, \n    focusable: kotlin.Boolean, \n    onPreviewKeyEvent: (androidx.compose.ui.input.key.KeyEvent) -> kotlin.Boolean, \n    onKeyEvent: (androidx.compose.ui.input.key.KeyEvent) -> kotlin.Boolean, \n    content: @Composable() (DialogWindowScope.() -> kotlin.Unit)\n    ): kotlin.Unit { /* compiled code */ }\n\n\n```\n\n大部分的参数都可以直接看出他的作用，主要看一下state参数，该参数可以控制弹窗的位置及大小，例如我们配置一个在屏幕中央，宽高为500*300dp的弹窗，那么示例代码如下：\n\n```\nstate = DialogState(\n            position = WindowPosition(Alignment.Center),\n            size = DpSize(500.dp, 300.dp),\n        )\n\n```\n\n不过这个弹窗没有阴影，如果想添加的话可以内部套一层Surface来做出阴影效果：\n\n```\nSurface(\n    modifier = Modifier.fillMaxSize().padding(20.dp),\n    elevation = 10.dp,\n    shape = RoundedCornerShape(16.dp)\n)\n\n```\n\n## 文件选择器\n\n关于文件选择器这一块目前Compose还没有专门的函数，但是我们还是可以使用原有的方案：\n\n   javax.swing.JFileChooser   \n   java.awt.FileDialog   \n\n个人还是更偏向于使用JFileChooser，因为使用第二种方案的话，在页面重组的情况下总是会莫名的弹出选择框来。一个简单的文件选择器如下所示：\n\n```\nprivate fun showFileSelector(\n    suffixList: Array<String>,\n    onFileSelected: (String) -> Unit\n) {\n    JFileChooser().apply {\n        //设置页面风格\n        try {\n            val lookAndFeel = UIManager.getSystemLookAndFeelClassName()\n            UIManager.setLookAndFeel(lookAndFeel)\n            SwingUtilities.updateComponentTreeUI(this)\n        } catch (e: Throwable) {\n            e.printStackTrace()\n        }\n\n        fileSelectionMode = JFileChooser.FILES_ONLY\n        isMultiSelectionEnabled = false\n        fileFilter = FileNameExtensionFilter(\"文件过滤\", *suffixList)\n\n        val result = showOpenDialog(ComposeWindow())\n        if (result == JFileChooser.APPROVE_OPTION) {\n            val dir = this.currentDirectory\n            val file = this.selectedFile\n            println(\"Current apk dir: ${dir.absolutePath} ${dir.name}\")\n            println(\"Current apk name: ${file.absolutePath} ${file.name}\")\n            onFileSelected(file.absolutePath)\n        }\n    }\n}\n\n```\n\n该方式在使用的过程中也有一定的缺陷，就是每次打开文件弹窗总是会卡顿一下，所以后续也是有了寻找其他高效选择文件方式的想法。\n\n## 文件拖拽\n\n选择文件除了上面的弹窗选择方式，还有另一种神奇的方式 - 拖拽选择，本来也是没有头绪，然而在Slack闲逛的时候发现了Jim Sproch推荐了一篇相关的文章：[dev.to/tkuenneth/f…](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3Dhttps%253A%252F%252Fdev.to%252Ftkuenneth%252Ffrom-swing-to-jetpack-compose-desktop-2-4a4h) 。看完后也是恍然大悟，但是在Compose Desktop中，window是整个窗口，如何让某一个指定的区域响应我们的文件拖拽事件呢？\n\n还记得在Android上有ComposeView吧，用来嵌套原来的那一套View体系。那么在这里我也是采用了类似的这么一种方式，实例一个空的JPanel控件然后给它安排到window中去。具体位置及大小的设置呢，在Compose中可以通过 **onPlaced(onPlaced: (LayoutCoordinates) -> Unit)** 修饰符来获取到，示例代码如下所示：\n\n```\n@OptIn(ExperimentalComposeUiApi::class)\n@Composable\nfun DropBoxPanel(\n    modifier: Modifier,\n    window: ComposeWindow,\n    component: JPanel = JPanel(),\n    onFileDrop: (List<String>) -> Unit\n) {\n\n    val dropBoundsBean = remember {\n        mutableStateOf(DropBoundsBean())\n    }\n\n    Box(\n        modifier = modifier.onPlaced {\n            dropBoundsBean.value = DropBoundsBean(\n                x = it.positionInWindow().x,\n                y = it.positionInWindow().y,\n                width = it.size.width,\n                height = it.size.height\n            )\n        }) {\n        LaunchedEffect(true) {\n            component.setBounds(\n                dropBoundsBean.value.x.roundToInt(),\n                dropBoundsBean.value.y.roundToInt(),\n                dropBoundsBean.value.width,\n                dropBoundsBean.value.height\n            )\n            window.contentPane.add(component)\n\n            val target = object : DropTarget(component, object : DropTargetAdapter() {\n                override fun drop(event: DropTargetDropEvent) {\n\n                    event.acceptDrop(DnDConstants.ACTION_REFERENCE)\n                    val dataFlavors = event.transferable.transferDataFlavors\n                    dataFlavors.forEach {\n                        if (it == DataFlavor.javaFileListFlavor) {\n                            val list = event.transferable.getTransferData(it) as List<*>\n\n                            val pathList = mutableListOf<String>()\n                            list.forEach { filePath ->\n                                pathList.add(filePath.toString())\n                            }\n                            onFileDrop(pathList)\n                        }\n                    }\n                    event.dropComplete(true)\n\n                }\n            }) {\n\n            }\n        }\n\n        SideEffect {\n            component.setBounds(\n                dropBoundsBean.value.x.roundToInt(),\n                dropBoundsBean.value.y.roundToInt(),\n                dropBoundsBean.value.width,\n                dropBoundsBean.value.height\n            )\n        }\n\n        DisposableEffect(true) {\n            onDispose {\n                window.contentPane.remove(component)\n            }\n        }\n    }\n}\n\n```\n\n实际运行效果如下，个人感觉基本还是能达到目的的。\n\n## 数据的保存\n\n最开始的时候，功能很少，每个配置的数据都是使用了txt文件来一行行保存，但是到了后来功能越来越复杂，单纯的按行来处理貌似有点捉襟见肘了，所以考虑使用json来保存复杂的类型数据。\n\njson数据的处理从原生JSON到FastJson，Gson，Moshi等都已经体验过了，于是乎便采用了之前未使用过的[**Jackson**](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3D)。然而不得不说，就目前为止，jackson是我用过最简洁、优雅的一款解析库。\n\n假如我有一个List类型的列表数据，那么当我要把这个数据存储到文件的时候只需：\n\n```\njacksonObjectMapper().writeValue(File, List<String>)\n\n```\n\n而从文件中读取数据也是简单的狠啊：\n\n```\n//方式1\nval list = jacksonObjectMapper().readValue<List<String>>(jsonFile)\n\n//方式2\nval list : List<String> = jacksonObjectMapper().readValue(jsonFile)\n\n```\n\n这种简洁真的是深入我心。继续深入了解下Jackson，你会发现它的可扩展性以及可定制性都很强，简直相见恨晚啊。之前也是在一个舒适圈待习惯了，这次主动跳出来居然有了意想不到的收获。\n\n但是呢，每个框架也会有它自己的注意点，比如jackson，属性命名不可以是is开头，否则序列化等就会报错。这点似乎在阿里巴巴JAVA手册中好像也有提到，具体原因请大家自行百度（Google）。\n\n## 资源的拷贝\n\n当我们使用[java -jar xxx.jar]命令执行jar文件的时候，需要明确指定 jar文件的地址，但是在Compose Desktop中我们要怎么存放并读取这个jar文件呢 ？我们可以从Compose Desktop中读取并展示图片的相关代码中得到启发，假如有一个sample.svg图标文件存放到了项目的 resources 文件夹下，那么我们在引用这张图片的时候就可以使用：\n\n```\npainterResource(\"sample.svg\")\n\n```\n\n我们点进去这个方法看下：\n\n```\n@OptIn(ExperimentalComposeUiApi::class)\n@Composable\nfun painterResource(\n    resourcePath: String\n): Painter = painterResource(\n    resourcePath,\n    ResourceLoader.Default\n)\n\n@ExperimentalComposeUiApi\n@Composable\nfun painterResource(\n    resourcePath: String,\n    loader: ResourceLoader\n): Painter = when (resourcePath.substringAfterLast(\".\")) {\n    \"svg\" -> rememberSvgResource(resourcePath, loader)\n    \"xml\" -> rememberVectorXmlResource(resourcePath, loader)\n    else -> rememberBitmapResource(resourcePath, loader)\n}\n\n```\n\n里面居然有个ResourceLoader类，这名字一听就有戏啊，大概率就是我们需要的内容，而传递的默认参数是ResourceLoader.Default，那么就看下Default的源码吧：\n\n```\n//==========Resources.desktop.kt文件==========\n@ExperimentalComposeUiApi\ninterface ResourceLoader {\n    companion object {\n        /**\n         * Resource loader which is capable to load resources from `resources` folder in an application's\n         * project. Ability to load from dependent modules resources is not guaranteed in the future.\n         * Use explicit `ClassLoaderResourceLoader` instance if such guarantee is needed.\n         */\n        @ExperimentalComposeUiApi\n        val Default = ClassLoaderResourceLoader()\n    }\n    fun load(resourcePath: String): InputStream\n}\n\n@ExperimentalComposeUiApi\nclass ClassLoaderResourceLoader : ResourceLoader {\n    override fun load(resourcePath: String): InputStream {\n        // TODO(https://github.com/JetBrains/compose-jb/issues/618): probably we shouldn't use\n        //  contextClassLoader here, as it is not defined in threads created by non-JVM\n        val contextClassLoader = Thread.currentThread().contextClassLoader!!\n        val resource = contextClassLoader.getResourceAsStream(resourcePath)\n            ?: (::ClassLoaderResourceLoader.javaClass).getResourceAsStream(resourcePath)\n        return requireNotNull(resource) { \"Resource $resourcePath not found\" }\n    }\n}\n\n//==========ClassLoader类==========\npublic InputStream getResourceAsStream(String name) {\n    Objects.requireNonNull(name);\n    URL url = getResource(name);\n    try {\n        return url != null ? url.openStream() : null;\n    } catch (IOException e) {\n        return null;\n    }\n}\n\npublic URL getResource(String name) {\n    Objects.requireNonNull(name);\n    URL url;\n    if (parent != null) {\n        url = parent.getResource(name);\n    } else {\n        url = BootLoader.findResource(name);\n    }\n    if (url == null) {\n        url = findResource(name);\n    }\n    return url;\n}\n\n```\n\n上述源码的整个逻辑基本上就是两步，根据资源文件名获取到资源文件，然后获取资源文件的输入流。看到这里其实我们已经有两种方案了：\n\n   方案一：直接拿到文件的URL然后获取到文件的路径   \n   方案二：根据文件的输入流，将文件重新保存到本机相关目录   \n\n然而事情并没有这么简单，如果我们使用方案一，那么在编译运行的时候完全没有问题，所有的资源文件会被保存到【\\build\\processedResources\\jvm】下，此时我们直接可以通过文件的URL获取到文件路径，然后调用即可。但是，当我们打包成安装包后，例如在Windows下使用packageMsi命令打包出msi文件并安装到电脑上后，运行程序，这时候你就会发现资源文件所在的路径就很奇怪，例如我的工程下是【C:\\Program Files\\工程名\\app\\工程名-jvm-1.0-SNAPSHOT-xxxxxx.jar**!/**资源文件名】，也就是说所有的资源文件被打包进了这个快照文件，如果此时直接使用该路径运行java -jar 等命令，那么肯定就会报错了。\n\n所以最稳妥的方式还是使用方案二，使用ResourceLoader获取到资源文件流然后重新保存到本机上的相关目录就好了，伪代码如下：\n\n```\nResourceLoader.Default.load(resourcesPath)\n    .use { inputStream ->\n        val fos = FileOutputStream(file)\n        val buffer = ByteArray(1024)\n        var len: Int\n            while (((inputStream.read(buffer).also { len = it })) != -1) {\n                fos.write(buffer, 0, len)\n                }\n          fos.flush()\n              inputStream.close()\n              fos.close()\n          }\n\n```\n\n## 打包MSI\n\n在Windows环境下打包Msi格式安装包的时候，有一个downloadWix的Task，该Task涉及到了Wix资源的下载，如下 ：\n\n> \nTask :downloadWix Download [github.com/wixtoolset/…](https://links.jianshu.com/go?to=https%3A%2F%2Flink.juejin.cn%3Ftarget%3Dhttps%253A%252F%252Fgithub.com%252Fwixtoolset%252Fwix3%252Freleases%252Fdownload%252Fwix3112rtm%252Fwix311-binaries.zip)\n\n\n在IDEA中下载可能会非常的缓慢，此时我们可以复制上述地址，登上梯子，然后直接去GitHub下载。下载完毕后直接放入【/build/wixToolset】目录下即可，再次编译速度就会起飞了。\n\n# 总结\n\n简直没想到啊，作为一个Android开发者，现在借助Compose Desktop开发起桌面端居然能这么的轻车熟路，我对Compose真是越来越喜欢了。\n\n另外呢，跳出业务这一段时间来处理这些东西也让我对干预APK的打包等过程从理论迈出了实践的一步，同时对市场和运营同学的工作也有了更多了解，通过该工具帮助其处理了部分重复机械式的工作，部门间的感情也得到了进一步的增温（狗头滑稽）。\n\n就编到这吧，桌面工具还需要持续的维护跟优化，基本是面向市场和运营同事编程了。关于开头说的Jenkins那一套其实早就写好了，是鄙人少有的万字长文，但是中间变故太大，一直也没发布出来，接下来会重新整理下并发布，还请大家多多指正。\n\n> \n<p>作者：乐翁龙\n\n链接：[https://juejin.cn/post/7122645579439538183](https://links.jianshu.com/go?to=https%3A%2F%2Fjuejin.cn%2Fpost%2F7122645579439538183)</p>\n\n",
    "labels": [],
    "created_at": "2024-12-21T04:21:03Z"
  },
  {
    "id": 16,
    "title": "1.kuberetes的安装与部署",
    "url": "https://www.jianshu.com/p/a4ec985977c2",
    "content": "\n> \n<p>环境信息: centos7.9 + kubernetes 1.23.8 + docker 20.10.17 + virtualBox 6.1\n\n文章编写时间: 2022-06-30\n\n部署方式: kubeadm\n\n组件: 网络组件calico、dashboard组件</p>\n\n\n## 一、前置工作与注意事项\n\n   这里我们的centos使用的是 **centos7.9**, 不同版本的系统对k8s影响较大，具体看实际情况而定。 有的还需要更新系统内核。   \n   \n我们先准备了3台虚拟机，配置好网络(映射好ssh端口)。虚拟机情况如下：\n<pre><code class=\"shell\">k8s_master-1  192.168.56.105\nk8s_slave-2   192.168.56.106\nk8s_slave-3   192.168.56.107\n</code></pre>\nhosts配置(每台机器都需要设置)\n<pre><code class=\"shell\">vim /etc/hosts\n\n192.168.56.105 k8s-master01\n192.168.56.106 k8s-slave02\n192.168.56.107 k8s-slave03\n</code></pre>\nhostname配置(每台机器都需要配置，这里我们以192.168.56.105 为例，我们需要设置hostname为 \"k8s_master-1\" ，与hosts 相匹配)\n如果不配置hostname 默认会配置为`localhost.localdomain`，k8s 运行时会报错`Error getting node\" err=\"node \\\"localhost.localdomain\\\" not found`\n<pre><code class=\"shell\"># 设置当前机器的hostname\nhostnamectl set-hostname k8s-master01\n# 查看当前机器hostname\nhostname\n</code></pre>\n   \n   系统配置要求：**2c** **2g** **20g** 以上,， cpu 至少为2核，否则k8s初始化无法成功。 建议`master`节点内存给4g   \n   \nk8s安装有多种方式:\n<ol>\n   使用minikube安装单节点集群，用于测试   \n   \n**采用工具kubeadm -- 我们使用的这种方式**（开发环境，机器比较少(几十台以下)）   \n   使用kubespray, google官方提供的工具   \n   全手动: 二进制安装(运维)   \n   全自动安装: rancher、kubesphere (大型生产环境，百台，万台机器)   \n</ol>\n   \n   \nk8s health会依赖一些端口，为了不出现网络问题，我们在**虚拟机**(master)中**开放**以下端口：\n<ul>\n   **6443**         **主要**   \n   2379   \n   2380   \n   kubeadm 帮助我们安装的`ca 证书`时限是**一年**，所以不推荐正式环境使用，或需要手动配置ca证书。   \n\n## 二、安装\n\n### 1. 初始准备\n\n以下为实际安装步骤与流程:\n\n```\n# 基础依赖包安装\n yum -y install wget vim net-tools ntpdate  bash-completion\n \n# 设置当前机器的hostname\nhostnamectl set-hostname k8s-master01\n# 查看当前机器hostname\nhostname\n\n```\n\n0.系统时钟同步\n\n```\n# 向阿里云服务器同步时间\nntpdate time1.aliyun.com\n# 删除本地时间并设置时区为上海\nrm -rf /etc/localtime\nln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\n# 查看时间\ndate -R || date\n\n```\n\n1.关闭防火墙、selinux\n\n```\n systemctl stop firewalld\n systemctl disable firewalld\n sed -i 's/enforcing/disabled/' /etc/selinux/config\n setenforce 0\n\n```\n\n3.**关闭swap**\n\n```\n# 临时关闭Swap\nswapoff -a\n# 修改 /etc/fstab 删除或者注释掉swap的挂载，可永久关闭swap\nsed -i '/swap/s/^/#/' /etc/fstab\n# 修改完后我们检测以下，看最后一行swap 都是0 就成功了\nfree -m\n#----------------start----------------------\n              total        used        free      shared  buff/cache   available\nMem:           1837         721          69          10        1046         944\nSwap:             0           0           0\n#-----------------end---------------------\n\n```\n\n4.网桥过滤\n\n```\n# 网桥过滤\nvim /etc/sysctl.conf\n\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-arptables = 1\nnet.ipv4.ip_forward=1\nnet.ipv4.ip_forward_use_pmtu = 0\n\n# 生效命令\nsysctl --system\n# 查看效果\nsysctl -a|grep \"ip_forward\"\n\n```\n\n5.开启**ipvs**（kubernetes1.8版本开始，新增了kube-proxy对**ipvs**的支持，性能和追踪问题比iptable强）------ 此步骤为选填项，如果不执行那么默认使用iptables\n\n```\n# 安装IPVS\nyum -y install ipset ipvsdm\n# 编译ipvs.modules文件\nvi /etc/sysconfig/modules/ipvs.modules\n# 文件内容如下\n#!/bin/bash\nmodprobe -- ip_vs\nmodprobe -- ip_vs_rr\nmodprobe -- ip_vs_wrr\nmodprobe -- ip_vs_sh\nmodprobe -- nf_conntrack_ipv4\n# 赋予权限并执行\nchmod 755 /etc/sysconfig/modules/ipvs.modules & &  bash /etc/sysconfig/modules/ipvs.modules & & lsmod | grep -e ip_vs -e nf_conntrack_ipv4\n# 重启电脑，检查是否生效\nreboot\nlsmod | grep ip_vs_rr\n\n```\n\n**修改hosts文件**，添加解析\n\n```\nvim /etc/hosts\n\n192.168.56.105 k8s-master01\n192.168.56.106 k8s-slave02\n192.168.56.107 k8s-slave03\n\n```\n\n### 2. docker安装\n\ndocker 换源安装\n\n```\n# 安装yum utils\nyum install -y yum-utils\n# yum docker-ce config 换源\nyum-config-manager \\\n    --add-repo \\\n    https://download.docker.com/linux/centos/docker-ce.repo\n# 安装docker\nyum -y install docker-ce docker-ce-cli containerd.io\n# 启动docker， enable 为必须，k8s会检测docker.service\nsystemctl enable docker & &  systemctl start docker\n\n```\n\ndocker配置镜像加速\n\n```\n# 创建docker目录\nmkdir -p /etc/docker\n# 设置镜像源, exec-opts必须指定否则k8s启动报错（cgroup、systemd）\ntee /etc/docker/daemon.json <<-'EOF'\n{\n  \"exec-opts\": [\"native.cgroupdriver=systemd\"],\n  \"registry-mirrors\": [\"https://fl791z1h.mirror.aliyuncs.com\"]\n}\nEOF\n# 重启docke并生效镜像加速\nsystemctl daemon-reload & &  systemctl restart docker\n\n```\n\n### 3. k8s安装\n\n配置kubernetes源\n\n```\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\n```\n\n**注意：阿里源并未与官网同步gpg（由于官网未开放同步方式, 可能会有索引gpg检查失败的情况，这时请用如下命令安装）**\n\n安装kubernets,**最好指定版本，否则会使用最新版本**(k8s 每个版本的变化都比较大，这里我们的k8s使用1.23.8 版本)\n\n```\n# 检测可用的k8s版本(--nogpgcheck 忽略gpg检测)\nyum list --nogpgcheck  --showduplicates kubeadm --disableexcludes=kubernetes\n# 找到我们想要安装的版本，并安装--------------这里我们用1.23.8版本，最新版目前是1.24.0 版本安装启用了docker 会有一些问题。\n# 安装kubelet、kubeadm、kubectl 组件--- 这里要注意，docker 版本和 k8s版本有关系，尽量使用支持区间的版本\n# yum install --nogpgcheck kubelet-1.23.8 kubeadm-1.23.8 kubectl-1.23.8\nyum -y install --nogpgcheck kubelet-1.23.8 kubeadm-1.23.8 kubectl-1.23.8\n\n```\n\n安装完成后我们检查一下\n\n```\n# 检查kubectl version\nkubectl version\n##########show start############\nClient Version: version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.23.8\", GitCommit:\"5575935422cc1cf5169dfc8847cb587aa47bac5a\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T13:00:45Z\", GoVersion:\"go1.15.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n###########show end###########\n# 检查kubeadm版本\nkubeadm version\n##########show start############\nkubeadm version: & version.Info{Major:\"1\", Minor:\"20\", GitVersion:\"v1.23.8\", GitCommit:\"5575935422cc1cf5169dfc8847cb587aa47bac5a\", GitTreeState:\"clean\", BuildDate:\"2021-06-16T12:58:46Z\", GoVersion:\"go1.15.13\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n##########show end############\n\n```\n\n启动k8s服务\n\n```\n# 启动k8s服务\nsystemctl enable kubelet & &  systemctl start kubelet\n# 查看服务状态\nsystemctl status kubelet\n\n# 如果不指定版本初始化那么会使用最新的k8s，有可能存在的报错信息如下，需要先手动设置下\n# 运行初始化后有可能会报错:\n#########错误信息--start###########\n[init] Using Kubernetes version: v1.24.2\n[preflight] Running pre-flight checks\nerror execution phase preflight: [preflight] Some fatal errors occurred:\n        [ERROR CRI]: container runtime is not running: output: E0627 16:44:11.772277   16359 remote_runtime.go:925] \"Status from runtime service failed\" err=\"rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService\"\ntime=\"2022-06-27T16:44:11+08:00\" level=fatal msg=\"getting status of runtime: rpc error: code = Unimplemented desc = unknown service runtime.v1alpha2.RuntimeService\"\n, error: exit status 1\n[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\nTo see the stack trace of this error execute with --v=5 or higher\n#########错误信息--end###########\n# 解决方案：\nvim /etc/containerd/config.toml\n# 从disabled_plugins 数组值中删除 \"cri\"属性\n# 重启容器\nsystemctl restart containerd\n\n```\n\nk8s **master 主节点初始化**（**仅master节点执行**-- 这里考虑的是单master，多slave）\n\n```\n# 初始化\nkubeadm init  \\\n--image-repository registry.aliyuncs.com/google_containers  \\\n--apiserver-advertise-address=192.168.56.105  \\\n--service-cidr=10.222.0.0/16 \\\n--pod-network-cidr=10.244.0.0/16\n\n# 初始化过程比较长，需要下载一些资源\n\n#---------------打印信息 start---------------------\nI0628 15:14:20.293469    5655 version.go:255] remote version is much newer: v1.24.2; falling back to: stable-1.23\n[init] Using Kubernetes version: v1.23.8\n[preflight] Running pre-flight checks\n[preflight] Pulling images required for setting up a Kubernetes cluster\n[preflight] This might take a minute or two, depending on the speed of your internet connection\n[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'\n[certs] Using certificateDir folder \"/etc/kubernetes/pki\"\n[certs] Generating \"ca\" certificate and key\n[certs] Generating \"apiserver\" certificate and key\n[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local localhost.localdomain] and IPs [10.222.0.1 192.168.56.105]\n[certs] Generating \"apiserver-kubelet-client\" certificate and key\n[certs] Generating \"front-proxy-ca\" certificate and key\n[certs] Generating \"front-proxy-client\" certificate and key\n[certs] Generating \"etcd/ca\" certificate and key\n[certs] Generating \"etcd/server\" certificate and key\n[certs] etcd/server serving cert is signed for DNS names [localhost localhost.localdomain] and IPs [192.168.56.105 127.0.0.1 ::1]\n[certs] Generating \"etcd/peer\" certificate and key\n[certs] etcd/peer serving cert is signed for DNS names [localhost localhost.localdomain] and IPs [192.168.56.105 127.0.0.1 ::1]\n[certs] Generating \"etcd/healthcheck-client\" certificate and key\n[certs] Generating \"apiserver-etcd-client\" certificate and key\n[certs] Generating \"sa\" key and public key\n[kubeconfig] Using kubeconfig folder \"/etc/kubernetes\"\n[kubeconfig] Writing \"admin.conf\" kubeconfig file\n[kubeconfig] Writing \"kubelet.conf\" kubeconfig file\n[kubeconfig] Writing \"controller-manager.conf\" kubeconfig file\n[kubeconfig] Writing \"scheduler.conf\" kubeconfig file\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Starting the kubelet\n[control-plane] Using manifest folder \"/etc/kubernetes/manifests\"\n[control-plane] Creating static Pod manifest for \"kube-apiserver\"\n[control-plane] Creating static Pod manifest for \"kube-controller-manager\"\n[control-plane] Creating static Pod manifest for \"kube-scheduler\"\n[etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\"\n[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s\n[apiclient] All control plane components are healthy after 4.502756 seconds\n[upload-config] Storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.23\" in namespace kube-system with the configuration for the kubelets in the cluster\nNOTE: The \"kubelet-config-1.23\" naming of the kubelet ConfigMap is deprecated. Once the UnversionedKubeletConfigMap feature gate graduates to Beta the default name will become just \"kubelet-config\". Kubeadm upgrade will handle this transition transparently.\n[upload-certs] Skipping phase. Please see --upload-certs\n[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the labels: [node-role.kubernetes.io/master(deprecated) node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]\n[mark-control-plane] Marking the node localhost.localdomain as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[bootstrap-token] Using token: 4yipfl.er9r8aqnq0hpd8a4\n[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes\n[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstrap-token] Creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[kubelet-finalize] Updating \"/etc/kubernetes/kubelet.conf\" to point to a rotatable kubelet client certificate and key\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes control-plane has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nAlternatively, if you are the root user, you can run:\n\n  export KUBECONFIG=/etc/kubernetes/admin.conf\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 192.168.56.105:6443 --token 4yipfl.er9r8aqnq0hpd8a4 \\\n        --discovery-token-ca-cert-hash sha256:afa76da3ced528e667374693fb4b0edd160530c251471ae11ece13c65d3d162a\n\n#---------------打印信息 end---------------------\n\n```\n\n初始化参数解析\n\n|参数名|示例值|含义\n|------|------\n|--kubernetes-version|v1.23.8|版本\n|--apiserver-advertise-address|192.168.56.105|当前机器节点ip\n|--image-repository|registry.aliyuncs.com/google_containers|镜像仓库\n|--service-cidr|10.222.0.0/16|service 网段\n|--pod-network-cidr|10.244.0.0/16|k8s内部pod节点直接网段，不能和--service-cide相同\n\n**至此，kubeadm init(master 主节点)安装完成。**\n\n还需要进行一些收尾工作，根据`kubeadm init log` 提示，执行以下命令\n\n```\nmkdir -p $HOME/.kube\ncp -i /etc/kubernetes/admin.conf $HOME/.kube/config\nchown $(id -u):$(id -g) $HOME/.kube/config\n\n```\n\n然后，我们就可以进行**k8s的节点查询**\n\n```\n# 查询节点\nkubectl get nodes\n\n#------------展示信息 start--------------\nNAME           STATUS     ROLES                  AGE     VERSION\nk8s-master01   NotReady   control-plane,master   6m21s   v1.23.8\n#------------展示信息 end----------------\n\n```\n\n此时， STATUS 是 `NotReady`状态，因为网络组件还未安装,Pod之间还不能通讯\n\n查看各命名空间下的Pod信息\n\n```\nkubectl get pods --all-namespaces\n\n#------------展示信息 start--------------\n\nNAMESPACE     NAME                                   READY   STATUS    RESTARTS   AGE\nkube-system   coredns-6d8c4cb4d-5bjmr                0/1     Pending   0          12m\nkube-system   coredns-6d8c4cb4d-7w72l                0/1     Pending   0          12m\nkube-system   etcd-k8s-master01                      1/1     Running   0          12m\nkube-system   kube-apiserver-k8s-master01            1/1     Running   0          12m\nkube-system   kube-controller-manager-k8s-master01   1/1     Running   0          12m\nkube-system   kube-proxy-rcsfg                       1/1     Running   0          12m\nkube-system   kube-scheduler-k8s-master01            1/1     Running   0          12m\n#------------展示信息 end----------------\n\n```\n\n可以看到NDS解析服务`coredns`的pod还处于Pending状态未运行，也是因为网络组件还没安装\n\n## 三、网络插件的安装\n\n下面我们进行网络组件的安装\n\n### 1. 常用网络插件\n\n这里只简单说明下，推荐使用calico。\n\nflannel 和 calico 是常用的网络插件。\n\ncalico 的性能更好，使用场景更广一些。\n\nflannel 没有网络策略，不能控制pod的访问。\n\n这里我们用calico插件\n\n### 2. 插件安装\n\n```\n# calico插件安装\nkubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml\n\n# 插件安装过程较慢，请耐心等待\n# 安装后我们查看pod状态,直到 所有 STATUS 为 Running 才启动成功\nkubectl get pod --all-namespaces\n\n#----------显示如下 start-------------\nNAMESPACE     NAME                                       READY   STATUS    RESTARTS   AGE\nkube-system   calico-kube-controllers-7bc6547ffb-2bnbh   1/1     Running   0          5m57s\nkube-system   calico-node-rnhcv                          1/1     Running   0          5m57s\nkube-system   coredns-6d8c4cb4d-5bjmr                    1/1     Running   0          90m\nkube-system   coredns-6d8c4cb4d-7w72l                    1/1     Running   0          90m\nkube-system   etcd-k8s-master01                          1/1     Running   0          91m\nkube-system   kube-apiserver-k8s-master01                1/1     Running   0          91m\nkube-system   kube-controller-manager-k8s-master01       1/1     Running   0          91m\nkube-system   kube-proxy-rcsfg                           1/1     Running   0          90m\nkube-system   kube-scheduler-k8s-master01                1/1     Running   0          91m\n#----------显示如下 end---------------\n\n# 查看k8s node 状态\nkubectl get nodes\n\n#----------显示如下 start-------------\nk8s-master01   Ready    control-plane,master   99m   v1.23.8\n#----------显示如下 end---------------\n\n```\n\n## 四、安装dashboard\n\n### 1.注意事项\n\ndashboard 在github上开源地址:\n\ndashboard 的版本和 k8s的版本有关系，因为每个k8s版本改动较大，所以在选择dashboard时尽量选择兼容的版本，否则某些功能有可能使用异常。\n\n版本的选择方式：\n\n   github 点击`releases`   \n   \n查看dashboard版本的兼容情况，并选择对应版本\n\n\n\n\n\n<img data-original-src=\"//upload-images.jianshu.io/upload_images/18299538-3b2aea8e065ed880.png\" data-original-width=\"730\" data-original-height=\"819\" data-original-format=\"image/png\" data-original-filesize=\"94992\" referrerpolicy=\"no-referrer\">\n\n\n\nimage.png\n\n   \n\n3.找到支持的兼容版本，复制安装语句并执行\n\n### 2.安装\n\n```\n# 安装dashboard\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.5.1/aio/deploy/recommended.yaml\n\n#----------显示如下 start-------------\nnamespace/kubernetes-dashboard configured\nserviceaccount/kubernetes-dashboard created\nservice/kubernetes-dashboard created\nsecret/kubernetes-dashboard-certs created\nsecret/kubernetes-dashboard-csrf created\nsecret/kubernetes-dashboard-key-holder created\nconfigmap/kubernetes-dashboard-settings created\nrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrole.rbac.authorization.k8s.io/kubernetes-dashboard created\nrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\nclusterrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard created\ndeployment.apps/kubernetes-dashboard created\nservice/dashboard-metrics-scraper created\ndeployment.apps/dashboard-metrics-scraper created\n#----------显示如下 end---------------\n\n# 配置dashboard访问端口等信息\nvim k8s-dashboard.yaml\n\n#----------编辑内容 start------------- \nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort\n  ports:\n    - port: 443\n      targetPort: 8443\n      # 对外暴露的端口（端口范围 30000~32767）\n      nodePort: 30443\n  selector:\n    k8s-app: kubernetes-dashboard\n\n#----------编辑内容 end--------------- \n\n# 运行yaml\nkubectl apply -f k8s-dashboard.yaml\n\n#----------显示如下 start-------------\nservice/kubernetes-dashboard created\n#----------显示如下 end---------------\n\n# 这样我们的dashboard服务就部署完毕了，下面我们验证下\n# 查看pod，我们会看到两个pod 的status 都是`Running`\nkubectl get pods -n kubernetes-dashboard\n\n#----------显示如下 start-------------\nNAME                                         READY   STATUS    RESTARTS   AGE\ndashboard-metrics-scraper-799d786dbf-j85zw   1/1     Running   0          18m\nkubernetes-dashboard-fb8648fd9-qcrzk         1/1     Running   0          18m\n#----------显示如下 end---------------\n\n# 查看服务\nkubectl get svc -n kubernetes-dashboard -o wide\n#----------显示如下 start-------------\nNAME                        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)         AGE   SELECTOR\ndashboard-metrics-scraper   ClusterIP   10.222.211.134   <none>        8000/TCP        19m   k8s-app=dashboard-metrics-scraper\nkubernetes-dashboard        NodePort    10.222.156.236   <none>        443:30443/TCP   19m   k8s-app=kubernetes-dashboard\n#----------显示如下 end---------------\n\n```\n\n上面我们说到dashboard 开放的端口是`30443`，我们的虚拟机也需要对外暴露该端口\n\n\n\n\n\n设置好后，我们访问`https://192.168.56.105:30443`（注意：这里一定要用**https**）\n\n\n\n\n\n### 3.权限配置与登录验证\n\n上面我们已经完成了dashboard的安装，但是我们发现登录的时候需要使用`Token`,下边我们就说下Token的生成和配置\n\n```\n# 为了规范化处理，我们制定配置文件路径\ncd /opt/kube-dashboard/conf\n# 创建rbac配置文件\nvim admin-user-dashboard.yaml\n\n#----------编辑内容 start------------- \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n \n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n \n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io/autoupdate: \"true\"\n  labels:\n    kubernetes.io/bootstrapping: rbac-defaults\n  name: cluster-view\nrules:\n- apiGroups:\n  - '*'\n  resources:\n  - '*'\n  verbs:\n  - get\n  - list\n  - watch\n- nonResourceURLs:\n  - '*'\n  verbs:\n  - get\n  - list\n  - watch\n \n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: view-user\n  namespace: kubernetes-dashboard\n \n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: view-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-view\nsubjects:\n- kind: ServiceAccount\n  name: view-user\n  namespace: kubernetes-dashboard\n#----------编辑内容 end--------------- \n\n# 运行权限配置\nkubectl apply -f admin-user-dashboard.yaml\n\n# 生成登录token\n# admin-user 为角色名称\nkubectl describe secret admin-user -n kubernetes-dashboard\n# 或者(还是推荐上边的语句，比较简单)\n# kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\"\n\n#展示内容如下，我们复制token部分信息即可\n\n#----------------------------------------------\nName:         admin-user-token-2gt2w\nNamespace:    kubernetes-dashboard\nLabels:       <none>\nAnnotations:  kubernetes.io/service-account.name: admin-user\n              kubernetes.io/service-account.uid: ce38f197-f395-45ed-9385-104707df07c7\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6ImgzM0taa283elhJbjFvc2NFNTJULXVDTGNURjZJaV9zSzV0X3U1VkgwUFEifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLTJndDJ3Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJjZTM4ZjE5Ny1mMzk1LTQ1ZWQtOTM4NS0xMDQ3MDdkZjA3YzciLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.RoYrf2VCgGn8RiHVgBvMS7El4DWa6XmAT_Prrjs_Kk2nOFOjG2z3i4I_1Db9S6Jq0ZC-L2lCeGDQSdMmOBW1eYMNw6-vSwteKR_Un7GhshPLK4AJML3CK8uHsgYnhM64EinyTcdbBj9ade6OdJ3ypFi_Dw_oms4CUnuD57zLynZnh_JGMj-HJEMmtjBDV_FE-yJUn7_Y626e5Uw92p_xcW9up68TPEMuOSTedlxHJ61jpGf0H8ZGdinslvgpEbp7jUJeXoU_caLHhKGc28pQzJgjtHkatHJS7HmYdcPmSSON-2HZztmNlNfHI0luEfEg2KCAU3hxQeDKMw89jye1eg\nca.crt:     1099 bytes\n\n#----------------------------------------------\n\n\n```\n\n下面我们复制登录token，登录dashboard，显示如下页面\n\n## 五、子节点加入\n\n子节点需要执行 `一`、`二`两点操作，注意`二`点我们除了`kubeadm init`操作外其他的都执行\n\n子节点加入的命令在 `step3-》k8s安装-》kubeadm init` 输出的日志中我们可以找到。\n\n```\nkubeadm join 192.168.56.105:6443 --token 4yipfl.er9r8aqnq0hpd8a4 \\\n        --discovery-token-ca-cert-hash sha256:afa76da3ced528e667374693fb4b0edd160530c251471ae11ece13c65d3d162a\n\n```\n\n下面我们简单说下 子节点服务器的操作流程:\n\n```\n# 基础依赖包安装\nyum -y install wget vim net-tools ntpdate bash-completion\n# 修改当前机器名 \nhostnamectl set-hostname k8s-slave02\n# 或 hostnamectl set-hostname k8s-slave03\n\n# 修改hosts文件\nvim /etc/hosts\n\n192.168.56.105 k8s-master01\n192.168.56.106 k8s-slave02\n192.168.56.107 k8s-slave03\n\n# 系统时钟同步与时区配置\nntpdate time1.aliyun.com\nrm -rf /etc/localtime\nln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime\ndate -R || date\n\n# 关闭防火墙、selinux\nsystemctl stop firewalld\nsystemctl disable firewalld\nsed -i 's/enforcing/disabled/' /etc/selinux/config\nsetenforce 0\n\n# 关闭swap\nswapoff -a\nsed -i '/swap/s/^/#/' /etc/fstab\nfree -m\n\n# 网桥过滤\nvim /etc/sysctl.conf\n\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-arptables = 1\nnet.ipv4.ip_forward=1\nnet.ipv4.ip_forward_use_pmtu = 0\n\n# 生效命令 与 查看\nsysctl --system\nsysctl -a|grep \"ip_forward\"\n\n# docker安装\nyum install -y yum-utils\nyum-config-manager \\\n--add-repo \\\nhttps://download.docker.com/linux/centos/docker-ce.repo\nyum -y install docker-ce docker-ce-cli containerd.io\nsystemctl enable docker & &  systemctl start docker\n\n# docker 镜像加速 与 cgroup配置\nmkdir -p /etc/docker\n\ntee /etc/docker/daemon.json <<-'EOF'\n{\n\"exec-opts\": [\"native.cgroupdriver=systemd\"],\n\"registry-mirrors\": [\"https://fl791z1h.mirror.aliyuncs.com\"]\n}\nEOF\n\nsystemctl daemon-reload & &  systemctl restart docker\n\n# k8s安装\ncat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/\nenabled=1\ngpgcheck=1\nrepo_gpgcheck=1\ngpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg\nEOF\n\nyum -y install --nogpgcheck kubelet-1.23.8 kubeadm-1.23.8 kubectl-1.23.8\n\n# 启动k8s服务\nsystemctl enable kubelet & &  systemctl start kubelet\n\n# join k8s网络\nkubeadm join 192.168.56.105:6443 --token 4yipfl.er9r8aqnq0hpd8a4 \\\n        --discovery-token-ca-cert-hash sha256:afa76da3ced528e667374693fb4b0edd160530c251471ae11ece13c65d3d162a\n# 注意： token 有效时间是24小时，如果过来24小时那么需要刷新token\n# 在master 主节点上运行命令，刷新 token\nkubeadm token create --print-join-command\n# 得到以下结果\nkubeadm join 192.168.56.105:6443 --token o26r8i.zg2t9ade0tyuh4tp --discovery-token-ca-cert-hash sha256:de2d35d81f3740e93aca8a461713ca4ab1fcb9e7e881dc0f8836dd06d8a40229\n# 在slave 节点上运行 上述 `kubeadm join` 语句\n# 运行成功，显示如下:\n\n#------------运行成功 start-----------------\n[preflight] Running pre-flight checks\n[preflight] Reading configuration from the cluster...\n[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'\n[kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet-start] Starting the kubelet\n[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...\n\nThis node has joined the cluster:\n* Certificate signing request was sent to apiserver and a response was received.\n* The Kubelet was informed of the new secure connection details.\n\nRun 'kubectl get nodes' on the control-plane to see this node join the cluster.\n#------------运行成功 end-------------------\n\n# 在主节点上 查看所有nodes\nkubectl get nodes\n\n\nNAME           STATUS     ROLES                  AGE   VERSION\nk8s-master01   Ready      control-plane,master   18h   v1.23.8\nk8s-slave02    NotReady   <none>                 46s   v1.23.8\n\n# 这个时候我们看到 `k8s-slave02` 已经加入到 k8s集群中了，但是`STATUS`还是`NotReady`\n# 此步骤比较耗时，我们多等一会，直到 `NotReady` 变为 `Ready`\n\nNAME           STATUS   ROLES                  AGE     VERSION\nk8s-master01   Ready    control-plane,master   18h     v1.23.8\nk8s-slave02    Ready    <none>                 3m16s   v1.23.8\n\n# 至此，子节点加入成功\n# 为了稳妥起见，我们在 master 上发现子节点 状态由 `NotReady` 变成 `Ready` 后，我们对子节点k8s 进行重启\nsystemctl restart kubelet\n\n```\n\n子节点上我们尝试运行 k8s命令\n\n```\n# 查看节点信息\nkubectl get nodes\n# 展示信息\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n\n```\n\n我们发现：**子节点无法运行kubectl命令**\n\n原因：`kubectl`命令需要使用 `kubernetes-admin` 来运行\n\n解决办法: 我们将master节点的 admin.conf 复制到子节点上\n\n```\n# master节点上操作：从master节点 复制到 子节点\nscp /etc/kubernetes/admin.conf root@192.168.56.106:/etc/kubernetes/\n# scp /etc/kubernetes/admin.conf root@192.168.56.107:/etc/kubernetes/\n\n# node节点上操作：配置环境变量\necho \"export KUBECONFIG=/etc/kubernetes/admin.conf\" >> ~/.bash_profile\nsource ~/.bash_profile\n\n# 而后我们在子节点上运行 命令\nkubectl get nodes\n\nNAME           STATUS   ROLES                  AGE   VERSION\nk8s-master01   Ready    control-plane,master   19h   v1.23.8\nk8s-slave02    Ready    <none>                 49m   v1.23.8\nk8s-slave03    Ready    <none>                 14m   v1.23.8\n\n\n\n```\n\n## 六、常用信息\n\n### 1. 常用命令\n\n```\n# 查看k8s 运行日志命令, 这个比较有用，在k8s 启动、kubeadm init、kubeadm join 阶段可以辅助分析问题。\njournalctl -xefu kubelet \n# 查看k8s驱动\nsystemctl show --property=Environment kubelet |cat\n# 重启k8s\nsystemctl restart kubelet\n# 启动k8s\nsystemctl start kubelet\n# 停止k8s\nsystemctl stop kubelet\n# 开机自启k8s\nsystemctl enable kubelet\n\n# dashboard 获取token\nkubectl describe secret admin-user -n kubernetes-dashboard\n\n# kubeadm 重置， 有些时候我们在使用kubeadm init 命令时会报错，我们根据错误提示修复问题后需要重新进行 init 操作，因此需要进行reset重置\nkubeadm reset\n\n```\n\n### 2. 环境信息\n\n```\n# k8s 安装目录\n/etc/kubernetes/\n\n总用量 32\n-rw-------. 1 root root 5642 6月  28 15:19 admin.conf\n-rw-------. 1 root root 5674 6月  28 15:19 controller-manager.conf\n-rw-------. 1 root root 1986 6月  28 15:19 kubelet.conf\ndrwxr-xr-x. 2 root root  113 6月  28 15:19 manifests\ndrwxr-xr-x. 3 root root 4096 6月  28 15:19 pki\n-rw-------. 1 root root 5618 6月  28 15:19 scheduler.conf\n\n# 组件配置文件目录\n/etc/kubernetes/manifests/\n\n总用量 16\n-rw-------. 1 root root 2310 6月  28 15:19 etcd.yaml\n-rw-------. 1 root root 3378 6月  28 15:19 kube-apiserver.yaml\n-rw-------. 1 root root 2879 6月  28 15:19 kube-controller-manager.yaml\n-rw-------. 1 root root 1464 6月  28 15:19 kube-scheduler.yaml\n\n# 自定义dashboard yaml文件目录\n/opt/kube-dashboard/conf/\n\n总用量 8\n-rw-r--r--. 1 root root 1124 6月  29 08:41 admin-user-dashboard.yaml\n-rw-r--r--. 1 root root  285 6月  29 08:25 k8s-dashboard.yaml\n\n\n```\n\n## 附录\n\n### 参考链接\n",
    "labels": [],
    "created_at": "2024-12-21T04:21:01Z"
  },
  {
    "id": 17,
    "title": "13.pytho上下文管理器详解",
    "url": "https://www.jianshu.com/p/033fdb93197b",
    "content": "\n> \n使用上下文管理器，可以让代码更加优雅简洁。当然，上下文的管理器的作用不止于此，它内部的实现机制，能很好的处理代码异常，提升代码的复用性\n\n\n#### 1、先看看最简单的例子，with语句\n\n```\n# 创建一个文件写入字符串“Python”\nf = open('123.txt', 'w')\nf.write(\"python\")\nf.close()\n\n\n# 使用with语句调用上下文实现文件写入操作\nwith open('123.txt', 'w') as f:\n    f.write('python')\n\n```\n\n**with语句的作用：**\n\n可以自动管理上下文资源，不论什么原因跳出with块，都能确保文件正确的关闭，以此来达到释放资源的目的。\n\n#### 2、什么是上下文管理器？\n\n上下文是 context 直译的叫法，在程序中用来表示代码执行过程中所处的前后环境，比如在文件操作时，文件需要打开关闭，而文件读写操作就处于文件操作的上下文环境中；\n\n上下文管理器，上下文管理器是指在一段代码执行之前，执行一些预处理的工作，代码执行之后再执行一些清理工作。\n\n上下文管理器中有**enter**()和 **exit**() 两个方法，**enter**()方法在执行 with 后面的语句时执行，一般用来处理操作前的内容,比如一些创建对象，初始化等； **exit**() 方法在 with 内的代码执行完毕后执行，一般用来处理一些善后收尾工作，比如文件的关闭，数据库的关闭等。\n\n#### 3、上下文管理器的原理过程如下：\n\n调用**enter**()方法，进行预处理操作\n\n执行用户操作\n\n调用 **exit**() 方法，完成清理操作\n\n#### 4、上下文管理器的应用场景：\n\n资源管理功能，即文件处理、网络连接、数据库连接等操作时需要关闭资源。\n\n也可以在代码执行前后增加功能，类似于装饰器，比如代码之前做权限验证等。\n\n##### 4.1 数据库连接\n\n```\nimport pymysql\n\nclass DBConnection(object):\n    def __init__(self,ip,user,passwd,db):\n        self.ip = ip\n        self.user = user\n        self.passwd = passwd\n        self.db = db\n\n    def __enter__(self):\n        self.conn = pymysql.connect(self.ip, user=self.user, passwd=self.passwd, db=self.db)\n        self.cur = conn.cursor()\n        return self.cur\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.cur.close()\n        self.conn.close()\n\nwith DBConnection('192.168.121.xxx', user=\"xxx\", passwd=\"123456\", db=\"xxx\") as cur:\n    cur.execute(\"select * from studnet;\")\n    result = cur.fetchall()\n    print(result)\n\n\n```\n\n**完成DBConnection这个类，每次连接数据库时，只要简单的调用with语句即可，不需要关心数据库的关闭、异常等**\n\n##### 4.2、上下文管理器的异常处理\n\n```\nclass MyOpen(object):\n    \"\"\"自定义上下文管理类\"\"\"\n\n    def __init__(self, file, mode):\n        self._file = file\n        self._mode = mode\n\n    def __enter__(self):\n        self._handle = open(self._file, self._mode)\n        return self._handle\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        # print('Type: ', exc_type)\n        # print('Value:', exc_val)\n        # print('TreacBack:', exc_tb)\n        self._handle.close()\n        print(\"异常已被处理\")\n        return True\n\n# 读的模式打开文件,进行写操作，不支持\nwith MyOpen('123.txt', 'r') as f:\n    f.write('python')\n\n#输出：\n异常已被处理\n\n```\n\n**with 语法不仅可以简化资源操作的后续清除操作，还可以代替 try/finally 进行异常处理**\n\n当with中执行的语句发生异常时，异常信息会被发送到 **exit**()方法的参数中， **exit**() 方法有如下三个参数:\n\nexc_type : 异常类型\n\nexc_val : 异常值\n\nexc_tb : 异常回溯追踪\n\n这三个参数都与异常有关，with语句会把异常的exc_type ,exc_val 和exc_tb传递给 **exit**() 方法，它让**exit**() 方法来处理异常 ，如果**exit**()返回的是True，那么这个异常就被忽略，并按照我们定义的方式进行抛出。如果**exit**()返回的是True以外的任何东西，那么这个异常将被with语句抛出。\n\n##### 4.3、通过contextlib实现\n\npython内置了contextlib这个模块用于实现上下文管理器，它是通过生成器yield实现的，这个模块让我们不必创建类和**enter**和**exit**了。\n\n```\nfrom contextlib import contextmanager\n\n@contextmanager\ndef diy_open(filename, **kwargs):\n    f = open(filename, **kwargs)\n    try:\n        yield f\n    finally:\n       f.close()\n\nwith diy_open('test.txt', encoding='utf-8') as f:\n    print(f.readlines())\n\n```\n",
    "labels": [],
    "created_at": "2024-12-21T04:21:00Z"
  },
  {
    "id": 4,
    "title": "拦截烂SQL，解读GaussDB(DWS)查询过滤器过滤规则原理",
    "url": "https://www.cnblogs.com/huaweiyun/p/18619491",
    "content": "\n本文分享自华为云社区[《GaussDB(DWS)查询过滤器过滤规则原理与使用介绍》](https://bbs.huaweicloud.com/blogs/441767?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)，作者： 清道夫。\n\n# 1. 前言\n\n**适用版本：【9.1.0.100（及以上）】**\n\n查询过滤器在9.1.0.100之前就具备提供查询过滤功能的能力，但仅支持自动隔离反复查询被终止的查询，防止烂SQL再次执行。\n\n老版本主要面向异常熔断机制和紧急拦截场景，前者可以与异常规则联动，自动将触发异常规则的语句添加到黑名单中，后者是需要手动找到core或者引发hang的语句进行屏蔽。\n\n\n\n大家有兴趣可以翻一下之前的这篇文章[GaussDB(DWS)查询过滤器原理与应用](https://bbs.huaweicloud.com/blogs/401188)。\n\n9.1.0.100及9.1.0.200版本对查询过滤器做了功能的改进，可以通过多维度进行烂SQL识别，功能更丰富，配置更灵活。\n\n# 2. 原理介绍\n\n在原理介绍之前，先举个简单的例子。在业务开发过程中，要想禁止对2张以上的表进行关联查询，此时可以使用DDL语句创建过滤规则：\n\n```\nCREATE BLOCK RULE forbid_2_t_sel FOR SELECT FILTER BY  SQL('test_block_rule') with(table_num='2');\n```\n\ntable_num指的是一个语句中出现的表的个数，此时所有查询语句不能包含有两张表以上的查询。\n\n```\n--两张表直接关联查询，可以正常执行\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\n c1 | c2 | c1 | c2\n----+----+----+----\n(0 rows)\n\n--三张表直接关联查询，被拦截\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2 join test_block_rule3 t3 on t2.c1=t3.c1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 2(3))\n```\n\n说到这，整体逻辑就非常清楚了。用户可以提前识别烂SQL的特征，然后抽象出来，用DDL语句创建规则，后续会对查询的语句进行过滤，被规则筛选出来的便是烂SQL，执行前会报错，反之则可以正常执行。\n\n查询过滤器框架及功能原理概况：\n\n<img src=\"https://img2024.cnblogs.com/blog/2030258/202412/2030258-20241220160540342-411141587.png\" alt=\"\" referrerpolicy=\"no-referrer\">\n\n\n\n从图中可以看出，之前的查询过滤器的功能依然存在，可以保证与异常规则的联动，新版本的增强更注重规则的灵活性和功能的丰富性。\n\n# 3. 使用介绍\n\n## 3.1 查询过滤规则元数据管理\n\n查询过滤规则，可以通过DDL进行新增、删除或者修改，其语法如下：\n\n### （1）创建\n\n```\nCREATE BLOCK RULE [ IF NOT EXISTS ] block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE ] |\n    FILTER BY\n    { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) }\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n其中，\n\n   \n**block_name：**过滤规则的名称\n   \n   \n**user_name：**规则应用的对象用户\n   \n   \n**host：**是规则应用的客户端IP\n   \n   \n**FOR：**语句类型，支持对UPDATE/SELECT/INSERT/DELETE/MEGE INTO五种类型语句进行限制\n   \n   \n**FILTER BY：**过滤方法，包含两种形式\n   \n   \n**SQL：**根据关键词对语句进行正则匹配，例如表名，其长度不能超过1024，建议尽量精简\n   \n   \n**TEMPLATE：**\n   \n   \n**unique_sql_id：**归一化的64位哈希值，重复概率较sql_hash大一些\n   \n   \n**sql_hash：**归一化的哈希值（md5），一般不会重复，相较unique_sql_id更推荐使用\n   \n   \n**with_parameter：**查询过滤规则选项参数，可以附加多个条件，满足其一便会匹配过滤。\n   \n   \n**application_name：**客户端名称\n   \n   \n**query_band：**负载标识\n   \n   \n**table_num：**包含的基表个数\n   \n   \n**partition_num：**扫描分区的数量\n   \n   \n**estimate_row：**基表输出行数\n   \n   \n**resource_pool：**切换的目标资源池，仅适用于9.1.0.200及以上\n   \n   \n**max_active_num：**可并发执行的语句个数，仅适用于9.1.0.200及以上\n   \n   \n**is_warning：**改变拦截行为为告警，而非默认的报错，仅适用于9.1.0.200及以上\n   \n\n其中，user_name和FILTER BY是必选项，其他可以通过业务实际需要进行配置。\n\n### （2）修改\n\n```\nALTER BLOCK RULE block_name RENAME to new_block_name;\n```\n\n通过rename对查询过滤的规则进行重命名。\n\n```\nALTER BLOCK RULE block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] | [ TO DEFAULT ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE | DEFAULT ] |\n    [ [ FILTER BY ]\n    [ { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) } ] ]\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n所有选项均支持二次修改，如果需要去除部分字段的限制，可以指定default关键词，例如：\n\n```\n--修改为只能查询1张表\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num='1');\nALTER BLOCK RULE\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 1(2))\npostgres=# select * from test_block_rule1 t1;\n c1 | c2\n----+----\n(0 rows)\n\n--去除查询中表个数的限制\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num=default);\nALTER BLOCK RULE\n--再次查询报错拦截\npostgres=# select * from test_block_rule1 t1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule)\n```\n\n### （3）删除\n\n```\nDROP BLOCK RULE [ IF EXISTS ] block_name;\n```\n\n## 3.2 权限问题\n\n对于普通用户来讲是没有创建查询过滤规则权限的，需要管理员或者管理员将权限赋给某一普通用户才可以。\n\n```\n--切换至普通用户\npostgres=# set role jack password 'xxx';\nSET\n--创建查询过滤规则报错提示无权限\npostgres=> create block rule bl2 filter by sql('test');\nERROR:  CREATE/ALTER/DROP BLOCK RULE permission denied for current user\n--重置user\npostgres=> reset role;\nRESET\n--对普通用户进行授权\npostgres=# grant gs_role_block to jack;\nGRANT ROLE\n--切换普通用户\npostgres=# set role jack password 'xxx';\nSET\n--再次创建成功\npostgres=> create block rule bl2 filter by sql('test');\nCREATE BLOCK RULE\n```\n\n建议创建查询过滤规则时尽量缩小适用范围，避免误过滤，或者范围过大导致性能劣化。\n\n## 3.3 备份恢复\n\n对于查询过滤规则的备份或者恢复的权限与操作元数据的权限一致，需要管理员或者管理员讲权限赋值给某一普通用户才可以，用户可以通过gs_dump导出查询过滤规则定义。\n\n如果想查看或者导入查询过滤规则的定义，可以通过pg_get_blockruledef进行查询。\n\n```\npostgres=# select * from pg_get_blockruledef('test');\n                         pg_get_blockruledef\n----------------------------------------------------------------------\n CREATE BLOCK RULE test FILTER BY SQL('test') WITH(estimate_row='3');\n(1 row)\n```\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n## 3.4 使用举例\n\n### （1）使用关键词进行查询过滤\n\n```\nCREATE BLOCK RULE bl1\nTo block_user\nFOR SELECT\nFILTER BY SQL ('tt')\nWITH(partition_num='2',\n     table_num='1',\n     estimate_row='5'\n     );\n\npostgres=> select * from tt;\nERROR:  hit block rule bl1(user_name: block_user, block_type: SELECT, regexp_sql: tt, partition_num: 2(3), table_num: 1(1), estimate_row: 5(1))\n```\n\n从上面的查询可以看出，查询语句包含了tt关键字，并且扫描的分区个数超过了2，此时执行语句被过滤拦截。需要注意的是，**扫描分区的个数并不总是准确的**，仅能识别**静态**的分区剪枝个数，执行过程中的**动态剪枝**并不能被识别。\n\n\n\n**小技巧：**使用关键词过滤时可以先使用正则匹配符~*进行测试，正则匹配是忽略大小写的。\n\n另外，由于查询过滤器的规则直接作用在用户block_user上，因此在删除用户block_user时，会提示有依赖项，此时可以通过在语句最后加上cascade进行删除，此时作用在此用户上的查询过滤规则也会被一同删除。\n\n\n\n受限于篇幅，其他选项就不再一一列举。需要注意的是，过滤规则命中的依据是，with_parameter命中任意一项，且其他字段的特征也符合即会判定为符合查询过滤规则。\n\n\n\n特别注意，不同的计划，可能部分字段无法按照预期进行拦截，例如：\n\n```\npostgres=# create block rule test filter by sql('test')with(estimate_row='3');\nCREATE BLOCK RULE\npostgres=# select * from test;\n c1 | c2\n----+----\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n(5 rows)\n```\n\n此时，语句关键字是可以匹配上的，查询的行数也超过了3行的限制，那为什么无法拦截呢？\n\n```\npostgres=# explain verbose select * from test;\n                                          QUERY PLAN\n-----------------------------------------------------------------------------------------------\n  id |                  operation                   | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------------+--------+------------+---------+---------\n   1 | ->  Data Node Scan on \"__REMOTE_FQS_QUERY__\" |      0 |            |       0 | 0.00\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Data Node Scan on \"__REMOTE_FQS_QUERY__\"\n         Output: test.c1, test.c2\n         Node/s: All datanodes (node_group, bucket:16384)\n         Remote query: SELECT c1, c2 FROM public.test\n```\n\n通过计划可以看出，此时是FQS计划，导致没有估算信息。因此此时无法进行拦截，对于CN轻量化的计划也是一样的，如果我们让语句强制走stream计划，那么就可以拦截成功：\n\n```\npostgres=# set enable_stream_operator=on;\nSET\npostgres=# set enable_fast_query_shipping=off;\nSET\npostgres=# select * from test;\nERROR:  hit block rule test(regexp_sql: test, estimate_row: 3(5))\npostgres=#  explain verbose select * from test;\n                                       QUERY PLAN\n-----------------------------------------------------------------------------------------\n  id |               operation                | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------+--------+------------+---------+---------\n   1 | ->  Row Adapter                        |      5 |            |       8 | 69.00\n   2 |    ->  Vector Streaming (type: GATHER) |      5 |            |       8 | 69.00\n   3 |       ->  CStore Scan on public.test   |      5 |            |       8 | 59.01\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Row Adapter\n         Output: c1, c2\n   2 --Vector Streaming (type: GATHER)\n         Output: c1, c2\n         Node/s: All datanodes (node_group, bucket:16384)\n   3 --CStore Scan on public.test\n         Output: c1, c2\n         Distribute Key: c1\n```\n\n所以，如果估算信息不准确，也会导致误拦截或者漏拦截的情况，因为计划的信息是通过估算得到的，因此这种情况无法避免。\n\n### （2）使用语句归一化特征值进行查询过滤\n\n语句归一化的特征值，目前有两个，分别是unique_sql_id和sql_hash，两者均是对查询树进行哈希计算之后得出的，区别在于前者是64位哈希值，后者是md5值，因此前者的重复概率会大于后者，在使用时尽量使用sql_hash进行过滤。\n\n\n\n很多小伙伴会问，这两个值如何获取呢？两种方法：\n\n   \n查看explain结果\n   \n\n```\npostgres=> explain verbose select * from tt where a>1;\n                                              QUERY PLAN\n ----------------------------------------------------------------------------------------------------\n   id |                     operation                     | E-rows | E-distinct | E-width | E-costs\n  ----+---------------------------------------------------+--------+------------+---------+---------\n    1 | ->  Row Adapter                                   |      1 |            |       8 | 16.00\n    2 |    ->  Vector Streaming (type: GATHER)            |      1 |            |       8 | 16.00\n    3 |       ->  Vector Partition Iterator               |      1 |            |       8 | 6.00\n    4 |          ->  Partitioned CStore Scan on public.tt |      1 |            |       8 | 6.00\n\n    Predicate Information (identified by plan id)\n  -------------------------------------------------\n    3 --Vector Partition Iterator\n          Iterations: 3\n    4 --Partitioned CStore Scan on public.tt\n          Filter: (tt.a > 1)\n          Pushdown Predicate Filter: (tt.a > 1)\n          Partitions Selected by Static Prune: 1..3\n\n  Targetlist Information (identified by plan id)\n  ----------------------------------------------\n    1 --Row Adapter\n          Output: a, b\n    2 --Vector Streaming (type: GATHER)\n          Output: a, b\n          Node/s: datanode1\n    3 --Vector Partition Iterator\n          Output: a, b\n    4 --Partitioned CStore Scan on public.tt\n          Output: a, b\n\n               ====== Query Summary =====\n  -----------------------------------------------------\n  Parser runtime: 0.029 ms\n  Planner runtime: 0.286 ms\n  Unique SQL Id: 2229243778\n  Unique SQL Hash: sql_aae71adfaa3d91bfe75499d92ad969e8\n (34 rows)\n```\n\n   \n查看topsql记录\n   \n\n```\n queryid                     | 95701492082350773\n query                       | select * from tt where a>10;\n query_plan                  | 1 | Row Adapter  (cost=14.00..14.00 rows=1 width=8)\n                             | 2 |  ->Vector Streaming (type: GATHER)  (cost=0.06..14.00 rows=1 width=8)\n                             | 3 |   ->Vector Partition Iterator  (cost=0.00..4.00 rows=1 width=8)\n                             |   |     Iterations: 2\n                             | 4 |    ->Partitioned CStore Scan on public.tt  (cost=0.00..4.00 rows=1 width=8)\n                             |   |      Filter: (tt.a > 10)\n                             |   |      Pushdown Predicate Filter: (tt.a > 10)\n                             |   |      Partitions Selected by Static Prune: 2..3\n node_group                  | installation\n pid                         | 139803379566936\n lane                        | fast\n unique_sql_id               | 2229243778\n session_id                  | 1732413324.139803379566936.coordinator1\n min_read_bytes              | 0\n max_read_bytes              | 0\n average_read_bytes          | 0\n min_write_bytes             | 0\n max_write_bytes             | 0\n average_write_bytes         | 0\n recv_pkg                    | 2\n send_pkg                    | 2\n recv_bytes                  | 3297\n send_bytes                  | 57\n stmt_type                   | SELECT\n except_info                 |\n unique_plan_id              | 0\n sql_hash                    | sql_aae71adfaa3d91bfe75499d92ad969e8\n```\n\n可以看出两种方法都可以轻松获取这两个语句归一化的特征值，explain可以在事前提前获取，topsql可以在语句执行后进行获取。\n\n\n\n这个时候，可能很多小伙伴又会有疑问，语句中的条件有变化，是否会影响归一化的特征值呢？\n\n答案是不会，因为归一化过程中会去除常量的影响，上述的举例中两个语句条件中的常量值并不相同，但归一化的特征值确实一样的。\n\n### （3）查询过滤的性能\n\n由于语句的过滤，特别是关键词的正则匹配通常是比较耗时的，此时如果有过多的过滤规则，可能导致执行时间的劣化，特别是对于短查询可能影响更为明显。\n\n\n\n**本地实测：**正则匹配关键词长度1024，建立查询过滤规则1000条左右时，对于查询的影响在27.72ms左右，且如果考虑其他匹配项，可能影响会更大，所以，不建议添加太多的查询过滤规则。且业务稳定后可以只对特定开发或者新业务的用户创建查询过滤规则，此时查询过滤规则会优先通过绑定的用户跳过无效的过滤，减少对性能的性能的影响。\n\n### （4）过滤时间查看\n\n可以配置GUC参数analysis_options查看查询过滤规则对正常语句所消耗的时间。\n\n```\nset analysis_options='on(BLOCK_RULE)';\n\n-- explain performance + query\n\n                    User Define Profiling\n-----------------------------------------------------------------\nSegment Id: 3  Track name: Datanode build connection\n      datanode1 (time=0.288 total_calls=1 loops=1)\n      datanode2 (time=0.301 total_calls=1 loops=1)\n      datanode3 (time=0.321 total_calls=1 loops=1)\n      datanode4 (time=0.268 total_calls=1 loops=1)\nSegment Id: 3  Track name: Datanode wait connection\n      datanode1 (time=0.016 total_calls=1 loops=1)\n      datanode2 (time=0.038 total_calls=1 loops=1)\n      datanode3 (time=0.021 total_calls=1 loops=1)\n      datanode4 (time=0.017 total_calls=1 loops=1)\nSegment Id: 1  Track name: block rule check time\n      coordinator1 (time=0.028 total_calls=1 loops=1)\n```\n\n### （5）拦截记录\n\n**[仅适用于****9.1.0.200****及以上]**\n\n创建查询过滤规则后会拦截很多烂SQL，如何看拦截的语句有哪些呢？可以通过topsql进行查看，abort_info会记录拦截信息，也就是查询的报错信息。\n\n```\npostgres=# select abort_info,query from GS_WLM_SESSION_INFO where abort_info like '%hit block rule test%';\n                        abort_info                         |        query\n-----------------------------------------------------------+---------------------\n hit block rule test(regexp_sql: test, estimate_row: 3(5)) | select * from test;\n(1 rows)\n```\n\n# 4. 总结\n\n查询过滤器在9.1.0.100和9.1.0.200版本丰富了大量的功能，提高了烂SQL拦截的灵活性。\n\n管控面后续版本同样可以直接通过前端页面对查询过滤规则进行管理，大家敬请期待。\n\n有任何问题欢迎留言讨论，我们将不断丰富和完善查询过滤功能，让烂SQL无门可入。\n\n\n\n华为开发者空间，汇聚鸿蒙、昇腾、鲲鹏、GaussDB、欧拉等各项根技术的开发资源及工具，致力于为每位开发者提供一台云主机、一套开发工具及云上存储空间，让开发者基于华为根生态创新。[点击链接](https://developer.huaweicloud.com/space/devportal/desktop?utm_source=kfzwzdspace& utm_adplace=nrcbds)，免费领取您的专属云主机\n\n\n\n[**点击关注，第一时间了解华为云新鲜技术~**](https://bbs.huaweicloud.com/blogs?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)\n",
    "labels": [],
    "created_at": "2024-12-21T04:09:26Z"
  },
  {
    "id": 11,
    "title": "\n\n检索增强生成（RAG）：解密AI如何融合记忆与搜索\n",
    "url": "https://mp.weixin.qq.com/s/JNSy2HbmvqiT3FCVxKpgSg",
    "content": "\n人工智能（AI）在生成类似人类的文本和准确回答问题方面取得了飞跃，但存在局限性。传统模型完全依赖于它们的训练数据——在训练期间吸收的固定知识。它们无法查找新鲜或专业信息，这限制了它们在动态情况下的实用性。\n\n\n检索增强生成（RAG）通过使 AI 能够实时“咨询外部来源”来解决这个问题，就像学生在开卷考试中参考教科书一样。这种检索和生成的混合方法使 RAG 能够产生更准确、上下文相关和最新的回答。让我们深入了解 RAG 是如何工作的以及为什么它是 AI 演变中的变革性步骤。\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"305488235\" data-ratio=\"0.4648148148148148\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_png/wCXWZ24UhxRhvL6Aza01zSdxI9ia5glgER5yiawhl8u7TH2X5oIadxP5f4tcNb7zzEURP9ib3Rib0ccd44Iw1jYF0A/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n**什么是检索增强生成（RAG）？**\n\n检索增强生成（Retrieval-Augmented Generation, RAG）是一种结合“检索”和“生成”能力的人工智能技术，类似于学生在开卷考试中的答题策略。想象一个学生亚历克斯正在准备一场开卷考试。在考试中，亚历克斯不仅依赖记忆，还会在遇到难题时查阅课本中的相关章节（检索信息），然后结合自己的知识写出完整、有依据的答案（生成回答）。这正是RAG的核心理念。\n\nRAG 将两种主要的AI能力——检索和生成——巧妙融合。\n\n**检索**：当面临问题时，RAG并不局限于其训练数据（类似于“记忆”），而是主动从外部知识源中检索相关信息。这些外部来源可以是数据库、文档或其他结构化知识库，就像亚历克斯在答题时查阅课本一样，确保获取准确且最新的知识。\n\n**生成**：在获取到相关信息后，RAG结合这些检索结果与自身已有的知识，生成一个连贯且准确的回答。这个过程类似于亚历克斯将课本中的知识与自己的理解融合，以写出逻辑清晰的答案。\n\nRAG的这种能力，使其特别适用于处理需要最新信息或复杂背景的动态查询。通过同时依赖外部知识和内在推理，RAG不仅可以回答精确的问题，还能应对更广泛、深层次的内容需求，为用户提供更智能、更可靠的解决方案。\n\n**RAG 的工作原理：分解核心组件**\n\n检索增强生成（RAG）依赖几个核心组件来高效完成信息的检索和生成。这些组件协同工作，确保 RAG 能够准确理解问题、检索相关信息，并生成连贯的回答。\n\nRAG 的第一个核心是 **向量嵌入技术**，它用于高效检索信息。向量嵌入是一种将文本转化为数值表示（向量）的方法，根据文本的语义相似性对信息进行组织和匹配。这种方式使得 RAG 能快速找到与问题相关的信息。类似于教科书按章节分类，向量嵌入帮助系统迅速定位关键内容，而无需逐页搜索。当提出一个问题时，RAG 会通过向量嵌入匹配问题与数据库中的相关段落，从而实现高效的信息检索，即使面对庞大的数据量也能快速响应。\n\n第二个核心是 **检索模型**，其主要任务是从外部来源中定位相关信息。检索模型通过分析问题的语义，与外部知识库、文档集合甚至网络实时检索中找到最相关的内容，同时过滤掉无关的信息。这一步对 RAG 的整体能力至关重要，尤其是在回答关于最新事件或特定领域问题时。例如，用户询问最近的科学发现时，检索模型可以访问科学数据库并提取最新的研究结果，从而让 RAG 提供及时准确的回答。\n\n在完成检索后， **生成模型** 接管任务。生成模型将检索到的信息与系统内部固有的知识相结合，生成自然流畅且与上下文相关的回答。这一过程不仅确保回答的内容准确，还让语言表达更加连贯。类似于学生将笔记和课本内容整合成全面的答案，而非直接复制原文，生成模型能够根据检索结果进行推理和扩展，从而为用户提供更有深度的回应。\n\n通过将向量嵌入、检索模型和生成模型紧密结合，RAG 实现了从问题理解到知识获取再到语言生成的完整流程。这种多层次的协作使其能够应对复杂的动态查询，尤其在需要实时更新或专业知识支持的场景中表现尤为出色。\n\n**为什么 RAG 很重要：检索增强生成的优势**\n\n检索增强生成（RAG）为人工智能的发展带来了重要突破，尤其是在需要提供最新、上下文敏感答案的应用中表现出色。以下是 RAG 的关键优势：\n\nRAG 能够获取最新信息，这是传统模型无法实现的能力。传统模型的知识来源于训练数据，无法应对实时信息的需求，例如当前事件或最新研究成果。这种局限性在信息快速更新的场景中尤为明显。而 RAG 的检索组件能够从外部来源实时获取所需内容，使其在处理经常变化的话题时表现尤为出色。例如，当需要解答最新科技突破或新闻动态时，RAG 能够快速调取相关资料，提供及时且准确的回答。\n\n通过检索相关主题的精确信息，RAG 提高了回答的准确性和相关性。相比完全依赖训练数据的传统模型，RAG 可以根据查询需求动态获取专业内容，特别是在处理复杂或技术性问题时。例如，在解决专业性较强的医学问题时，RAG 可以直接引用相关文献或研究，确保答案更具权威性和针对性。这种结合内在知识与外部数据的能力，使其回答更加符合实际需求。\n\nRAG 的高效内存使用是其另一大优势。传统模型需要记住海量数据，其中很多内容可能在实际应用中很少被使用，而这种“死记硬背”的方式对内存和计算资源的消耗较大。RAG 则避免了这一点。它只在需要时检索相关信息，将计算资源集中在通用知识的管理上，从而优化了内存使用效率。这种按需获取的机制不仅减少了冗余信息的存储，还提升了模型的整体性能。\n\nRAG 还展现了高度的领域适应性。它可以根据特定行业需求，从特定数据库或知识库中检索信息，满足不同领域的专业需求。例如，在医疗领域，RAG 能从医学数据库中提取病例分析或药物研究；在金融领域，它可以获取实时的经济数据或市场动态。这种可定制化能力，使 RAG 能够成为各行业内的高效工具，满足特定场景的知识需求。\n\n综合来看，RAG 不仅突破了传统模型的局限性，还在实时性、准确性、内存使用效率以及领域适应性方面表现出众。其独特的检索与生成结合能力，使其在复杂、多变的实际应用中成为不可或缺的智能系统。\n\n**LangChain Memory 结合 LLMs：为聊天机器人赋予记忆能力**\n\n在人工智能技术迅速发展的今天，聊天机器人已成为我们日常生活中不可或缺的工具。从解答客户问题到提供个性化服务，这些智能助手覆盖了多种场景。然而，传统聊天机器人面临一个核心限制：它们往往是无状态的，无法记住用户的上下文信息。每次用户的查询都被视为独立交互，而之前的对话内容则被忽略。这种局限性使得对话缺乏连贯性，偶尔显得机械和缺乏人情味。\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"305488238\" data-ratio=\"0.38796296296296295\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_png/wCXWZ24UhxRhvL6Aza01zSdxI9ia5glgEUbYPIkg6CEroIMpU08b4Oiaml0vEwI8Yhq0DvnWmuAXzF6JMF2IcvDw/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n为了解决这一问题，**LangChain Memory** 应运而生。作为一种创新的对话记忆解决方案，LangChain Memory 能够让聊天机器人记住过去的交互历史，并利用这些信息生成更贴近用户需求、更自然的回答。通过引入对话记忆，聊天机器人不仅变得更流畅，还能提供个性化和连贯性的对话体验，为用户带来更加贴心的服务。\n\n\n\n\nLangChain Memory 的核心功能在于其模块化和高效的设计，使开发人员能够轻松将其集成到聊天机器人或其他对话代理中。它提供了一个统一的接口，能够管理不同类型的记忆，包括对话缓冲记忆和对话摘要记忆。这种灵活性让开发者可以根据需求轻松存储和检索对话历史，从而优化聊天机器人的性能。\n\n\n\n\n除此之外，LangChain Memory 还具备学习和适应新信息的能力。它能够根据存储的上下文信息，生成更准确和相关的响应。例如，当用户与机器人进行多轮对话时，LangChain Memory 可以通过记住用户的偏好和先前提到的内容，提供更个性化的服务。这种上下文意识让聊天机器人显得更加智能和人性化。\n\n\n\n\n值得一提的是，LangChain Memory 可以与多种语言模型（LLMs）无缝集成，包括预训练模型如 GPT-3、ChatGPT，以及自定义模型。开发人员可以根据具体需求选择合适的语言模型，并结合 LangChain Memory 的记忆功能，打造反应迅速、体验出色的聊天系统。\n\n\n\n\n通过结合 Streamlit 和 OpenAI GPT API，LangChain Memory 的潜力进一步被放大。开发者可以利用 Streamlit 构建友好的前端界面，并通过 GPT API 提供强大的语言生成能力。这种结合不仅让聊天机器人更智能，还为开发人员提供了一个高效的开发框架，快速实现从基础对话到复杂互动的全流程体验。\n\n\n\n\nLangChain Memory 的出现，为解决传统聊天机器人“无状态”这一痛点提供了突破性方案。它不仅提升了对话的连贯性和自然性，还大幅拓展了聊天机器人的应用边界。未来，随着更多技术的融合，聊天机器人将以更智能、更人性化的姿态，为用户提供更加优质的服务体验。\n                  \n```\npip install langchain\npip install openai\nimport os \nos.environ['OPENAI_API_KEY'] = \"your-openai-api-key\"\nfrom langchain.llms import OpenAI\nfrom langchain.chains import ConversationChain\n```\n                                 \n```\nfrom langchain.memory import ConversationBufferMemory\nconversation_with_memory = ConversationChain(\n    llm=OpenAI(temperature=0,openai_api_key=os.getenv(\"OPENAI_API_KEY\")), \n    memory=ConversationBufferMemory(), \n    verbose=True\n)\n\n\nconversation_with_memory.predict(input=\"你好，我是Kevin\") \nconversation_with_memory.predict(input=\"我是一个人工智能爱好者，喜欢通过公众号分享人工智能领域相关的知识\") \nconversation_with_memory.predict(input=\"我希望你能用我的名字为我的公众号设计一个专业名称\") \nconversation_with_memory.predict(input=\"你还可以给出更多选项吗\")\n```\n\n\n\n\n在 LangChain 中，链通常用于分解任务，由链接组成。Lang Chain提供了 ConversationChain，它是专门为有一些记忆概念的场景而创建的。创建类的实例时ConversationChain，必须提供三个参数：\n\n\n\n\nllm，指定用于生成响应的语言模型；\n\n\n\n\nmemory，它确定用于存储对话历史记录的内存类型；\n\n\n\n\nverbose，控制通话过程中是否打印提示等信息。\n\n\n\n\n**更多内容参考：https://zhuanlan.zhihu.com/p/640240974**\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-21T04:09:25Z"
  },
  {
    "id": 13,
    "title": "\n\nRAG和RAU：自然语言处理中检索增强语言模型的调查\n",
    "url": "https://mp.weixin.qq.com/s/_68g-Ph9TOdwPVTEeghbPw",
    "content": "\n大型语言模型（LLMs）推动了自然语言处理（NLP）的显著进步，但它们也面临着幻觉和需要特定领域知识等挑战。\n\n为了减轻这些问题，最近的方法将来自外部资源的信息与LLMs相结合，在 NLP 任务中显著提高了其性能。这篇综述论文解决了检索增强语言模型（RALMs）缺乏全面概述的问题，包括检索增强生成（RAG）和检索增强理解（RAU），深入探讨了它们的范式、演变、分类和应用。论文讨论了 RALMs 的基本组件，包括检索器、语言模型和增强，以及它们的交互如何导致不同的模型结构和应用。\n\nRALMs 在各种任务中显示出实用性，从翻译和对话系统到知识密集型应用。综述包括几种 RALMs 的评估方法，强调在评估中稳健性、准确性和相关性的重要性。它也承认了 RALMs 的限制，特别是在检索质量和计算效率方面，为未来的研究提供了方向。\n\n总之，这项调查旨在为 RALMs、它们的潜力以及它们在 NLP 未来发展的途径提供一个结构化的见解。\n\n论文还附有一个包含所调查作品和进一步研究资源的 GitHub 仓库：https://github.com/2471023025/RALM_Survey。\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"305488206\" data-ratio=\"0.3712962962962963\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_png/wCXWZ24UhxT4wXZYJrwr0Mcl4aeKoJibBYdCicia10AXprzc2ybmc23mz2suyfLibZNNeLeTytXxuxb8xR9sl0GbbA/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n我们的这份调查总结了 RALM 的多个方面，包括：定义、检索器、LM、增强、数据源、应用、评估等。\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"305488207\" data-ratio=\"0.9138888888888889\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_png/wCXWZ24UhxT4wXZYJrwr0Mcl4aeKoJibBwdZG05KGEt9nBeXQqicDEzVdwB3SjquG7jk9Ca8x3skPgTWflBU7Pug/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n检索增强语言模型（RALM）是通过检索信息来优化语言模型（LM）的输出，以获得用户满意的结果的过程。\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"305488208\" data-ratio=\"0.712037037037037\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_png/wCXWZ24UhxT4wXZYJrwr0Mcl4aeKoJibB1f2yufqBicz4nZicoEFhtPbPuqmnNl24nXJHP5pqQo6yPWJbFIXG5IgQ/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\n槽填充是一种在自然语言处理中使用的技巧，用于识别和从用户提供的文本或语音中提取特定信息。在槽填充中，系统预先定义一组槽位，每个槽位代表一个特定的信息需求。\n\n提高检索质量可以从两个方面考虑：提高用于检索的数据集质量，以及提高检索技术的性能。如今，许多数据集被提供给LLM生成相关内容，而由于LLM本身存在“幻觉”，必须采取某些手段来确保数据的准确性，例如使用人类监督进行细化。\n\n在智能对话任务中，意图识别是一种非常重要的技术，它可以帮助系统理解用户的输入，从而提供更加准确和个性化的回答和服务。\n\n意图识别和槽位填充是对话系统中的基础任务。下面仓库实现了一个基于BERT的意图（intent）和槽位（slots）联合预测模块。\n\n想法上实际与JoinBERT类似，利用 [CLS] token对应的last hidden state去预测整句话的intent，并利用句子tokens的last hidden states做序列标注，找出包含slot values的tokens。\n\n你可以自定义自己的意图和槽位标签，并提供自己的数据，通过下述流程训练自己的模型，并在JointIntentSlotDetector类中加载训练好的模型直接进行意图和槽值预测。\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"305488209\" data-ratio=\"0.3527777777777778\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_png/wCXWZ24UhxT4wXZYJrwr0Mcl4aeKoJibBibdQAODNtyibibtH85Lt6mhfMnLhNAvY5h6OesQaKZibONe8G8ukbDAdYQ/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n训练脚本：python train.py\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n```\nimport os\nimport argparse\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\nfrom seqeval.metrics import accuracy_score\n\n\nimport torch\nfrom torch.utils.data import DataLoader\n\n\nfrom transformers import BertTokenizer\nfrom transformers import get_linear_schedule_with_warmup\n\n\nfrom datasets import IntentSlotDataset\nfrom models import JointBert\nfrom tools import save_module, split_data\n\n\n\n\ndef dev(model, val_dataloader, device, slot_dict):\n    model.eval()\n    intent_acc, slot_acc = 0, 0\n    all_true_intent, all_pred_intent = [], []\n    with torch.no_grad():\n        for step, batch in enumerate(val_dataloader):\n            input_ids, intent_labels, slot_labels = batch\n\n\n            outputs = model(\n                input_ids=torch.tensor(input_ids).long().to(device),\n                intent_labels=torch.tensor(intent_labels).long().to(device),\n                slot_labels=torch.tensor(slot_labels).long().to(device)\n            )\n\n\n            intent_probs = torch.softmax(outputs[\"intent_logits\"], dim=-1).detach().cpu().numpy()\n            slot_probs = torch.softmax(outputs[\"slot_logits\"], dim=-1).detach().cpu().numpy()\n            slot_ids = np.argmax(slot_probs, axis=-1)\n            intent_ids = np.argmax(intent_probs, axis=-1)\n            slot_ids = slot_ids.tolist()\n            intent_ids = intent_ids.tolist()\n\n\n            slot_ids = [[slot_dict[i] for i in line] for line in slot_ids]\n            slot_labels = [[slot_dict[i] for i in line] for line in slot_labels]\n\n\n            all_true_intent.extend(intent_labels)\n            all_pred_intent.extend(intent_ids)\n\n\n            intent_acc += accuracy_score(intent_labels, intent_ids)\n            slot_acc += accuracy_score(slot_labels, slot_ids)\n\n\n    intent_avg, slot_avg = intent_acc / len(val_dataloader), slot_acc / len(val_dataloader)\n    dev_acc = intent_avg + slot_avg\n    return dev_acc, intent_avg, slot_avg\n\n\n\n\ndef train(args):\n    # 模型保存位置\n    model_save_dir = args.save_dir + \"/\" + args.model_path.split(\"/\")[-1]\n\n\n    with open(args.slot_label_path, 'r') as f:\n        slot_labels = f.read().strip('\\n').split('\\n')\n    slot_dict = dict(zip(range(len(slot_labels)), slot_labels))\n\n\n    # -----------set cuda environment-------------\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.cuda_devices\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n\n    # -----------load tokenizer-----------\n    tokenizer = BertTokenizer.from_pretrained(args.model_path)\n    save_module(tokenizer, model_save_dir)\n\n\n    # -----------load data-----------------\n    train_data, val_data = split_data(args.train_data_path, args.train_val_data_split)\n\n\n    train_dataset = IntentSlotDataset.load_from_path(\n        data_content=train_data,\n        intent_label_path=args.intent_label_path,\n        slot_label_path=args.slot_label_path,\n        tokenizer=tokenizer\n    )\n\n\n    val_dataset = IntentSlotDataset.load_from_path(\n        data_content=val_data,\n        intent_label_path=args.intent_label_path,\n        slot_label_path=args.slot_label_path,\n        tokenizer=tokenizer\n    )\n\n\n    # -----------load model and dataset-----------\n    model = JointBert.from_pretrained(\n        args.model_path,\n        slot_label_num=train_dataset.slot_label_num,\n        intent_label_num=train_dataset.intent_label_num\n    )\n    model = model.to(device).train()\n\n\n    train_dataloader = DataLoader(\n        train_dataset,\n        shuffle=True,\n        batch_size=args.batch_size,\n        collate_fn=train_dataset.batch_collate_fn)\n\n\n    val_dataloader = DataLoader(\n        val_dataset,\n        shuffle=True,\n        batch_size=args.batch_size,\n        collate_fn=val_dataset.batch_collate_fn)\n\n\n    # -----------calculate training steps-----------\n    if args.max_training_steps > 0:\n        total_steps = args.max_training_steps\n    else:\n        total_steps = len(train_dataset) * args.train_epochs // args.gradient_accumulation_steps // args.batch_size\n\n\n    print('calculated total optimizer update steps : {}'.format(total_steps))\n\n\n    # -----------prepare optimizer and schedule------------\n    parameter_names_no_decay = ['bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        # 这些参数会被应用常规的权重衰减（由 args.weight_decay 指定）\n        {'params': [\n            para for para_name, para in model.named_parameters()\n            if not any(nd_name in para_name for nd_name in parameter_names_no_decay)\n        ],\n            'weight_decay': args.weight_decay},\n        # 这些参数的权重衰减被设置为0\n        {'params': [\n            para for para_name, para in model.named_parameters()\n            if any(nd_name in para_name for nd_name in parameter_names_no_decay)\n        ],\n            'weight_decay': 0.0}\n    ]\n    optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    # 学习率变化（更新）方式\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=total_steps)\n\n\n    # -----------training-------------\n    max_acc = 0\n    for epoch in range(args.train_epochs):\n        total_loss = 0\n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            input_ids, intent_labels, slot_labels = batch\n\n\n            outputs = model(\n                input_ids=torch.tensor(input_ids).long().to(device),\n                intent_labels=torch.tensor(intent_labels).long().to(device),\n                slot_labels=torch.tensor(slot_labels).long().to(device)\n            )\n\n\n            loss = outputs['loss']\n            total_loss += loss.item()\n\n\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n\n            loss.backward()\n\n\n            if step % args.gradient_accumulation_steps == 0:\n                # 用于对梯度进行裁剪，以防止在神经网络训练过程中出现梯度爆炸的问题。\n                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n\n\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n\n\n        train_loss = total_loss / len(train_dataloader)\n\n\n        dev_acc, intent_avg, slot_avg = dev(model, val_dataloader, device, slot_dict)\n\n\n        flag = False\n        if max_acc < dev_acc:\n            max_acc = dev_acc\n            flag = True\n            save_module(model, model_save_dir)\n        print(f\"[{epoch}/{args.train_epochs}] train loss: {train_loss}  dev intent_avg: {intent_avg} \"\n              f\"def slot_avg: {slot_avg} save best model: {'*' if flag else ''}\")\n\n\n    dev_acc, intent_avg, slot_avg = dev(model, val_dataloader, device, slot_dict)\n    print(\"last model dev intent_avg: {} def slot_avg: {}\".format(intent_avg, slot_avg))\n    print(\"模型保存位置：\" + model_save_dir)\n\n\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n\n    # environment parameters\n    parser.add_argument(\"--cuda_devices\", type=str, default='0', help='set cuda device numbers')\n\n\n    # model parameters\n    parser.add_argument(\"--model_path\", type=str, default='./bert-base-chinese', help=\"pretrained model loading path\")\n\n\n    # data parameters\n    parser.add_argument(\"--train_data_path\", type=str, default='data/SMP2019/data.json', help=\"training data path\")\n    parser.add_argument(\"--train_val_data_split\", type=float, default=0.8, help=\"training data and val data split rate\")\n    parser.add_argument(\"--slot_label_path\", type=str, default='data/SMP2019/slot_labels.txt', help=\"slot label path\")\n    parser.add_argument(\"--intent_label_path\", type=str, default='data/SMP2019/intent_labels.txt', help=\"intent label path\")\n\n\n    # training parameters\n    parser.add_argument(\"--save_dir\", type=str, default='./save_model', help=\"directory to save the model\")\n    parser.add_argument(\"--max_training_steps\", type=int, default=0, help='max training step for optimizer(优化器的最大训练步数), if larger than 0')\n    parser.add_argument(\"--gradient_accumulation_steps\", type=int, default=1, help=\"number of updates steps to accumulate before performing a backward() pass.(执行 backward() 之前累积的更新步数数量)\")\n\n\n    parser.add_argument(\"--batch_size\", type=int, default=32, help='training data batch size')\n    parser.add_argument(\"--train_epochs\", type=int, default=20, help='training epoch number')\n\n\n    parser.add_argument(\"--learning_rate\", type=float, default=5e-5, help='learning rate')\n    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-8, help=\"epsilon for Adam optimizer\")\n    parser.add_argument(\"--warmup_steps\", type=int, default=0, help=\"warmup step number\")\n    parser.add_argument(\"--weight_decay\", type=float, default=0.0, help=\"weight decay rate\")\n    parser.add_argument(\"--max_grad_norm\", type=float, default=1.0, help=\"maximum norm for gradients\")\n\n\n    args = parser.parse_args()\n\n\n    train(args)\n```\n\n推理脚本：python predict.py\n                                                            \n```\nfrom detector import JointIntentSlotDetector\nimport time\n\n\nstart1_time = time.perf_counter()\nmodel = JointIntentSlotDetector.from_pretrained(\n    model_path='./save_model/bert-base-chinese',\n    tokenizer_path='./save_model/bert-base-chinese',\n    intent_label_path='./data/SMP2019/intent_labels.txt',\n    slot_label_path='./data/SMP2019/slot_labels.txt'\n)\nstart2_time = time.perf_counter()\nall_text = ['定位我现在的位置', \"现在几点了\", \"2013年亚洲冠军联赛恒广州恒大比赛时间。\", \"帮我查一下赣州到厦门的汽车\", \"导航到望江西路上去\", \"把张玉娟的手机号码发送给吴伟\", \"打电话给xxx\", \"经XXX的电话号码发给lc\"\n            \"发信息给盛吉\", \"将你在哪发送给纲吉\", \"发信息给老妈说我在吃饭\", \"我要听稻香\", \"访问浏览器\", \"中国制用英文怎么说\"]\nfor i in all_text:\n    print(model.detect(i))\nend_time = time.perf_counter()\ntime1 = (end_time - start1_time) / 3600\ntime2 = (end_time - start2_time) / 3600\nprint(\"所有检测时间（包括加载模型）：\", time1, \"s\", \"除去模型加载时间：\", time2, \"s\",\n      \"总预测数据量：\", len(all_text), \"平均预测一条的时间（除去加载模型）：\", time2 / len(all_text), \"s/条\")\n```\n\n源码：https://github.com/mzc421/NLP/tree/main/bert-intent-slot\n\n<img class=\"rich_pages wxw-img\" data-galleryid=\"\" data-imgfileid=\"305486805\" data-ratio=\"0.3435185185185185\" data-s=\"300,640\" src=\"https://mmbiz.qpic.cn/mmbiz_png/wCXWZ24UhxRVjRlnzkIBia8BsDVoETqMzHHaTn2NhzbWr1aofbRcibNpibXyRR6Rx0BCr9eibH3cia595QriabcV9ZvA/640?wx_fmt=png& from=appmsg\" data-type=\"png\" data-w=\"1080\"  referrerpolicy=\"no-referrer\">\n\n\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-21T04:09:22Z"
  },
  {
    "id": 14,
    "title": "只改一行代码，在第四代至强®可扩展平台上高效微调优化ChatGLM-6B",
    "url": "https://blog.csdn.net/inteldevzone/article/details/144559604",
    "content": "\n作者：英特尔公司 夏磊\n\n[https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/fine-tuning-optimization-chatglm-6b-modification.html](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/fine-tuning-optimization-chatglm-6b-modification.html)\n\n### 概述：\n    开源预训练大模型 ChatGLM-6B 通过以下三个方面可实现基于第四代英特尔® 至强® 可扩展处理器的高效微调优化：一是借助英特尔® 高级矩阵扩展，大幅提升模型微调计算速度；二是结合英特尔® MPI 库充分利用处理器架构特点和多核配置，发挥 CPU 的整体效率；三是利用英特尔® 至强® CPU Max 系列处理器集成的 HBM 满足大模型微调所需的大内存带宽。    \n#### **大语言模型的应用与微调优化必要性**\n\nChatGPT 的横空出世开启了大语言模型 (LLM) 的普及元年，BERT、GPT-4、ChatGLM 等模型的非凡能力则展现出类似通用人工智能 (AI) 的巨大潜力，也因此得到了多行业、多领域的广泛关注。为加速这些大模型与特定领域的深度融合，以及更好地适应特定任务，基于任务特性对这些模型进行定制化微调至关重要。然而，它们庞大的参数使得用传统方式对大模型进行调优面临诸多挑战，不仅要求相关人员熟练掌握微调技巧，还需要付出巨大的训练成本。近年来，出现了参数高效微调 (Parameter-Efficient Fine-Tuning, PEFT) 和提示微调 (Prompt-tuning) 技术。这些技术因其成本更低、应用方式更简单便捷，正在逐渐取代大模型传统调优方法。\n\n本文结合目前在中文应用场景中具有出色表现的开源预训练大模型 ChatGLM-6B，介绍如何通过对其开源 Prompt-tuning 代码进行极少量的修改，并结合第四代英特尔® 至强® 可扩展处理器1的全新内置 AI 加速引擎——英特尔® 高级矩阵扩展 (Intel® Advanced Matrix Extension，简称英特尔® AMX) 及配套的软件工具，来实现高效、低成本的大模型微调。\n\n\n\n#### **基于英特尔****® 架构硬件的微调优化方案**\n\n本文通过以下三个方面实现了基于第四代英特尔® 至强® 可扩展处理器的 ChatGLM 高效微调优化：\n \n\n**1. 借助英特尔****® AMX，大幅提升模型微调计算速度**\n\nAMX 是内置于第四代英特尔® 至强® 可扩展处理器中的矩阵乘法加速器，能够更快速地处理 BFloat16 (BF16) 或 INT8 数据类型的矩阵乘加运算，从而显著提升模型训练和推理的性能。\n\n<img alt=\"\" height=\"927\" src=\"https://i-blog.csdnimg.cn/direct/89fc63559734469ea29993798260d27e.png\" width=\"1200\" referrerpolicy=\"no-referrer\">\n\n\n\n\n\n图 1. 英特尔® AMX 技术架构\n\n\n\n目前，现行的 PyTorch 框架中，已经可以通过具备 BF16 自动混合精度功能自动实现对 AMX 加速器的利用。\n\n就 ChatGLM-6B 而言，其开源微调代码的 autocast_smart_context_manager() 函数，也已具备对 CPU 自动混合精度的支持。因此，只需在启动微调时加入 CPU 自动混合精度的使能参数即可直接利用英特尔® AMX 带来的优势。\n\n通过trainer.py 中的 autocast_smart_context_manager() 函数，在 ChatGLM-6B 开源 prompt-tuning 目录下实现对 CPU 和 GPU 的自动混合精度支持\n\n\n\n\n\n<img alt=\"\" height=\"927\" src=\"https://i-blog.csdnimg.cn/direct/5d1ddb6538c543d5b1ca793d42b66479.png\" width=\"1200\" referrerpolicy=\"no-referrer\">\n\n图 2. 通过 trainer.py 中的 autocast_smart_context_manager() 函数，在 ChatGLM-6B 开源 prompt-tuning 目录下实现对 CPU 和 GPU 的自动混合精度支持\n\n\n\n具体方法是在启动微调的 train.sh 脚本时做如下修改：\n\n```\npython3 main.py \\\n  --do_train \\\n  …\n  --half_precision_backend cpu_amp \\\n  --bf16\n```\n\n\n\n\n\n\n\n**2. 结合英特尔****® MPI 库充分利用处理器架构特点和多核配置，发挥 CPU 的整体效率**\n\n第四代英特尔® 至强® 可扩展处理器最多可拥有 60 个内核。这些内核通过 4 个集群 (cluster) 的方式进行内部组织。理论上，当多个处理器内核并行处理一个计算任务并需要共享或交换数据时，同一个集群内的内核之间的通信时延较低。因此，在使用 PyTorch 框架进行模型微调时，我们可以将同一个集群上的内核资源分配给同一个 PyTorch 实例，从而为单个实例提供更理想的计算效率。此外，通过利用 PyTorch 的分布式数据并行 (Distributed Data Parallel, DDP) 功能，还可将两个 CPU 上的 8 个集群的内核资源汇集在一起，充分发挥整体效率。\n\n\n\n\n\n<img alt=\"\" height=\"927\" src=\"https://i-blog.csdnimg.cn/direct/f10b7dbc485a421b9f1dc654601b6cbe.png\" width=\"1200\" referrerpolicy=\"no-referrer\">\n\n图 3. 第四代英特尔® 至强® 可扩展处理器的内部集群 (cluster) 架构\n\n\n\n为实现从应用程序代码到数据通信的整体简化，PyTorch 框架支持多种分布式数据并行后端 (backend)，其中 MPI 后端方式能够很好地满足我们的优化需求。\n\n\n\n<img alt=\"\" height=\"927\" src=\"https://i-blog.csdnimg.cn/direct/04348f91f75945588bb827856ecbdc64.png\" width=\"1200\" referrerpolicy=\"no-referrer\">\n\n[](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/fine-tuning-optimization-chatglm-6b-modification.html#)\n\n图 4. PyTorch 支持的多种分布式数据并行的后端（来源：PyTorch 官网）\n\n\n\n但是，通过 pip 或 conda 来安装的预编译PyTorch 二进制包中并未将 MPI 的后端作为缺省功能编译。因此，我们需要安装 MPI 协议工具库并通过手工编译来获得对 MPI 后端的支持。\n\n英特尔® MPI 库2是一个实现 MPICH 规范的多结构消息传递库，使用该库可创建、维护和测试能够在英特尔® 处理器上实现更优性能的先进和复杂的应用。它采用 OFI 来处理所有通信，能够提供更高的吞吐量、更低的时延和更简单的程序设计。\n\n以下是基于英特尔® MPI库的 PyTorch 编译步骤:\n\n#下载英特尔® MPI库并安装\n\n```\nwget https://registrationcenter-download.intel.com/akdlm/IRC_NAS/718d6f8f-2546-4b36-b97b-bc58d5482ebf/l_mpi_oneapi_p_2021.9.0.43482_offline.sh \nbash l_mpi_oneapi_p_2021.9.0.43482_offline.sh\nsource your_installation_path/vars.sh\n```\n\n# 安装 PyTorch 编译依赖包\n\n```\npip install numpy pyyaml mkl mkl-include setuptools cmake cffi typing\npip install mkl_include\n```\n\n#下载 PyTorch 源码并完成编译、安装\n\n```\ngit clone --recursive  https://github.com/pytorch/pytorch.git\ncd pytorch\npython setup.py install\n```\n\n在获得了支持 MPI 后端的 PyTorch 后，只需按如下方法在 ChatGLM Prompt-tuning 目录下的 main.py 修改一行代码：\n\n将dist.init_process_group(backend='gloo', world_size=1, rank=0) 改为**dist.init_process_group(backend='mpi')**\n\n\n\n<img alt=\"\" height=\"927\" src=\"https://i-blog.csdnimg.cn/direct/95737892061a4b0aa349e90b646df417.png\" width=\"1200\" referrerpolicy=\"no-referrer\">\n\n[](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/fine-tuning-optimization-chatglm-6b-modification.html#)\n\n图 5. 修改前的 main.py\n\n<img alt=\"\" height=\"927\" src=\"https://i-blog.csdnimg.cn/direct/010ee645aa574a74b89053d7af14a982.png\" width=\"1200\" referrerpolicy=\"no-referrer\">\n\n[](https://www.intel.cn/content/www/cn/zh/customer-spotlight/cases/fine-tuning-optimization-chatglm-6b-modification.html#)\n\n图 6. 修改后的 main.py\n\n\n\n**3. 利用至强****® CPU Max 系列集成的 HBM 满足大模型微调所需的大内存带宽**\n\n基于 Transformer 的大模型，由于参数、训练数据和模型规模的复杂程度较高，因此内存复杂度通常是 O(**n****2**)。这意味着这些大模型需要足够大的内存带宽支持才能获得更好的运行性能。英特尔® 至强® CPU Max 系列3，配备 64 GB 的 HBM2e 高带宽内存，为在 CPU 上高效运行大模型提供了高达~1TB/s的内存带宽支持3。\n\n该 CPU 集成的 HBM，能够在 3 种模式下灵活配置：\n \n   HBM-Only 模式——支持内存容量需求不超过 64 GB 的工作负载，具备每核 1 至 2 GB 的内存扩展能力，无需更改代码和另购 DDR，即可启动系统。      HBM Flat 模式——可为需要大内存容量的应用提供灵活性，通过 HBM 和 DRAM 提供一个平面内存区域 (flat memory region)，适用于每核内存需求 >2 GB 的工作负载。可能需要更改代码。      HBM 高速缓存模式——为内存容量 >64 GB或每核内存需求 >2GB 的工作负载提供更优性能。无需更改代码，HBM 将用作 DDR 的高速缓存。   \n\n\n针对 ChatGLM-6B 微调，试验结果显示：与其他两种模式相比，**HBM 高速缓存模式**在性能和使用方便性方面均更胜一筹。\n\n在英特尔® 至强® CPU Max 系列产品上，结合之前的两项优化，我们可以通过以下命令行启动 ChatGLM-6B 微调：\n\n<img alt=\"\" height=\"927\" src=\"https://i-blog.csdnimg.cn/direct/5c7d2eeb379144d787a5a044c88c1309.png\" width=\"1200\" referrerpolicy=\"no-referrer\">\n\n\n\n\n\n\n\n图 7. 在拥有 32 个物理核的英特尔® 至强® CPU Max 9462 双路服务器上启动微调\n\n\n\n#### **优化结果**\n\n通过以上简单软、硬件综合优化，无须采用昂贵的 GPU 硬件，即可实现对 ChatGLM-6B 模型的高性能微调。\n\n**注：**以上代码修改需要配合 python 工具包 accelerate 0.18.0 和 transformers 4.28.0。\n\n\n\n**作者简介：**\n 夏磊，英特尔（中国）有限公司人工智能首席工程师，拥有近 20 年的人工智能从业经验，在软件算法、自动控制和工程管理等领域积累了丰富经验。\n",
    "labels": [],
    "created_at": "2024-12-21T04:09:20Z"
  },
  {
    "id": 8,
    "title": "\n\nVLM论文深度解析：揭秘多模态大模型如何联动权重、任务与视觉嵌入\n",
    "url": "https://mp.weixin.qq.com/s/b5xegxL3Ij9gh1pKY2d-Kw",
    "content": "\n自大数据时代的到来以来，大型语言模型（LLMs）取得了显著进展，展现了前所未有的应用场景和出色的泛化能力。这些进展为各类智能应用奠定了基础，涵盖从自然语言处理到复杂的推理任务等多个领域。\n\n为了进一步提升模型的能力，研究者们开始引入视觉图像作为输入，推动了多模态大型语言模型（MLLMs）的发展。这类模型不仅能生成具有连贯性的语言响应，还能在跨模态理解方面展现出卓越的能力，能够处理诸如图像标题生成、视觉问题回答以及图像中不同对象的定位等任务。\n\n在现有的多模态语言模型中，研究者们探索了不同的策略，以提升LLMs对视觉指令的响应能力。首先，有的研究通过在预训练阶段冻结LLMs，仅使用一个投影网络来进行视觉语言对齐。\n\n例如，LLaMA-Adapter V2通过引入一个简单的MLP层，而mPLUG-Owl则基于注意力机制设计了视觉摘要器。其次，部分方法通过构建新的训练任务数据，赋予模型新的视觉理解能力。例如，Kosmos-2引入了指称对话任务，而Shikra则通过区域级定位来增强模型的视觉理解能力。另外，也有模型通过引入高级图像编码器来提取视觉嵌入，如LLaVA使用了CLIP编码器，而MiniGPT-4则采用了Q-Former。\n\n在视觉语言模型（VLM）的丛林中，SPHINX 的诞生方式相当令人印象深刻。想象一下，像阿尔伯特·爱因斯坦、艾萨克·牛顿和尼古拉·特斯拉这样的专家一起合作在一个项目上！这正是 SPHINX 所做的事情——它将顶级 AI 模型的力量结合到一个“篮子”中，以实现多任务处理的流畅性。\n\n本文提出了一种创新的多模态语言模型——SPHINX，它结合了四个关键元素：模型权重、调优任务、视觉嵌入和高分辨率子图像。通过这种多维度的融合，SPHINX展示了在多个应用场景中的强大表现。接下来，我们将详细介绍这一方法的主要特点和实验发现。\n\nSPHINX模型的整体混合范式，采用了两阶段训练流程：第一阶段是视觉-语言对齐的预训练，第二阶段是视觉指令跟随的微调。每个阶段都应用了提出的模型权重混合策略和调优任务。整个模型由一个大型语言模型（如LLaMA-2）、一个视觉编码器的混合结构以及两个线性投影层组成。\n\n在阶段一的预训练过程中，我们解冻了LLM以进行视觉-语言对齐。与现有的多模态大型语言模型（如Zhu等，2023；Li等，2023d；Dai等，2023）通常采用的冻结LLM方法不同，后者在预训练阶段通常冻结整个LLM，仅训练中间的投影层来实现视觉-语言对齐。\n\n这种策略虽然能够避免LLMs过度拟合生成简短的句子，但也限制了其在大规模视觉-语言数据上的跨模态学习潜力。为了解决这一问题，SPHINX解冻了整个LLM以及可学习的线性投影层，从而实现了更充分的视觉-语言适应。同时，为了保持高质量的图像表示，视觉编码器在这一阶段保持冻结状态。\n\n与LLMs不同，它只处理文本数据，视觉语言 MLLMs 还必须处理图像，这意味着输入到LLM解码器的不仅仅是文本，还包括图像。然而，这是LLMs本身并不理解的信息类型。\n\n这既有优点也有缺点：\n\n优点：它可以防止LLMs过度拟合，仅生成简短文本，因为图像标题数据集中的文本通常非常简洁。\n\n缺点：LLMs无法从图像中学习信息或理解文本与图像之间的关系，这意味着在生成文本时，它们会错失图像中的大量信息。\n\n如何兼顾利弊？\n\n**解决方案**\n答案在于这个思路：解冻并预训练LLM，在这一过程中让模型从额外的数据集中学习（这是SPHINX的独特功能）。\n\n该阶段的详细步骤如下：\n\n我们将使用真实世界的数据集（LAION-400M）对LLM（具体为LLama2）进行预训练。此时，输入将是视觉嵌入（来自视觉编码器模块的输出）。\n\n在初步的LLM预训练之后，我们将以这些预训练权重为起点，在合成数据集（LAION-COCO）上进一步训练相同的LLM。这个步骤的原因将在后面解释。\n\n此外，我们还将使用另一个仅包含文本的数据集——RefinedWeb，对LLM进行预训练。\n\n在每次训练过程中，我们将从RefinedWeb中采样一个数据点，并从图像标题数据集中采样几个数据点。然后，我们使用两个损失函数依次进行训练，一个损失函数要求模型保持生成长文本的能力，另一个则确保模型能够准确描述图像内容。\n\n这种方法非常有效。没有与RefinedWeb一起训练时，模型会严重过拟合（如橙色所示）。相反，采用RefinedWeb进行训练时，模型不会过度拟合生成短文本（如绿色所示）。\n\n**通过组合LLM权重来综合知识**\n\n**理由**\n一个合成数据集可能包含真实数据集所缺乏的独特信息。因此，作者希望LLM能够同时处理这两种类型的信息。\n\n**解决方案**\n我们可以在两个数据集上同时训练模型，但作者认为这种做法会使模型难以收敛，并且可能过于苛刻。\n                                                   \n```\nfrom SPHINX import SPHINXModel\nfrom PIL import Image\nimport torch\n# Besides loading the `consolidated.*.pth` model weights, from_pretrained will also try to \n# use `tokenizer.model', 'meta.json', and 'config.json' under `pretrained_path` to configure\n# the `tokenizer_path`, `llama_type`, and `llama_config` of the model. You may also override\n# the configurations by explicitly specifying the arguments\nmodel = SPHINXModel.from_pretrained(pretrained_path=\"path/to/checkpoint\", with_visual=True)\nimage = Image.open(\"examples/1.jpg\")\nqas = [[\"What's in the image?\", None]]\nresponse = model.generate_response(qas, image, max_gen_len=1024, temperature=0.9, top_p=0.5, seed=0)\nprint(response)\n# if you wanna continue\nqas[-1][-1] = response\nqas.append([\"Then how does it look like?\", None])\nresponse2 = model.generate_response(qas, image, max_gen_len=1024, temperature=0.9, top_p=0.5, seed=0)\nprint(response2)\n```\n                                                                                                                     \n```\nfrom SPHINX import SPHINXModel\nfrom PIL import Image\nimport torch\nimport torch.distributed as dist\nimport multiprocessing as mp\ndef main(world_size, rank) -> None:\n    dist.init_process_group(\n        backend=\"nccl\", rank=rank, world_size=world_size,\n        init_method=f\"tcp://127.0.0.1:23560\",\n    )\n    torch.cuda.set_device(rank)\n\n\n    # mp_group tells the model which ranks will work together\n    # through model parallel to compose a complete model.\n    # When mp_group is None, a single-rank process group will\n    # be created and used, which means model parallel size = 1 (not enabled)\n    model = SPHINXModel.from_pretrained(\n        pretrained_path=\"path/to/checkpoint\", with_visual=True,\n        mp_group=dist.new_group(ranks=list(range(world_size)))\n    ) \n\n\n    # it's important to make sure that ranks within the same \n    # model parallel group should always receive the same input simultaneously\n    image = Image.open(\"examples/1.jpg\")\n    qas = [[\"What's in the image?\", None]]\n    response = model.generate_response(qas, image, max_gen_len=1024, temperature=0.9, top_p=0.5, seed=0)\nif __name__ == \"__main__\":\n    N_GPU = 2\n    assert N_GPU in [1, 2, 4, 8]\n    if N_GPU == 1:\n        main(world_size=1, rank=0)\n    else:\n        # You can use whatever method, e.g. torchrun, slurm, etc. for distributed launch\n        # Just be sure to initialize torch distributed (by invoking dist.init_process_group)\n        # before creating the SPHINX model if model parallel size > 1 is used\n        mp.set_start_method(\"spawn\")\n        for rank in range(N_GPU):\n            process = mp.Process(target=main, args=(N_GPU, rank))\n            process.start()\n```\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-20T16:43:07Z"
  },
  {
    "id": 9,
    "title": "\n\nLLM加速全攻略：教你降本增效，提升响应速度的必备技巧！\n",
    "url": "https://mp.weixin.qq.com/s/gGsZuxWwftmm05ZgN_TQhA",
    "content": "\n最近，我在深入研究和优化LLM（大语言模型）的应用链路，发现了许多实用技巧，不仅可以显著降低使用成本，还能有效缩短响应延迟。\n\n这些问题是许多开发者在实际项目中经常遇到的痛点，而找到解决方案后，你会惊讶于优化的效果能给模型性能带来多大的提升。\n\n接下来，我将结合实践经验，分享一些行之有效的方法，帮助你让LLM跑得更快、更高效！\n\n上图是一段讲解LLM（大语言模型）使用成本和响应延迟的内容。其核心内容包括以下几点：\n   **LLM的成本构成**：<ul >   成本主要由输入（inputtokens）和输出（outputtokens）的tokens数量决定。\n      具体计算公式以OpenAI的GPT-4Turbo为例，每100万tokens输入收费10美元，输出收费30美元。\n      \n\n   </ul>      **响应延迟的计算**：<ul >   响应延迟与tokens数量相关，但计算更复杂。\n      提供了一种简化公式，指出输入tokens的处理时间远小于输出tokens的处理时间。\n      \n\n   </ul>      **成本与延迟的重点**：<ul >   输入tokens的数量占用LLM使用成本的大头，而复杂任务中输出tokens的长度对延迟影响显著。\n      处理一个 output token 的时间是 input token 的约 200 倍。\n      \n\n   </ul>      **LLMAgents的特性**：<ul >   大约80%的tokens来自输入，包括初始提示词和推理过程中Agent的消耗。\n      输出 tokens 对于响应时间有更大的影响。\n      \n\n   </ul>      响应延迟与tokens数量相关，但计算更复杂。\n      提供了一种简化公式，指出输入tokens的处理时间远小于输出tokens的处理时间。\n      \n\n      大约80%的tokens来自输入，包括初始提示词和推理过程中Agent的消耗。\n      输出 tokens 对于响应时间有更大的影响。\n      \n\n   \n上述整体内容通过数学公式和具体案例，分析了成本和延迟的来源，为优化提供了理论支持。\n\n然而，在线服务场景中，例如聊天机器人，大语言模型（LargeLanguageModel,LLM）推理通常采用流式输出（streaming）的形式。\n\n其中，首个token的生成时延（即TimetoFirstToken,TTFT）是用户感知到的响应时间的关键指标，直接影响整体用户体验。为了提升用户体验，在线服务通常要求首token时延尽可能低，理想情况下控制在一秒以内。\n\nLLM的首token时延受到多个因素的影响，包括模型的参数规模、提示词（Prompt）的长度、批处理大小（BatchSize）以及所使用的GPU资源等。\n\n本文将深入探讨如何优化首token时延，尤其是通过**SystemPromptCaching**技术降低延迟，为在线服务的性能优化提供实用思路和建议。\n\n#### 1.**首 token 时延直接影响用户体验**\n\n首token时延是用户从发送请求到收到LLM推理结果的第一个token所需的时间。这是用户感知服务速度的核心指标。\n\n在聊天机器人、实时翻译等场景中，延迟过长会导致用户体验下降，例如对对话流畅性的预期失落。**理想的首token时延应控制在一秒以内**，以提供更流畅的交互体验。\n\n**举例**：假设用户询问一个问题，如果首token响应需要3秒，用户可能会认为服务卡顿或不够智能；而一秒以内的响应则会显得迅速且自然。\n\n#### 2.**影响首 token 时延的关键因素**\n\n首token时延受到以下因素的综合影响：\n   **模型参数规模：较大的模型（如 GPT-4）由于参数量大，计算复杂度高，生成首 token 的时间通常较长。**      <strong>\n</strong>      **Prompt长度：输入的提示词越长，模型需要解析和处理的数据越多，从而增加推理时间。**      <strong>\n</strong>      **BatchSize（批处理大小）：批量处理的规模会影响单个请求的响应时间，Batch Size 越大，单个请求的时延可能会增加。**      <strong>\n</strong>      **GPU资源：硬件性能直接决定了计算速度，使用高性能 GPU（如 A100 或 H100）可以明显降低时延。**      <strong>\n</strong>   \n**举例**：在一个在线问答系统中，用户提供一个包含500个单词的长提示词，假设模型使用低性能的GPU，可能需要花费多倍的时间生成第一个token，与使用更短提示词和高性能硬件相比，响应速度明显受限。\n\n#### 3.**System Prompt 重复计算导致的时延问题**\n\n在LLM推理中，**SystemPrompt**通常是一段较长的输入，用于定义模型的行为和上下文。例如，在聊天机器人中，SystemPrompt可能包括角色设定或服务规则。\n\n当用户进行多次请求时，每次推理都需要重复计算这段固定的输入内容，从而造成不必要的计算开销，显著增加了首token时延。\n\n**举例**：某用户连续提问五个相关问题，而每次请求中都需要重新处理相同的SystemPrompt，导致浪费大量算力和时间。\n\n#### 4.**System Prompt Caching 的优化作用**\n\n通过**SystemPromptCaching**技术，可以将SystemPrompt的计算结果缓存下来，避免重复计算，从而大幅提高首token的生成速度。\n\nSystemPromptCaching分为以下两种形式：\n   **PrefixSharing：在多用户或多请求场景下，共享计算后的 System Prompt 前缀，减少重复处理。**      <strong>\n</strong>      **PromptCache：直接缓存用户的 System Prompt 计算结果，在后续请求中复用。**      <strong>\n</strong>   \n**实际效果**：通过使用缓存技术，首token时延可以减少30%-50%，显著提升用户体验。\n\n#### 5.**TRT-LLM 推理引擎支持 System Prompt Caching**\n\n**TRT-LLM**是一款专为LLM推理优化的高性能引擎，已经支持SystemPromptCaching功能。这使得它成为降低首token时延的理想选择之一。\n\n**优势**：在实际测试中，通过启用SystemPromptCaching，生成首token的时延显著降低，尤其适用于需要频繁处理长SystemPrompt的场景。\n\n**限制**：当前版本中，SystemPromptCaching与FP8KVCache和INT8KVCache功能不兼容，这意味着同时使用这些功能时会遇到冲突。期待在下一个版本中修复这一问题。\n\n#### 6.**Triton &  TRT-LLM 是推荐的推理解决方案**\n\n在LLM推理领域，**Triton**与**TRT-LLM**是推荐的高性能解决方案：\n   **Triton：支持 RESTFul API，并且支持流式输出（streaming），可以满足各种在线服务的需求。**      <strong>\n</strong>      **TRT-LLM：结合了强大的优化功能，例如 System Prompt Caching 和高效推理引擎，进一步降低时延并提高推理效率。**      <strong>\n</strong>   \n**场景举例**：在电商客服机器人场景中，用户连续提问多个问题，启用SystemPromptCaching后，可以快速响应每个问题，显著减少用户等待时间，提升交互满意度。\n\n通过这篇文章，我们深入探讨了优化大型语言模型（LLM）推理中首token时延的技术关键点，这一指标对提升用户体验至关重要。文章详细介绍了**SystemPromptCaching**的核心概念，包括前缀共享（PrefixSharing）和提示缓存（PromptCache），作为降低计算负载和减少首token生成时间的创新解决方案。\n\n同时，文章还分析了支持该功能的**TRT-LLM**推理引擎，特别是在处理长系统提示时所展现的显著性能提升。\n\n这篇内容不仅为开发者和工程师提供了LLM性能优化的实际指导，还揭示了未来与其他缓存方法增强兼容性的潜力，表明该领域正在快速进步。\n\n如果您对LLM推理性能优化感兴趣，相信本文提供的见解能为您的项目带来启发和价值。随着相关技术的持续发展，更多创新方法将进一步推动LLM的高效应用。\n\n<mp-style-type data-value=\"3\"></mp-style-type>\n",
    "labels": [],
    "created_at": "2024-12-20T16:43:04Z"
  },
  {
    "id": 5,
    "title": "50张图，直观理解混合专家（MoE）大模型",
    "url": "https://blog.csdn.net/OneFlow_Official/article/details/144131666",
    "content": "\n<img alt=\"ba4d4c2ccfcf76a14f931690b2243f4d.png\" height=\"512\" src=\"https://img-blog.csdnimg.cn/img_convert/ba4d4c2ccfcf76a14f931690b2243f4d.png\" width=\"768\" referrerpolicy=\"no-referrer\">\n\n\n\n**[Mixtral 8x7B]()**的高效训练与推理效果曾引发AI社区对混合专家（MoE）模型的广泛关注，后来居上的国产开源大模型**[De‍epSeek]()**以及**[腾讯近期开源的Hunyuan-Large]()**（基于Transformer的最大MoE模型）也选择了MoE框架路线。为何大语言模型总是离不开MoE的身影？\n\n 借助50多个图例，数据科学家Maarten Grootendorst由浅入深多维度剖析了MoE模型，从基础概念出发，逐步介绍MoE核心组件专家和路由机制，以及它们在典型LLM架构中的应用。\n\n（本文经作者授权后由OneFlow编译发布。原文：https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts）\n\n**作者 |Maarten Grootendorst**\n\n**OneFlow编译**\n\n<strong>翻译｜张雪聃、林心宇\n**<strong>题图由****[SiliconCloud]()平台生成**</strong></strong>\n \n\n**1**\n\n### 什么是混合专家（MoE）？\n\n混合专家（MoE）是一种利用多个不同的子模型（或称为“专家”）来提升LLM质量的技术。\n\nMoE的两个主要组成部分是：\n    专家：每个前馈神经网络（FFNN）层现在都有一组“专家”，可以选择其中的一部分。这些“专家”通常也是FFNN。        路由或门控网络：决定哪些词元发送到哪些专家。    \n在每个具有MoE的模型层中，我们会找到（相对专业化的）专家：\n\n<img alt=\"b784290bc715b1d660f1db747145cb6c.png\" src=\"https://img-blog.csdnimg.cn/img_convert/b784290bc715b1d660f1db747145cb6c.png\" referrerpolicy=\"no-referrer\">\n\n\n\n需要注意的是，“专家”并不专注于特定领域，如“心理学”或“生物学”。专家在学习过程中最多只能掌握关于单词层面的句法信息：\n\n<img alt=\"cb36bc3b00a93702eea6ba30700f9a7c.png\" src=\"https://img-blog.csdnimg.cn/img_convert/cb36bc3b00a93702eea6ba30700f9a7c.png\" referrerpolicy=\"no-referrer\">\n\n\n\n更具体地说，专家的专长是在特定上下文中处理特定词元。\n\n路由（门控网络）选择最适合特定输入的专家：\n\n<img alt=\"17ef16aceb0832f55e6cc3a5ed26d26d.png\" src=\"https://img-blog.csdnimg.cn/img_convert/17ef16aceb0832f55e6cc3a5ed26d26d.png\" referrerpolicy=\"no-referrer\">\n\n\n\n单个专家并不是整个LLM，而是LLM架构中的一个子模型部分。\n\n**2**\n\n### 专家\n\n为了探讨专家的含义及其工作方式，我们首先需要了解MoE所替代的内容：密集层。\n\n#### 密集层\n\n混合专家（MoE）始于LLM的相对基本功能，即前馈神经网络（FFNN）。\n\n请记住，标准的仅解码Transformer架构在层归一化后应用FFNN：\n\n<img alt=\"d87f24e698c131dbb10e7d51aab65925.png\" src=\"https://img-blog.csdnimg.cn/img_convert/d87f24e698c131dbb10e7d51aab65925.png\" referrerpolicy=\"no-referrer\">\n\n\n\nFFNN使模型能够利用由注意力机制创建的上下文信息，进一步转化以捕捉数据中更复杂的关系。\n\n然而，FFNN的规模会迅速增长。为了学习这些复杂关系，它通常会扩展接收到的输入：\n\n<img alt=\"94053b88e26e4aa753e8d53c5e3a2524.png\" src=\"https://img-blog.csdnimg.cn/img_convert/94053b88e26e4aa753e8d53c5e3a2524.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### 稀疏层\n\n传统Transformer中的FFNN被称为密集模型，因为所有参数（权重和偏置）都会被激活。没有任何东西被遗漏，所有东西都用于计算输出。\n\n如果我们仔细观察密集模型，会发现输入在一定程度上激活了所有参数：\n\n<img alt=\"bd0dc4d41e937cc4e4b8907330f58681.png\" src=\"https://img-blog.csdnimg.cn/img_convert/bd0dc4d41e937cc4e4b8907330f58681.png\" referrerpolicy=\"no-referrer\">\n\n\n\n相比之下，稀疏模型仅激活其总参数的一部分，与混合专家密切相关。\n\n为了说明这一点，我们可以将密集模型分割成片段（即专家），重新训练，并在给定时间仅激活一部分专家：\n\n<img alt=\"78bdc7a09c44e4149dc106f9c67ed290.png\" src=\"https://img-blog.csdnimg.cn/img_convert/78bdc7a09c44e4149dc106f9c67ed290.png\" referrerpolicy=\"no-referrer\">\n\n\n\n其基本思想是每个专家在训练过程中学习不同的信息。然后，在运行推理时，仅使用与特定任务最相关的专家。\n\n当收到问题时，我们可以选择最适合特定任务的专家：\n\n<img alt=\"16620f73d124bfa8af81bebc596fe8be.png\" src=\"https://img-blog.csdnimg.cn/img_convert/16620f73d124bfa8af81bebc596fe8be.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### 专家学习的内容\n\n如前所述，专家学习到的信息比整个领域的信息更加精细。因此，称它们为“专家”有时被视为误导。\n\n<img alt=\"81455202e36ad3ad2a9a220269d3aa46.png\" src=\"https://img-blog.csdnimg.cn/img_convert/81455202e36ad3ad2a9a220269d3aa46.png\" referrerpolicy=\"no-referrer\">\n\n\n\n（ST-MoE论文中编码器模型的专家专业化）\n\n然而，解码器模型中的专家似乎并没有同样类型的专业化。但这并不意味着所有专家都是平等的（具有相同的能力）。\n\nMixtral 8x7B论文(https://arxiv.org/pdf/2401.04088) 中有一个很好的例子，其中每个词元都标记了第一个专家的选择。\n\n<img alt=\"cd7ab4f998f152d194347fddbe260926.png\" src=\"https://img-blog.csdnimg.cn/img_convert/cd7ab4f998f152d194347fddbe260926.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这一图片也表明，专家往往专注于句法而非特定领域。\n\n因此，尽管解码器专家似乎没有专业化，但它们似乎在特定类型的词元上使用得相对一致。\n\n#### 专家的架构\n\n尽管将专家可视化为切成块的密集模型的隐藏层很不错，但它们通常是完整的FFNN：\n\n<img alt=\"36b3a8ec615af5e5185cc411214b7c57.png\" src=\"https://img-blog.csdnimg.cn/img_convert/36b3a8ec615af5e5185cc411214b7c57.png\" referrerpolicy=\"no-referrer\">\n\n\n\n由于大多数LLM有多个解码器块，给定的文本在生成之前会经过多个专家：\n\n<img alt=\"0b63c3f186fc9ae2abad81e6c0329dac.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0b63c3f186fc9ae2abad81e6c0329dac.png\" referrerpolicy=\"no-referrer\">\n\n\n\n选择的专家可能因词元而异，从而导致采取不同的“路径”：\n\n<img alt=\"3af5b42089995816937ef2cdc675f2f7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/3af5b42089995816937ef2cdc675f2f7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n如果我们更新解码器块的图解，它现在将包含更多的FFNN（每个专家各一个）：\n\n<img alt=\"0b1cb121797d79d5a403d3912a37a148.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/0b1cb121797d79d5a403d3912a37a148.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n解码器块现在有多个FFNN（每个都是一个“专家”），可以在推理过程中使用。\n\n**3**\n\n### 路由机制\n\n现在我们有了一组专家，那么模型如何知道使用哪些专家呢？\n\n我们可以在专家层之前添加一个路由（也称为门控网络），它是专门训练用来选择针对特定词元的专家。\n\n#### 路由\n\n路由（或门控网络）也是一个前馈神经网络（FFNN），用于根据特定输入选择专家。它可以输出概率，用于选择最匹配的专家：\n\n<img alt=\"3b55be9bfcc4906037e71ce671af2bae.png\" src=\"https://img-blog.csdnimg.cn/img_convert/3b55be9bfcc4906037e71ce671af2bae.png\" referrerpolicy=\"no-referrer\">\n\n\n\n\n 专家层返回所选专家的输出，乘以门控值（选择概率）。\n\n路由与专家（其中只有少数被选择）共同构成**MoE层**：\n\n<img alt=\"ed98c53ab4bea08b33773107e7f04cfc.png\" src=\"https://img-blog.csdnimg.cn/img_convert/ed98c53ab4bea08b33773107e7f04cfc.png\" referrerpolicy=\"no-referrer\">\n\n\n\n给定的MoE层有两种类型：稀疏混合专家或密集混合专家。\n\n两者都使用路由器来选择专家，但稀疏MoE仅选择少数专家，而密集MoE则选择所有专家，但可能在不同的分布中。\n\n<img alt=\"d5904b334261713c1bacd60fd55094ca.png\" src=\"https://img-blog.csdnimg.cn/img_convert/d5904b334261713c1bacd60fd55094ca.png\" referrerpolicy=\"no-referrer\">\n\n\n\n例如，给定一组词元，MoE会将词元分配到所有专家，而稀疏MoE仅选择少数专家。\n\n在当前的LLM状态下，当看到“MoE”时，通常指的是稀疏MoE，因为它允许使用一部分专家。这在计算上更为经济（消耗的资源更少），这是LLM的重要特性。\n\n#### 选择专家\n\n门控网络可以说是任何MoE中最重要的组件，因为它不仅决定推理期间选择哪些专家，还决定训练时的选择。\n\n在最基本的形式中，我们将输入（x）乘以路由权重矩阵（**W**）：\n\n<img alt=\"53bdd80a3dc5eff339c5bc2b116c08f4.png\" src=\"https://img-blog.csdnimg.cn/img_convert/53bdd80a3dc5eff339c5bc2b116c08f4.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然后，我们对输出应用**SoftMax**，创建每个专家的概率分布**G(x)**：\n\n<img alt=\"42f12f2ff365403534b5337366cac966.png\" src=\"https://img-blog.csdnimg.cn/img_convert/42f12f2ff365403534b5337366cac966.png\" referrerpolicy=\"no-referrer\">\n\n\n\n路由使用这个概率分布来选择最匹配的专家。\n\n最后，我们将每个路由的输出与每个选定的专家相乘，并将结果相加。\n\n<img alt=\"f8b5444bec8cf711d59a0e8eed3cd873.png\" src=\"https://img-blog.csdnimg.cn/img_convert/f8b5444bec8cf711d59a0e8eed3cd873.png\" referrerpolicy=\"no-referrer\">\n\n\n\n我们可以将所有内容结合在一起，探索输入如何通过路由和专家流动：\n\n<img alt=\"9e9724df942c417ee6bee295388abbba.png\" src=\"https://img-blog.csdnimg.cn/img_convert/9e9724df942c417ee6bee295388abbba.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### <img alt=\"4ad184d60cc584f8906518875d7b7052.png\" src=\"https://img-blog.csdnimg.cn/img_convert/4ad184d60cc584f8906518875d7b7052.png\" referrerpolicy=\"no-referrer\">\n\n\n \n\n#### 路由的复杂性\n\n然而，这个简单的函数通常导致路由器选择相同的专家，因为某些专家可能学习得比其他专家更快：\n\n<img alt=\"ebe53efa8b467d7230fa423413a94721.png\" src=\"https://img-blog.csdnimg.cn/img_convert/ebe53efa8b467d7230fa423413a94721.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这不仅会导致选择的专家分布不均，而且一些专家几乎无法受到训练。这在训练和推理期间都会产生问题。\n\n相反，我们希望在训练和推理期间让专家之间保持均等的重要性，这称为负载均衡。这样可以防止对同一专家的过度拟合。\n\n### 负载均衡\n\n为平衡专家的重要性，我们需要把关注点放在路由上，因为它是在特定时间决定选用哪些专家的关键组件。\n\nKeepTopK\n\n对路由进行负载均衡的一种方式是借助\"KeepTopK\"（https://arxiv.org/pdf/1701.06538）直接扩展。通过引入可训练的（高斯）噪声，可以避免重复选择相同的专家。\n\n<img alt=\"91bb801dac14d559b7b870e3adc5910a.png\" src=\"https://img-blog.csdnimg.cn/img_convert/91bb801dac14d559b7b870e3adc5910a.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然后，除了想要激活的前k个专家（例2）之外，其余专家的权重将被设置为-∞：\n\n<img alt=\"1c4ac765b549725d35efa137ce340ed1.png\" src=\"https://img-blog.csdnimg.cn/img_convert/1c4ac765b549725d35efa137ce340ed1.png\" referrerpolicy=\"no-referrer\">\n\n\n\n通过将这些权重设置为-∞，这些权重上的SoftMax输出所产生的概率将会是0：\n\n<img alt=\"73976f43eb26df3d97972ff3f4759c4f.png\" src=\"https://img-blog.csdnimg.cn/img_convert/73976f43eb26df3d97972ff3f4759c4f.png\" referrerpolicy=\"no-referrer\">\n\n\n\n尽管许多替代方案都很有前景，但许多语言模型仍然使用KeepTopK策略。请注意，KeepTopK也可以在不添加额外噪声的情况下使用。\n    词元选择    \nKeepTopK策略将每个词元路由到少数选定的专家。这种方法称为词元选择，它允许将给定的词元发送给一个专家（top-1路由）：\n\n<img alt=\"59eb4b4dbe38873e79e1d348047aacaa.png\" src=\"https://img-blog.csdnimg.cn/img_convert/59eb4b4dbe38873e79e1d348047aacaa.png\" referrerpolicy=\"no-referrer\">\n\n\n\n或者发送给多个专家（top-k路由）：\n\n<img alt=\"f239c79f916f298484e511b12bc2e516.png\" src=\"https://img-blog.csdnimg.cn/img_convert/f239c79f916f298484e511b12bc2e516.png\" referrerpolicy=\"no-referrer\">\n\n\n\n一个主要的好处是它允许权衡和整合专家各自的贡献。\n    辅助损失    \n为了在训练期间使专家的分布更加均匀，辅助损失（也称为负载均衡损失）被添加到网络的常规损失中。\n\n它增加了一个约束条件，迫使专家具有同等的重要性。\n\n这个辅助损失的第一个组成部分是对整个批次中每个专家的路由值进行求和：\n\n<img alt=\"70c3ac634838a2d24cf4ddc61c6d932a.png\" src=\"https://img-blog.csdnimg.cn/img_convert/70c3ac634838a2d24cf4ddc61c6d932a.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这为我们提供了每个专家的重要性得分，它代表了在任何输入下，给定专家被选中的可能性。\n\n我们可以用这个来计算变异系数（CV），它告诉我们专家之间的重要性得分有多大差异。\n\n<img alt=\"92d5852f54919f67050ea4160775651e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/92d5852f54919f67050ea4160775651e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n例如，如果重要性得分有很大差异，变异系数就会很高：\n\n<img alt=\"28945d977fb63b696ab38c04f246bff5.png\" src=\"https://img-blog.csdnimg.cn/img_convert/28945d977fb63b696ab38c04f246bff5.png\" referrerpolicy=\"no-referrer\">\n\n\n\n相反，如果所有专家的重要性得分相似，变异系数就会很低（这是我们的目标）：\n\n<img alt=\"8c8c8221b5b2109570178dfcf661a112.png\" src=\"https://img-blog.csdnimg.cn/img_convert/8c8c8221b5b2109570178dfcf661a112.png\" referrerpolicy=\"no-referrer\">\n\n\n\n利用这个变异系数得分，我们可以在训练期间更新辅助损失，使其目标是尽可能降低变异系数得分（从而给予每个专家同等的重要性）：\n\n<img alt=\"6588aa93d06706853836998ec261c85d.png\" src=\"https://img-blog.csdnimg.cn/img_convert/6588aa93d06706853836998ec261c85d.png\" referrerpolicy=\"no-referrer\">\n\n\n\n最后，辅助损失被单独添加进来，作为一个独立的损失项在训练期间进行优化。\n\n#### 专家容量\n\n不平衡现象不仅存在于被选中的专家中，还存在于发送给专家的词元分布中。\n\n例如，如果输入的词元在分配给不同专家时比例失调，过多地发送给一个专家而较少地发送给另一个专家，那么可能会出现训练不足的问题。\n\n<img alt=\"5cfed29cab0ecb19f4981c116e0965f6.png\" src=\"https://img-blog.csdnimg.cn/img_convert/5cfed29cab0ecb19f4981c116e0965f6.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这里，问题不仅仅在于使用了哪些专家，还在于对它们的使用程度。\n\n这个问题的一个解决方案是限制给定专家可以处理的词元数量，即专家容量。当一位专家达到其容量时，后续的词元将被发送给下一位专家：\n\n<img alt=\"3e3e75724d22a3a2ff05d36649046bbe.png\" src=\"https://img-blog.csdnimg.cn/img_convert/3e3e75724d22a3a2ff05d36649046bbe.png\" referrerpolicy=\"no-referrer\">\n\n\n\n如果两位专家都达到了他们的容量，那么该词元将不会被任何专家处理，而是被发送到下一层。这被称为词元溢出（token overflow）。\n\n<img alt=\"00b871c70c20c0d6449330b0f4456f34.png\" src=\"https://img-blog.csdnimg.cn/img_convert/00b871c70c20c0d6449330b0f4456f34.png\" referrerpolicy=\"no-referrer\">\n\n\n\n#### 借助Switch Transformer简化MoE\n\n首批解决了基于Transformer的MoE（例如负载均衡等）训练不稳定性问题的模型之一是Switch Transformer。它极大地简化了架构和训练过程，同时提高了训练的稳定性。\n    切换层    \nSwitch Transformer是一个T5模型（编码器-解码器），它用切换层取代了传统的前馈神经网络层。切换层是一个稀疏的MoE层，它为每个词元选择一个专家（Top-1路由）。\n\n<img alt=\"93f5a9972a20669974abdd8e32ebafd7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/93f5a9972a20669974abdd8e32ebafd7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n路由在计算选择哪个专家时没有特殊技巧，它只是对输入乘以专家权重后的结果取Softmax（与我们之前所做的相同）。\n\n<img alt=\"0d3a57f21baf3a4ea6dfca3ed9514a6e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0d3a57f21baf3a4ea6dfca3ed9514a6e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这种架构（Top-1 路由）假定路由只需要一个专家就能学会如何对输入进行路由。这与我们之前看到的情况形成对比，之前我们假设词元应该被路由到多个专家（Top-k 路由）以学习路由行为。\n    容量因子    \n容量因子是一个重要的值，因为它决定了一个专家能够处理多少个词元。Switch Transformer在此基础上进行了扩展，直接引入了一个容量因子，它对专家容量产生直接影响。\n\n<img alt=\"0ace8f6781c3d2b20f598875a3b3fd0e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0ace8f6781c3d2b20f598875a3b3fd0e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n专家容量的组成部分很直接：\n\n<img alt=\"323cd5c60b2ab176b4bf0c13a9e33432.png\" src=\"https://img-blog.csdnimg.cn/img_convert/323cd5c60b2ab176b4bf0c13a9e33432.png\" referrerpolicy=\"no-referrer\">\n\n\n\n如果我们增加容量因子，每个专家将能够处理更多的词元。\n\n<img alt=\"4a1cb59e0ffb11054d48922a803fc5a4.png\" src=\"https://img-blog.csdnimg.cn/img_convert/4a1cb59e0ffb11054d48922a803fc5a4.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然而，如果容量因子太大，我们会浪费计算资源。相反，如果容量因子太小，由于词元溢出，模型性能将会下降。\n    辅助损失    \n为了进一步防止丢弃词元，引入了一个简化版的辅助损失。\n\n这个简化后的损失并非去计算变异系数，而是依据每个专家的路由概率所占的比例，来对分配给各个专家的词元的比例进行权衡。\n\n<img alt=\"ab6bff0473c5e1292d01c79899119734.png\" src=\"https://img-blog.csdnimg.cn/img_convert/ab6bff0473c5e1292d01c79899119734.png\" referrerpolicy=\"no-referrer\">\n\n\n\n由于目标是在N个专家之间实现词元的均匀路由，我们希望向量P和f的值为 1/N。\n\nα是一个超参数，我们可以在训练期间使用它来微调这个损失的重要性。过高的值将主导主要的损失函数，而过低的值对负载均衡的作用很小。\n\n**4**\n\n### 视觉模型中的混合专家\n\n混合专家（MoE）技术并非仅适用于语言模型。视觉模型（例如视觉transformerViT）利用基于transformer的架构，因此也有使用混合专家的潜力。\n\nViT（视觉transformer）是一种将图像分割成小块（patch）的架构，这些小块的处理方式与词元类似。\n\n<img alt=\"7fcb67b6de9749779b871f24a48aa965.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/7fcb67b6de9749779b871f24a48aa965.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n这些小块（或词元）随后被映射到嵌入中（带有额外的位置嵌入），然后再输入到常规编码器中：\n\n<img alt=\"74962afb106b9d9626bd0ee842562ddc.png\" src=\"https://img-blog.csdnimg.cn/img_convert/74962afb106b9d9626bd0ee842562ddc.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这些小块一旦进入编码器，就会像词元一样被处理，这使得这种架构非常适合用于混合专家。\n\n#### 视觉混合专家（Vision-MoE）\n\n视觉混合专家（V-MoE）是在图像模型中最早实现混合专家的方法之一。它采用我们之前看到的视觉变换器（ViT），并将编码器中的密集前馈神经网络替换为稀疏混合专家（Sparse MoE）。\n\n<img alt=\"0fe801b55975c52bd2bfeefac3cb4a9d.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0fe801b55975c52bd2bfeefac3cb4a9d.png\" referrerpolicy=\"no-referrer\">\n\n\n\n这使得通常比语言模型规模更小的ViT模型能够通过添加专家而大规模扩展。\n\n由于图像通常有很多小块，所以为每个专家使用了一个预先定义的小专家容量，以减少硬件限制。然而，低容量往往会导致小块被丢弃（类似于词元溢出）。\n\n<img alt=\"3fe37bb2bd6775f23e7c1fab11f78df8.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/3fe37bb2bd6775f23e7c1fab11f78df8.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n为了保持低容量，网络为小块分配重要性得分，并首先处理那些重要的小块，这样溢出的小块通常就不太重要了。这被称为批量优先级路由。\n\n<img alt=\"9b51109b0da0b4cc319c7f0e92cd4a0d.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/9b51109b0da0b4cc319c7f0e92cd4a0d.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n因此，如果词元的百分比降低，我们仍然应该看到重要的小块被路由。\n\n<img alt=\"0e7d74b9321d3fa438753fa094fb35c6.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/0e7d74b9321d3fa438753fa094fb35c6.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n优先级路由通过专注于最重要的小块，使得较少的小块被处理。\n\n#### 从稀疏混合专家到软混合专家\n\n在视觉混合专家（V-MoE）中，优先级评分器有助于区分更重要和不太重要的小块。然而，小块被分配给每个专家，未处理小块中的信息会丢失。\n\n软混合专家（Soft-MoE）旨在通过混合小块将离散的小块（词元）分配转变为软小块（词元）。\n\n在第一步中，我们将输入x（小块嵌入）与一个可学习的矩阵Φ相乘。这为我们提供了路由信息，告诉我们某个词元与给定专家的相关程度。\n\n<img alt=\"533430165adcfcbb7fde1be7670e9aa5.png\" src=\"https://img-blog.csdnimg.cn/img_convert/533430165adcfcbb7fde1be7670e9aa5.png\" referrerpolicy=\"no-referrer\">\n\n\n\n然后，通过对路由信息矩阵（在列上）取Softmax，我们更新每个小块的嵌入。\n\n<img alt=\"38c6f85b90e7a9770eb52e1a0ccbd9c7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/38c6f85b90e7a9770eb52e1a0ccbd9c7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n更新后的小块嵌入本质上是所有小块嵌入的加权平均值。\n\n<img alt=\"9960418f70a86276561a86f592a70c95.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/9960418f70a86276561a86f592a70c95.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n从视觉上看，就好像所有小块都被混合了。然后，这些组合后的小块被发送给每个专家。在生成输出后，它们再次与路由矩阵相乘。\n\n<img alt=\"615dd5c8de9267b10b4328e8b584e863.png\" src=\"https://img-blog.csdnimg.cn/img_convert/615dd5c8de9267b10b4328e8b584e863.png\" referrerpolicy=\"no-referrer\">\n\n\n\n路由矩阵在词元级别影响输入，在专家级别影响输出。\n\n结果，我们得到了被处理的“软”小块/词元，而不是离散的输入。\n\n#### Mixtral 8x7B的活跃参数与稀疏参数\n\n混合专家之所以有趣的很大一部分原因在于其计算需求。由于在给定时间只使用一部分专家，所以我们可以访问比实际使用更多的参数。\n\n虽然给定的混合专家模型有更多的参数要加载（稀疏参数），但由于在推理期间我们只使用一些专家，所以激活的参数较少（活跃参数）。\n\n<img alt=\"037624952b665e1bbf44e1a654d4ea95.png\" src=\"https://img-blog.csdnimg.cn/img_convert/037624952b665e1bbf44e1a654d4ea95.png\" referrerpolicy=\"no-referrer\">\n\n\n\n换句话说，我们仍然需要将整个模型（包括所有专家）加载到你的设备上（稀疏参数），但当我们进行推理时，我们只需要使用一部分（活跃参数）。混合专家模型需要更多的显存（VRAM）来加载所有专家，但在推理期间运行得更快。\n\n让我们以Mixtral 8x7B来探讨稀疏参数与活跃参数的数量。\n\n<img alt=\"b855a0b527927d70272a5bc795805a59.png\" src=\"https://img-blog.csdnimg.cn/img_convert/b855a0b527927d70272a5bc795805a59.png\" referrerpolicy=\"no-referrer\">\n\n\n\n在这里，我们可以看到每个专家的大小是5.6B，而不是7B（尽管有8个专家）。\n\n<img alt=\"f04a3085eef0ec98a0aacfc8f68c9fc7.png\" src=\"https://img-blog.csdnimg.cn/img_convert/f04a3085eef0ec98a0aacfc8f68c9fc7.png\" referrerpolicy=\"no-referrer\">\n\n\n\n我们将不得不加载8×5.6B（46.7B）的参数（以及所有共享参数），但在推理时我们只需要使用2×5.6B（12.8B）的参数。\n\n**5**\n\n### 总结\n\n至此，我们对混合专家的探索之旅就结束了。希望这篇文章能让你更好地理解这一有趣技术的潜力。如今，几乎所有的模型系列中都至少包含一种混合专家的变体，它将会持续存在下去。\n\n其他人都在看\n    [推算LLM训练的GPU内存需求]()        [用初中数学理解LLM工作原理]()        [企业AI调查:AI支出激增6倍，多模型部署盛行]()        [10倍工程师编码工具：Cursor x SiliconCloud]()        [强化学习之父Rich Sutton：AGI研究下一个范式]()        [大模型训练秘方:1000次超参数优化实验的发现]()        [1%预训练成本，实现最高20倍算力扩展效果]()    \n**让超级产品开发者实现“Token自由”**\n \n\n**<em>**[邀请好友体验SiliconCloud]()，****</em>**<em>**狂送2000万Token/人****</em>\n\n****邀请越多，Token奖励越多****\n****[siliconflow.cn/zh-cn/siliconcloud](http://siliconflow.cn/zh-cn/siliconcloud)****\n",
    "labels": [],
    "created_at": "2024-12-20T16:40:27Z"
  },
  {
    "id": 6,
    "title": "释放数字金融向新力奇富科技金融AI大模型成果落地上海金融科技国际论坛特展周活动",
    "url": "https://blog.csdn.net/csdnnews/article/details/144600708",
    "content": "\n12 月 19 日，在第六届上海金融科技国际论坛之“2024年度人工智能大模型金融领域示范场景及创新应用案例成果特展”活动周上，由上海金融科技产业联盟创新监管联合实验室指导，奇富科技主办的“攻坚金融科技创新与大模型应用、释放数字金融向新力” 主题成果展在沪举办，全面展示了奇富科技在金融科技领域的创新实践。同时，上海金融科技产业联盟创新监管实验室与奇富科技现场达成创新伙伴合作，双方将共同探索金融科技创新的路径，推动新技术在金融领域高效、安全的应用，助力上海全球金融科技中心建设。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/c0eba35f052d4581b55bfc49e6e27826.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n奇富科技CEO吴海生在特展现场强调，科技是驱动金融创新的关键，能打破传统边界，以智能化、个性化方案满足多元化金融需求。他表示，奇富科技将深化研发、加强与金融机构合作，紧抓这一千载难逢的变革机遇，积极探索金融科技无限可能。金融行业天然就适合大模型落地，奇富科技将勇于尝试，利用大模型无穷尽的潜力，为金融行业带来实实在在的提效。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/77df226e98da4f4cb3cd6fa922cb0019.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n 上海金融科技产业联盟副秘书长雎晓燕介绍，上海金融科技产业联盟自2020年成立以来，始终以“开放交流、前沿展示、合作共赢、生态创新”为宗旨，积极搭建具有国际视野的技术合作与产业促进平台，加快推进金融科技产业协同与生态建设，为上海打造全球金融科技中心提供必要支撑。未来，希望奇富科技积极参与到联盟的创新生态建设中来，抢抓数字金融发展的大机遇，为金融科技中心建设凝聚新力量、贡献新动能。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/3c77a3fc6d3b4813a3ce594bb00194bf.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n奇富科技技术委员会主席宋荣鑫阐述了公司从安全根基出发，技术深入落地场景，成为提升金融科技服务品质核心动能的技术理念。在模型层面，通过构建企业级知识库和AI Flow，AI STUDIO使得智能体开发最快可一天上线，实现矩阵式覆盖创新、智能体重塑业务流程。其中，金融专家大模型JARVIS AI生成代码占比高达30%，AI生成测试案例40%；奇富ChatBI通过NL2SQL、语义建模等能力构建金融科技BI新范式。奇富科技超百亿投入沉淀的技术能力，通过科技无损冷链模式，与合作银行实现小时级端到端能力传输，在包含风控、获客、信贷业务、增长等平台能力的FocusPRO 数字普惠信贷系统加持下共建业务生态。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/31a8ddcfc4f949cd9452397b6d6deb81.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n奇富科技首席算法科学家费浩峻深入剖析了金融大模型产品矩阵落地的三要素：一是深耕场景，以坐席提效助手奇富Copilot为例，通过精准定位业务场景，深度分析上亿通历史通话记录，提炼出高效沟通策略与话术模板，助力坐席更精准迅速地捕捉用户意图；二是数据飞轮，以大模型在小微金融中构建的关系图谱为例，通过持续迭代的数据反馈机制，确保模型性能形成良性循环；三是多智能体协作，奇富营销智能体目前可调用多个智能体协作，实现了营销从任务优化升级到自主智能解决问题。三要素相辅相成，共同构成了奇富科技在大模型领域的核心竞争力。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/baf0097b003045daa544e2e7d8691928.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n 活动中，奇富科技发布AI产品小奇，这一基于大模型技术的 AI 伴侣可全天候服务，深度理解并预测用户的金融需求。小奇基于奇富科技8年累计的海量金融交易数据、行为日志及客服对话语料进行训练。据奇富科技产品副总裁卢瑶介绍，小奇具备全程智能无人工、多轮交互及语控功能，可提升用户额度申请与营销效率，降低进线率。\n\n<img src=\"https://i-blog.csdnimg.cn/direct/8474248c7c5d415ab6ab3d411b52cd24.png#pic_center\" alt=\"在这里插入图片描述\" referrerpolicy=\"no-referrer\">\n\n\n\n作为奇富科技技术输出的核心平台，奇富数科在此次特展上正式发布了数字普惠信贷解决方案FocusPRO 2.0，旨在破解小微信贷领域的风险、成本与规模难题。奇富数科总裁周旭强表示，数字化、智能化技术重构成本结构并管控风险是打破困境的关键。FocusPRO 2.0不仅优化了消费贷服务，还拓展至小微经营贷，满足了市场更多元、更精细的金融需求。今年1月至9月，在FocusPRO的助力下，奇富数科放款月均复合增长率达到14%，业务增长势头强劲。\n\n中国社科院国家金融与发展实验室副主任杨涛表示，央行等七部门联合印发《推动数字金融高质量发展行动方案》，既提到“强化数字技术支撑能力”以助力金融业数字化转型，也突出要“运用数字技术提升重点领域金融服务质效”。我们看到，伴随新技术的快速演进与迭代，金融业作为特殊的信息处理行业，其核心要素、基本功能、运行模式、风险管理等都受到巨大冲击与影响。在保障合规与安全的前提下，更广泛地把前沿技术应用于金融服务创新，已成为金融高质量服务实体经济的重要“抓手”。尤其是生成式AI大模型的金融应用逐渐深入，虽然还存在许多挑战，但可以预见将对提升金融业的自动化和智能化水平，以及决策、管理和风控效率都带来积极效应。\n\n中关村互联网金融研究院首席研究员董希淼认为：日前，中国人民银行等七部门联合印发《推动数字金融高质量发展行动方案》，提出数字金融发展总体要求和具体措施，为数字金融发展明确了方向、目标，将加快金融机构数字化转型，促进数字金融健康发展。金融机构、科技公司如果能够深度运用以大模型所代表的生成式人工智能（GAI），大力推进数字化转型，持续创新产品服务，不断提升用户体验，就可能走出一条差异化、特色化的发展之路。从这个意义上讲，大模型不但将成为金融业的数字化劳动力，还将促进金融机构数字化转型，推动数字金融深度发展。\n\n奇富科技表示，AI大模型在金融行业中的价值已经从降本增效逐步升级到高效提升业务核心收益，这将带动全行业颠覆式产品服务模式创新，逐步实现为用户提供无缝嵌入生活的高度个性化金融服务。今年9月，上海市发布《上海高质量推进全球金融科技中心建设行动方案》，作为设立在上海的金融科技企业，奇富科技将持续加大在AI大模型技术上的研发与投入，利用其在数据处理、自然语言理解、决策优化等方面的强大能力，深入挖掘AI技术在金融业务的潜在价值，推动国产金融大模型技术迭代升级，为上海金融科技发展贡献力量，助力中国数字金融的高质量发展。\n",
    "labels": [],
    "created_at": "2024-12-20T16:40:25Z"
  },
  {
    "id": 7,
    "title": "AI半导体技术、市场与未来",
    "url": "https://blog.csdn.net/OneFlow_Official/article/details/144279431",
    "content": "\n<img alt=\"0adce903ec321617a1e752ee1d67a11f.png\" src=\"https://img-blog.csdnimg.cn/img_convert/0adce903ec321617a1e752ee1d67a11f.png\" referrerpolicy=\"no-referrer\">\n\n\n\n过去两年，英伟达崛起是科技领域的一个经典案例。通过CUDA系统，他们创建了一个使用GPU进行机器学习的开发者生态系统；通过Mellanox，他们成为了数据中心网络的领导者。然后，他们将所有硬件集成到服务器中，提供垂直集成的算力一体机。\n\n 凭借这一系列组合性技术优势，英伟达在“AI淘金热”中提供的铲子占据行业核心地位，这导致它成为有史以来最成功的公司之一。随之而来的是，不少挑战者入局以求从英伟达主导的市场分一杯羹。半导体行业的竞争愈加热烈。\n\n 在此背景下，AI半导体研究专家Austin Lyons与Eric Flaningam从AI与GPU行业的背景知识切入，结合当前AI半导体生态系统，通过行业关键数据，对未来发展趋势进行了深入分析。\n\n（本文由OneFlow编译发布，转载请联系授权。原文：https://www.generativevalue.com/p/the-ai-semiconductor-landscape）\n\n**来源 |****Eric Flaningam、****Austin Lyons**\n\n**翻译｜<strong>张雪聃、林心宇**</strong>\n\n**OneFlow编译**\n\n**题图由****[SiliconCloud]()平台生成**\n\n**1**\n\n### AI加速器的背景知识\n\n从一个非常宏观的角度看，所有逻辑半导体都包含以下组成部分：\n\n1. 计算核心——执行实际的计算操作。\n\n2. 存储器——存储要传递给计算核心的数据。\n\n3. 缓存——临时存储可快速检索的数据。\n\n4. 控制单元——控制并管理其他组件的操作顺序。\n\n传统情况下，CPU是一种通用计算机，设计用于执行任何计算任务，包括复杂的多步流程。如下图所示，CPU具有更多的缓存、更强大的控制单元以及更小的计算核心（即CPU中的算术逻辑单元，ALU）。\n\n<img alt=\"fa75ae40cda07669b24796dfe1d94b38.png\" src=\"https://img-blog.csdnimg.cn/img_convert/fa75ae40cda07669b24796dfe1d94b38.png\" referrerpolicy=\"no-referrer\">\n\n\n\n另一方面，GPU专为处理大量小型计算任务或并行计算而设计。最初，GPU用于图形处理，需要同时进行大量的小型计算以生成显示内容。这种基础架构非常适合AI的工作负载。英伟达率先通过早期的GPU引入可编程着色器，并推出CUDA，使所有GPU都能成为可编程计算机。\n\n**为何GPU如此适合AI？**\n\n大多数AI模型的基本单元是神经网络，其结构由多层节点组成。这些节点通过加权处理，尽可能准确地表示训练数据的特性。\n\n当模型完成训练后，可以输入新的数据，模型则会预测输出结果（即推理）。\n\n这种“数据传递”涉及大量的小型计算，主要以矩阵乘法的形式实现：（某一层的节点与权重）×（另一层的节点与权重）。\n\n矩阵乘法是GPU的强项，因其具备出色的并行处理能力。\n\n<img alt=\"53ffab4623ec1f02a3bb9251a5213351.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/53ffab4623ec1f02a3bb9251a5213351.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n（Stephen Wolfram撰写了一篇详细解析ChatGPT工作原理的文章：https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/）\n\n#### 当今的GPU\n\nGPU的规模不断扩大，计算能力和内存也在持续增强，并且更加专注于适配矩阵乘法等工作负载。\n\n以英伟达的H100为例。它由CUDA和Tensor核心（基本处理器）、处理集群（由多个核心组成）以及高带宽内存组成。H100的设计目标是以尽可能高的数据流量处理尽可能多的计算。\n\n<img alt=\"71dd223c116502541895cde1866f9711.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/71dd223c116502541895cde1866f9711.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n提升性能的目标不仅限于芯片本身，而是聚焦于整个系统的优化。在芯片之外，通过连接GPU构建计算集群（https://www.nvidia.com/en-us/data-center/dgx-superpod/），将服务器设计为一体化的计算设备(https://www.nvidia.com/en-gb/data-center/dgx-h100/)，甚至从系统层面优化数据中心的设计(https://www.fabricatedknowledge.com/p/the-data-center-is-the-new-compute)。\n \n\n**1**\n\n### 训练与推理的背景知识\n\n要理解AI半导体的格局，需要先回顾一下AI架构的基础知识。\n\n训练指的是通过大规模数据集进行迭代，创建能够表示复杂场景的模型；而推理则是向该模型提供新的数据以生成预测。\n\n<img alt=\"f3964799113fd269107fcf1c47bd0182.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/f3964799113fd269107fcf1c47bd0182.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n以下是推理的几个关键特性：\n\n1. **时延和位置至关重要**——推理为终端用户运行工作负载，因此响应速度至关重要。这使得在边缘设备或边缘云环境中进行推理更具优势，而训练则可以在任何位置完成。\n\n**2. 可靠性稍显次要**——训练最前沿的模型可能需要数月时间，并需要庞大的训练集群。由于集群内部高度依赖，一个环节出错可能拖慢整个训练进程。而推理的工作负载更小且相互独立，即使发生错误，仅会影响单个请求，并能快速重新运行。\n\n**3. 硬件扩展性的重要性较低**——英伟达的核心优势之一是其通过软件和网络实现大规模系统扩展的能力。然而在推理中，这种扩展性的重要性较低。\n\n以上原因解释了为什么许多新兴半导体公司将重心放在推理上。与训练相比，推理的进入门槛更低。\n\n英伟达的网络和软件能够支持其构建更大、更高性能和更可靠的训练集群。英伟达在AI训练方面的竞争壁垒非常牢固，其他竞争者难以企及。\n\n接下来，我们来谈谈竞争格局。\n\n**3**\n\n### AI半导体行业概览\n\nAI半导体行业可以大致分为三个主要领域：\n    数据中心训练芯片        数据中心推理芯片        边缘推理芯片    \n下图是相关公司展示：\n\n<img alt=\"cafbd2b843418757819e4df5190e033e.png\" src=\"https://img-blog.csdnimg.cn/img_convert/cafbd2b843418757819e4df5190e033e.png\" referrerpolicy=\"no-referrer\">\n\n\n\n（AI半导体价值链的思维模型图；该图并未展示所有公司和细分领域）\n\n#### 数据中心半导体市场\n\n简单来说，英伟达主导了数据中心半导体市场，而AMD是唯一具有竞争力的通用替代方案。超大规模云公司（Hyperscalers）倾向于开发自研芯片，而大部分初创公司专注于推理或特定架构的专用硬件。\n\n预计英伟达将在2024年销售超过1000亿美元的AI系统，而AMD紧随其后，预计实现50亿美元的收入。\n\n以下是截至2023年底，数据中心处理器市场份额的分布：\n\n<img alt=\"d550ff283d396d39793b3ebe1ba55512.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/d550ff283d396d39793b3ebe1ba55512.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n（数据来源：https://www.crn.com/news/components-peripherals/2024/google-was-third-biggest-data-center-processor-supplier-last-year-research）\n\n在超大规模云公司中，Google提供了最先进的加速器，其TPU备受关注。据TechInsights估计，Google去年出货了200万个TPU，仅次于英伟达的AI加速器。\n\n亚马逊则开发了自研神经网络芯片（Nitro）、CPU（Graviton）、推理芯片（Inferentia）和训练芯片（Trainium）。TechInsights估计，亚马逊在2023年向客户“出租”了230万个此类芯片。\n\n微软最近推出了CPU（Cobalt）和GPU（Maia），但由于产品较新，目前尚无法评估其市场表现。\n\n最后值得一提的是，英特尔原本预计今年销售约5亿美元的Gaudi 3芯片，但在最近的财报中表示这一目标无法实现。\n\n尽管英伟达凭借其软件和网络功能在训练领域占据主导地位，但推理领域由于架构差异，竞争格局更加多样化。\n\n#### 推理领域：更具吸引力的市场！\n\nRunPod最近对Nvidia H100和AMD MI300X进行了有趣的对比研究，结果表明，MI300X在超大批处理和极小批处理的推理任务中更具成本优势。\n\n<img alt=\"e789403f7761f24d403432d0b312efdf.png\" src=\"https://img-blog.csdnimg.cn/img_convert/e789403f7761f24d403432d0b312efdf.png\" referrerpolicy=\"no-referrer\">\n\n\n\n（数据来源：https://blog.runpod.io/amd-mi300x-vs-nvidia-h100-sxm-performance-comparison-on-mixtral-8x7b-inference/）\n\n同时，许多硬件初创公司筹集了大量资金，试图抢占这一市场的一席之地：\n\n<img alt=\"6afc719c09ca3040f35e886f3f9db21d.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/6afc719c09ca3040f35e886f3f9db21d.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n（数据来源：https://www.chipstrat.com/p/etched-silicon-valleys-speedrun）\n\n一个有趣的趋势是，这些初创公司正向软件领域扩展。例如，[**Groq**]()、[**Cerebras**]()和SambaNova三家领先的初创公司都在提供推理软件服务。理论上，这种垂直整合能够为终端用户提供成本和性能上的双重优势。\n\nAI半导体市场的最后一块拼图是边缘AI，这也是业内广泛关注的热点话题。\n\n### **4**\n 边缘AI？\n\n训练最庞大且功能最强的AI模型需要高昂的成本，甚至可能需要整个数据中心的GPU资源支持。然而，模型训练完成后，可以运行在性能相对较低的硬件上。实际上，AI模型甚至可以在智能手机或笔记本电脑等“边缘”设备上运行。边缘设备通常由SoC（系统级芯片）提供支持，这种芯片包含CPU、GPU、内存，通常还集成了NPU（神经网络处理单元）。\n\n<img alt=\"db71451b5af5eba87dd9219e32c98c41.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/db71451b5af5eba87dd9219e32c98c41.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n例如，一个运行在智能手机上的AI模型必须足够小，以适应手机内存的限制。因此，与大型云端模型相比，边缘模型通常更小、更简单。然而，模型在本地运行能够安全地访问用户特定数据（如位置、短信），而无需将数据传输到云端。\n\n尽管AI模型可以技术上运行在手机的CPU上，但矩阵乘法更适合由GPU这样支持并行处理的硬件来完成。由于CPU优化用于顺序处理，在推理任务中可能表现较慢，即使是处理小型模型也是如此。作为一种替代方案，AI模型可以运行在手机的GPU上。\n\n然而，智能手机的GPU主要为图形处理而设计。在玩游戏时，用户期望GPU能全力支持平滑的画面效果。如果同时分配GPU资源给AI模型，可能会显著影响游戏体验。\n\n因此，NPU成为边缘设备AI推理的理想选择。智能手机可以利用NPU来运行AI任务，而不会增加GPU或CPU的负担。由于电池寿命对边缘设备至关重要，NPU经过优化以降低功耗。与GPU相比，NPU在执行AI任务时的功耗可能低至其1/5到1/10，大大延长了设备的电池续航时间。\n\n边缘推理不仅应用于消费类设备，还广泛用于工业和汽车领域。例如，自动驾驶汽车依靠边缘推理实时处理传感器数据，从而做出快速决策以确保行车安全。而在工业物联网中，传感器和摄像头数据的本地推理可以支持主动维护等措施。\n\n工业和汽车领域的设备通常具备比消费类设备更高的功率支持，因此可以部署高性能计算平台。例如，Nvidia的Orin平台就内置了与数据中心GPU类似的功能（详见https://www.nvidia.com/en-us/edge-computing/products/igx/）。需要远程硬件可编程性的场景，则可以使用例如Altera的FPGA（详见：https://www.intel.com/content/www/us/en/content-details/765466/altera-fpgas-and-socs-with-fpga-ai-suite-and-openvino-toolkit-drive-embedded-edge-ai-machine-learning-applications-white-paper.html)。\n\n**5**\n\n### 对市场的一些思考\n\n最后，我想分享几个我认为在AI半导体领域中最有趣的问题：\n\n**1. 英伟达的优势壁垒有多深厚？**\n\n多年来，英伟达在数据中心GPU市场上维持了90%以上的市场份额。作为一家极具前瞻性的公司，其在过去二十年内做出了正确的技术和战略决策。在这个领域中，经常有人问我有关英伟达的壁垒的问题。\n\n对此，我有两个看似矛盾的观点：首先，我认为英伟达在不断扩展自己的优势，从AI软件、基础设施到模型和云服务都在积极布局。他们还在投资网络和垂直整合。这家公司持续高效执行其战略，令人惊叹。\n\n其次，在收入上，每家公司都想与英伟达分一杯羹。如今，全球企业正投入数千亿美元争相开发通用人工智能（AGI）。英伟达最大的客户们正在斥资数十亿美元减少对其依赖，同时投资者也向竞争对手注入巨额资金，希望能瓜分英伟达的市场份额。\n\n总结来说：英伟达是目前全球AI领域最有优势的公司，同时也面临来自竞争者、客户和投资者的数百亿美元投入的挑战。\n\n**2. 初创公司的机会在哪里？**\n\n半导体初创公司面临着极其艰难的挑战，要建立可持续的商业模式难度很大。而当竞争对手是英伟达时，这种挑战更显艰巨。不过，市场总是在通用性与专业性之间进行权衡。如果公司能够有效地专注于足够大的细分市场，就有机会成长为一家规模庞大的企业。这包括专为推理设计的硬件和特定模型优化的硬件。\n\n我尤其对能够加速专用芯片开发的方案感兴趣。这类方法能够降低芯片开发的门槛，同时利用专业化带来性能优势。\n\n然而，半导体行业极其复杂，这类产品需要经过时间和多代技术的积累才能成熟。这些公司需要持续的资金支持，才能完成多个产品代际的研发。\n\n**3. 边缘AI会成为现实吗？**\n\n从历史来看，技术的颠覆往往发生在一种新产品以更低价格提供了较少的功能时，而现有企业难以与之竞争。例如，大型主机让位于小型计算机，小型计算机又被PC取代，PC之后是智能手机。\n\n这些颠覆的关键变量在于性能的“过剩供应”。顶级解决方案常常解决了大多数人非必需的问题。许多计算领域的颠覆来自于计算能力的去中心化，因为消费者并不需要额外的性能。\n\n在AI领域，我认为这种“性能过剩”还没有出现。ChatGPT很强大，但还称不上完美。一旦它变得足够优秀，边缘AI的时代才会到来。小型语言模型和NPU将推动这一时代的到来。届时，问题将不再是“AI是否会进入边缘设备”，而是“什么时候进入”。\n\n其他人都在看\n    [生成式AI推理技术、市场与未来]()        [比GPU快20倍？d-Matrix推理性价比分析]()        [Tenstorrent虫洞分析：挑战英伟达的新玩家]()        [Cerebras：全球最快AI推理芯片的“魔法”]()        [Groq：从头设计一个张量流式处理器架构]()        [LLM逻辑推演策略：推理时计算vs训练时计算]()        [1%预训练成本，实现最高20倍算力扩展效果]()    \n**让超级产品开发者实现“Token自由”**\n \n\n**<em>**[邀好友用SiliconCloud]()，****</em>**<em>**狂送2000万Token/人****</em>\n\n****[<em><strong>即刻体验****QwQ-32B-Preview]()</strong></em>\n****[siliconflow.cn/zh-cn/siliconcloud](http://siliconflow.cn/zh-cn/siliconcloud)****\n",
    "labels": [],
    "created_at": "2024-12-20T16:40:23Z"
  },
  {
    "id": 3,
    "title": "拦截烂SQL，解读GaussDB(DWS)查询过滤器过滤规则原理",
    "url": "https://www.cnblogs.com/huaweiyun/p/18619491",
    "content": "\n本文分享自华为云社区[《GaussDB(DWS)查询过滤器过滤规则原理与使用介绍》](https://bbs.huaweicloud.com/blogs/441767?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)，作者： 清道夫。\n\n# 1. 前言\n\n**适用版本：【9.1.0.100（及以上）】**\n\n查询过滤器在9.1.0.100之前就具备提供查询过滤功能的能力，但仅支持自动隔离反复查询被终止的查询，防止烂SQL再次执行。\n\n老版本主要面向异常熔断机制和紧急拦截场景，前者可以与异常规则联动，自动将触发异常规则的语句添加到黑名单中，后者是需要手动找到core或者引发hang的语句进行屏蔽。\n\n\n\n大家有兴趣可以翻一下之前的这篇文章[GaussDB(DWS)查询过滤器原理与应用](https://bbs.huaweicloud.com/blogs/401188)。\n\n9.1.0.100及9.1.0.200版本对查询过滤器做了功能的改进，可以通过多维度进行烂SQL识别，功能更丰富，配置更灵活。\n\n# 2. 原理介绍\n\n在原理介绍之前，先举个简单的例子。在业务开发过程中，要想禁止对2张以上的表进行关联查询，此时可以使用DDL语句创建过滤规则：\n\n```\nCREATE BLOCK RULE forbid_2_t_sel FOR SELECT FILTER BY  SQL('test_block_rule') with(table_num='2');\n```\n\ntable_num指的是一个语句中出现的表的个数，此时所有查询语句不能包含有两张表以上的查询。\n\n```\n--两张表直接关联查询，可以正常执行\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\n c1 | c2 | c1 | c2\n----+----+----+----\n(0 rows)\n\n--三张表直接关联查询，被拦截\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2 join test_block_rule3 t3 on t2.c1=t3.c1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 2(3))\n```\n\n说到这，整体逻辑就非常清楚了。用户可以提前识别烂SQL的特征，然后抽象出来，用DDL语句创建规则，后续会对查询的语句进行过滤，被规则筛选出来的便是烂SQL，执行前会报错，反之则可以正常执行。\n\n查询过滤器框架及功能原理概况：\n\n<img src=\"https://img2024.cnblogs.com/blog/2030258/202412/2030258-20241220160540342-411141587.png\" alt=\"\" referrerpolicy=\"no-referrer\">\n\n\n\n从图中可以看出，之前的查询过滤器的功能依然存在，可以保证与异常规则的联动，新版本的增强更注重规则的灵活性和功能的丰富性。\n\n# 3. 使用介绍\n\n## 3.1 查询过滤规则元数据管理\n\n查询过滤规则，可以通过DDL进行新增、删除或者修改，其语法如下：\n\n### （1）创建\n\n```\nCREATE BLOCK RULE [ IF NOT EXISTS ] block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE ] |\n    FILTER BY\n    { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) }\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n其中，\n\n   \n**block_name：**过滤规则的名称\n   \n   \n**user_name：**规则应用的对象用户\n   \n   \n**host：**是规则应用的客户端IP\n   \n   \n**FOR：**语句类型，支持对UPDATE/SELECT/INSERT/DELETE/MEGE INTO五种类型语句进行限制\n   \n   \n**FILTER BY：**过滤方法，包含两种形式\n   \n   \n**SQL：**根据关键词对语句进行正则匹配，例如表名，其长度不能超过1024，建议尽量精简\n   \n   \n**TEMPLATE：**\n   \n   \n**unique_sql_id：**归一化的64位哈希值，重复概率较sql_hash大一些\n   \n   \n**sql_hash：**归一化的哈希值（md5），一般不会重复，相较unique_sql_id更推荐使用\n   \n   \n**with_parameter：**查询过滤规则选项参数，可以附加多个条件，满足其一便会匹配过滤。\n   \n   \n**application_name：**客户端名称\n   \n   \n**query_band：**负载标识\n   \n   \n**table_num：**包含的基表个数\n   \n   \n**partition_num：**扫描分区的数量\n   \n   \n**estimate_row：**基表输出行数\n   \n   \n**resource_pool：**切换的目标资源池，仅适用于9.1.0.200及以上\n   \n   \n**max_active_num：**可并发执行的语句个数，仅适用于9.1.0.200及以上\n   \n   \n**is_warning：**改变拦截行为为告警，而非默认的报错，仅适用于9.1.0.200及以上\n   \n\n其中，user_name和FILTER BY是必选项，其他可以通过业务实际需要进行配置。\n\n### （2）修改\n\n```\nALTER BLOCK RULE block_name RENAME to new_block_name;\n```\n\n通过rename对查询过滤的规则进行重命名。\n\n```\nALTER BLOCK RULE block_name\n    [ [ TO user_name@'host' ] | [ TO user_name ] | [ TO 'host' ] | [ TO DEFAULT ] ] |\n    [ FOR UPDATE | SELECT | INSERT | DELETE | MERGE | DEFAULT ] |\n    [ [ FILTER BY ]\n    [ { SQL ( 'text' ) | TEMPLATE ( template_parameter = value ) } ] ]\n    [ WITH ( { with_parameter = value }, [, ... ] ) ];\n```\n\n所有选项均支持二次修改，如果需要去除部分字段的限制，可以指定default关键词，例如：\n\n```\n--修改为只能查询1张表\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num='1');\nALTER BLOCK RULE\npostgres=# select * from test_block_rule1 t1 join test_block_rule2 t2 on t1.c1=t2.c2;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule, table_num: 1(2))\npostgres=# select * from test_block_rule1 t1;\n c1 | c2\n----+----\n(0 rows)\n\n--去除查询中表个数的限制\npostgres=# ALTER BLOCK RULE forbid_2_t_sel with(table_num=default);\nALTER BLOCK RULE\n--再次查询报错拦截\npostgres=# select * from test_block_rule1 t1;\nERROR:  hit block rule forbid_2_t_sel(block_type: SELECT, regexp_sql: test_block_rule)\n```\n\n### （3）删除\n\n```\nDROP BLOCK RULE [ IF EXISTS ] block_name;\n```\n\n## 3.2 权限问题\n\n对于普通用户来讲是没有创建查询过滤规则权限的，需要管理员或者管理员将权限赋给某一普通用户才可以。\n\n```\n--切换至普通用户\npostgres=# set role jack password 'xxx';\nSET\n--创建查询过滤规则报错提示无权限\npostgres=> create block rule bl2 filter by sql('test');\nERROR:  CREATE/ALTER/DROP BLOCK RULE permission denied for current user\n--重置user\npostgres=> reset role;\nRESET\n--对普通用户进行授权\npostgres=# grant gs_role_block to jack;\nGRANT ROLE\n--切换普通用户\npostgres=# set role jack password 'xxx';\nSET\n--再次创建成功\npostgres=> create block rule bl2 filter by sql('test');\nCREATE BLOCK RULE\n```\n\n建议创建查询过滤规则时尽量缩小适用范围，避免误过滤，或者范围过大导致性能劣化。\n\n## 3.3 备份恢复\n\n对于查询过滤规则的备份或者恢复的权限与操作元数据的权限一致，需要管理员或者管理员讲权限赋值给某一普通用户才可以，用户可以通过gs_dump导出查询过滤规则定义。\n\n如果想查看或者导入查询过滤规则的定义，可以通过pg_get_blockruledef进行查询。\n\n```\npostgres=# select * from pg_get_blockruledef('test');\n                         pg_get_blockruledef\n----------------------------------------------------------------------\n CREATE BLOCK RULE test FILTER BY SQL('test') WITH(estimate_row='3');\n(1 row)\n```\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n所有的查询过滤规则元数据全部保存在pg_blocklists系统表中，可以通过查看系统表浏览所有的查询过滤规则。\n\n## 3.4 使用举例\n\n### （1）使用关键词进行查询过滤\n\n```\nCREATE BLOCK RULE bl1\nTo block_user\nFOR SELECT\nFILTER BY SQL ('tt')\nWITH(partition_num='2',\n     table_num='1',\n     estimate_row='5'\n     );\n\npostgres=> select * from tt;\nERROR:  hit block rule bl1(user_name: block_user, block_type: SELECT, regexp_sql: tt, partition_num: 2(3), table_num: 1(1), estimate_row: 5(1))\n```\n\n从上面的查询可以看出，查询语句包含了tt关键字，并且扫描的分区个数超过了2，此时执行语句被过滤拦截。需要注意的是，**扫描分区的个数并不总是准确的**，仅能识别**静态**的分区剪枝个数，执行过程中的**动态剪枝**并不能被识别。\n\n\n\n**小技巧：**使用关键词过滤时可以先使用正则匹配符~*进行测试，正则匹配是忽略大小写的。\n\n另外，由于查询过滤器的规则直接作用在用户block_user上，因此在删除用户block_user时，会提示有依赖项，此时可以通过在语句最后加上cascade进行删除，此时作用在此用户上的查询过滤规则也会被一同删除。\n\n\n\n受限于篇幅，其他选项就不再一一列举。需要注意的是，过滤规则命中的依据是，with_parameter命中任意一项，且其他字段的特征也符合即会判定为符合查询过滤规则。\n\n\n\n特别注意，不同的计划，可能部分字段无法按照预期进行拦截，例如：\n\n```\npostgres=# create block rule test filter by sql('test')with(estimate_row='3');\nCREATE BLOCK RULE\npostgres=# select * from test;\n c1 | c2\n----+----\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n  1 |  2\n(5 rows)\n```\n\n此时，语句关键字是可以匹配上的，查询的行数也超过了3行的限制，那为什么无法拦截呢？\n\n```\npostgres=# explain verbose select * from test;\n                                          QUERY PLAN\n-----------------------------------------------------------------------------------------------\n  id |                  operation                   | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------------+--------+------------+---------+---------\n   1 | ->  Data Node Scan on \"__REMOTE_FQS_QUERY__\" |      0 |            |       0 | 0.00\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Data Node Scan on \"__REMOTE_FQS_QUERY__\"\n         Output: test.c1, test.c2\n         Node/s: All datanodes (node_group, bucket:16384)\n         Remote query: SELECT c1, c2 FROM public.test\n```\n\n通过计划可以看出，此时是FQS计划，导致没有估算信息。因此此时无法进行拦截，对于CN轻量化的计划也是一样的，如果我们让语句强制走stream计划，那么就可以拦截成功：\n\n```\npostgres=# set enable_stream_operator=on;\nSET\npostgres=# set enable_fast_query_shipping=off;\nSET\npostgres=# select * from test;\nERROR:  hit block rule test(regexp_sql: test, estimate_row: 3(5))\npostgres=#  explain verbose select * from test;\n                                       QUERY PLAN\n-----------------------------------------------------------------------------------------\n  id |               operation                | E-rows | E-distinct | E-width | E-costs\n ----+----------------------------------------+--------+------------+---------+---------\n   1 | ->  Row Adapter                        |      5 |            |       8 | 69.00\n   2 |    ->  Vector Streaming (type: GATHER) |      5 |            |       8 | 69.00\n   3 |       ->  CStore Scan on public.test   |      5 |            |       8 | 59.01\n\n      Targetlist Information (identified by plan id)\n --------------------------------------------------------\n   1 --Row Adapter\n         Output: c1, c2\n   2 --Vector Streaming (type: GATHER)\n         Output: c1, c2\n         Node/s: All datanodes (node_group, bucket:16384)\n   3 --CStore Scan on public.test\n         Output: c1, c2\n         Distribute Key: c1\n```\n\n所以，如果估算信息不准确，也会导致误拦截或者漏拦截的情况，因为计划的信息是通过估算得到的，因此这种情况无法避免。\n\n### （2）使用语句归一化特征值进行查询过滤\n\n语句归一化的特征值，目前有两个，分别是unique_sql_id和sql_hash，两者均是对查询树进行哈希计算之后得出的，区别在于前者是64位哈希值，后者是md5值，因此前者的重复概率会大于后者，在使用时尽量使用sql_hash进行过滤。\n\n\n\n很多小伙伴会问，这两个值如何获取呢？两种方法：\n\n   \n查看explain结果\n   \n\n```\npostgres=> explain verbose select * from tt where a>1;\n                                              QUERY PLAN\n ----------------------------------------------------------------------------------------------------\n   id |                     operation                     | E-rows | E-distinct | E-width | E-costs\n  ----+---------------------------------------------------+--------+------------+---------+---------\n    1 | ->  Row Adapter                                   |      1 |            |       8 | 16.00\n    2 |    ->  Vector Streaming (type: GATHER)            |      1 |            |       8 | 16.00\n    3 |       ->  Vector Partition Iterator               |      1 |            |       8 | 6.00\n    4 |          ->  Partitioned CStore Scan on public.tt |      1 |            |       8 | 6.00\n\n    Predicate Information (identified by plan id)\n  -------------------------------------------------\n    3 --Vector Partition Iterator\n          Iterations: 3\n    4 --Partitioned CStore Scan on public.tt\n          Filter: (tt.a > 1)\n          Pushdown Predicate Filter: (tt.a > 1)\n          Partitions Selected by Static Prune: 1..3\n\n  Targetlist Information (identified by plan id)\n  ----------------------------------------------\n    1 --Row Adapter\n          Output: a, b\n    2 --Vector Streaming (type: GATHER)\n          Output: a, b\n          Node/s: datanode1\n    3 --Vector Partition Iterator\n          Output: a, b\n    4 --Partitioned CStore Scan on public.tt\n          Output: a, b\n\n               ====== Query Summary =====\n  -----------------------------------------------------\n  Parser runtime: 0.029 ms\n  Planner runtime: 0.286 ms\n  Unique SQL Id: 2229243778\n  Unique SQL Hash: sql_aae71adfaa3d91bfe75499d92ad969e8\n (34 rows)\n```\n\n   \n查看topsql记录\n   \n\n```\n queryid                     | 95701492082350773\n query                       | select * from tt where a>10;\n query_plan                  | 1 | Row Adapter  (cost=14.00..14.00 rows=1 width=8)\n                             | 2 |  ->Vector Streaming (type: GATHER)  (cost=0.06..14.00 rows=1 width=8)\n                             | 3 |   ->Vector Partition Iterator  (cost=0.00..4.00 rows=1 width=8)\n                             |   |     Iterations: 2\n                             | 4 |    ->Partitioned CStore Scan on public.tt  (cost=0.00..4.00 rows=1 width=8)\n                             |   |      Filter: (tt.a > 10)\n                             |   |      Pushdown Predicate Filter: (tt.a > 10)\n                             |   |      Partitions Selected by Static Prune: 2..3\n node_group                  | installation\n pid                         | 139803379566936\n lane                        | fast\n unique_sql_id               | 2229243778\n session_id                  | 1732413324.139803379566936.coordinator1\n min_read_bytes              | 0\n max_read_bytes              | 0\n average_read_bytes          | 0\n min_write_bytes             | 0\n max_write_bytes             | 0\n average_write_bytes         | 0\n recv_pkg                    | 2\n send_pkg                    | 2\n recv_bytes                  | 3297\n send_bytes                  | 57\n stmt_type                   | SELECT\n except_info                 |\n unique_plan_id              | 0\n sql_hash                    | sql_aae71adfaa3d91bfe75499d92ad969e8\n```\n\n可以看出两种方法都可以轻松获取这两个语句归一化的特征值，explain可以在事前提前获取，topsql可以在语句执行后进行获取。\n\n\n\n这个时候，可能很多小伙伴又会有疑问，语句中的条件有变化，是否会影响归一化的特征值呢？\n\n答案是不会，因为归一化过程中会去除常量的影响，上述的举例中两个语句条件中的常量值并不相同，但归一化的特征值确实一样的。\n\n### （3）查询过滤的性能\n\n由于语句的过滤，特别是关键词的正则匹配通常是比较耗时的，此时如果有过多的过滤规则，可能导致执行时间的劣化，特别是对于短查询可能影响更为明显。\n\n\n\n**本地实测：**正则匹配关键词长度1024，建立查询过滤规则1000条左右时，对于查询的影响在27.72ms左右，且如果考虑其他匹配项，可能影响会更大，所以，不建议添加太多的查询过滤规则。且业务稳定后可以只对特定开发或者新业务的用户创建查询过滤规则，此时查询过滤规则会优先通过绑定的用户跳过无效的过滤，减少对性能的性能的影响。\n\n### （4）过滤时间查看\n\n可以配置GUC参数analysis_options查看查询过滤规则对正常语句所消耗的时间。\n\n```\nset analysis_options='on(BLOCK_RULE)';\n\n-- explain performance + query\n\n                    User Define Profiling\n-----------------------------------------------------------------\nSegment Id: 3  Track name: Datanode build connection\n      datanode1 (time=0.288 total_calls=1 loops=1)\n      datanode2 (time=0.301 total_calls=1 loops=1)\n      datanode3 (time=0.321 total_calls=1 loops=1)\n      datanode4 (time=0.268 total_calls=1 loops=1)\nSegment Id: 3  Track name: Datanode wait connection\n      datanode1 (time=0.016 total_calls=1 loops=1)\n      datanode2 (time=0.038 total_calls=1 loops=1)\n      datanode3 (time=0.021 total_calls=1 loops=1)\n      datanode4 (time=0.017 total_calls=1 loops=1)\nSegment Id: 1  Track name: block rule check time\n      coordinator1 (time=0.028 total_calls=1 loops=1)\n```\n\n### （5）拦截记录\n\n**[仅适用于****9.1.0.200****及以上]**\n\n创建查询过滤规则后会拦截很多烂SQL，如何看拦截的语句有哪些呢？可以通过topsql进行查看，abort_info会记录拦截信息，也就是查询的报错信息。\n\n```\npostgres=# select abort_info,query from GS_WLM_SESSION_INFO where abort_info like '%hit block rule test%';\n                        abort_info                         |        query\n-----------------------------------------------------------+---------------------\n hit block rule test(regexp_sql: test, estimate_row: 3(5)) | select * from test;\n(1 rows)\n```\n\n# 4. 总结\n\n查询过滤器在9.1.0.100和9.1.0.200版本丰富了大量的功能，提高了烂SQL拦截的灵活性。\n\n管控面后续版本同样可以直接通过前端页面对查询过滤规则进行管理，大家敬请期待。\n\n有任何问题欢迎留言讨论，我们将不断丰富和完善查询过滤功能，让烂SQL无门可入。\n\n\n\n华为开发者空间，汇聚鸿蒙、昇腾、鲲鹏、GaussDB、欧拉等各项根技术的开发资源及工具，致力于为每位开发者提供一台云主机、一套开发工具及云上存储空间，让开发者基于华为根生态创新。[点击链接](https://developer.huaweicloud.com/space/devportal/desktop?utm_source=kfzwzdspace& utm_adplace=nrcbds)，免费领取您的专属云主机\n\n\n\n[**点击关注，第一时间了解华为云新鲜技术~**](https://bbs.huaweicloud.com/blogs?utm_source=cnblog& utm_medium=bbs-ex& utm_campaign=other& utm_content=content)\n",
    "labels": [],
    "created_at": "2024-12-20T16:20:57Z"
  },
  {
    "id": 2,
    "title": "ChatGPT生成接口测试用例（一）",
    "url": "https://www.cnblogs.com/tester2test/p/18619460",
    "content": "\n<img src=\"https://img2024.cnblogs.com/blog/15184/202412/15184-20241220154829587-467838433.jpg\" width=\"300\"  referrerpolicy=\"no-referrer\">\n\n\n\n\n\n　　接口测试在软件开发生命周期中扮演着至关重要的角色，有助于验证不同模块之间的交互是否正确。若协议消息被恶意修改，系统是否能够恰当处理，以确保系统的功能正常运行，不会出现宕机或者安全问题。\n\n## 5.1 ChatGPT在接口测试中的角色\n\n　　接口测试是确保系统各个模块协同工作的关键环节。传统的接口测试用例编写通常依赖于测试人员的经验、手动操作，这可能导致接口测试用例的不充分和测试内容的遗漏。ChatGPT能够理解和生成文本使其成为编写接口测试用例的理想工具。\n\n### 5.1.1 理解系统需求和接口规范\n\n　　ChatGPT可以用来解析系统需求规格说明书和接口文档。通过输入相关文档，ChatGPT可以生成摘要、要点和问题列表等，帮助测试人员更好地把握测试的范围和重点。\n\n　　假设我们正在测试一个电子商务网站的购物车接口。我们有系统需求文档和接口文档，但它们很冗长，需要花费大量的时间阅读、理解。但是有了ChatGPT以后，只需要将这些文档输入ChatGPT，它就可以快速给出摘要，列出所有与购物车相关的功能和要点等。这有助于快速地确定测试重点，确保接口测试用例的全面性。\n\n### 5.1.2 生成测试用例模板\n\n　　ChatGPT可以协助测试团队生成测试用例的模板，包括输入数据、操作步骤和预期结果等。测试人员可以向ChatGPT提供接口描述和测试目标，ChatGPT会自动生成基本的测试用例结构。测试用例模板化以后，既可以加快测试用例的编写速度，又能确保测试用例的一致性。\n\n　　对于一个社交媒体平台的接口测试，我们需要测试用户发布帖子的功能。通过描述接口规范和测试目标，ChatGPT可以自动生成测试用例模板，包括帖子内容、发布操作的步骤和预期的发布结果等。这样，测试人员只需填写具体的内容，而不必从头开始编写每个接口测试用例。\n\n例如，对于一个电子商务网站的购物车接口，ChatGPT生成的接口文档如下所示。\n\n**ChatGPT****生成的接口文档**\n\n> \n**接口名称：**add_to_cart\n**接口概述：**该接口用于将指定商品添加到购物车。\n**请求方式：**POST请求\n**请求****URL****：**https://example.com/api/add_to_cart\n**参数列表：**\n<ul>\n   item_id（商品ID）   \n</ul>\n参数描述：用于指定要添加到购物车的商品的唯一标识。\n参数类型：整数\n是否必填：是\n<ul>\n   num（数量）   \n</ul>\n参数描述：指定要添加的商品数量。\n参数类型：整数\n是否必填：是\n**请求示例：**\nPOST请求https://example.com/api/add_to_cart\n**请求体：**\n{\n\"item_id\": 12345，\n\"num\": 2\n}\n\n**响应示例：**\n{\n\"status\": 200，\n\"message\": \"添加商品到购物车成功\"，\n\"data\": {\n\"cart_total_items\": 5\n}\n}\n\n**响应字段说明：**\nstatus：标识业务是否成功的状态码。\n<ul>\n    200 表示请求成功，商品成功添加到购物车。   \n    其他状态码可根据具体情况自行定义。   \n</ul>\nmessage：状态信息，对状态码的详细描述。\ndata：包含响应数据。\n<ul>\n    cart_total_items：购物车中的总商品数量。   \n</ul>\n\n\n\n\n### 5.1.3 探索边界条件\n\n 在接口测试中，通常需要测试各种输入数据的边界条件和异常情况。ChatGPT可以帮助测试人员生成边界条件的接口测试用例，确保系统能够正确处理各种情况。\n\n在电子商务网站的购物车接口的测试中，我们需要确保它能够正确处理购买的商品数量的输入，包括正常的商品数量输入和异常地输入。异常输入可能包括带小数点的数字、字母、全角数字、中文字符以及货币符号等特殊字符。不同类型的数据库具有不同的数字上下限，例如，在MySQL中，SMALLINT类型占用2个字节，可以存储从-32768到32767的整数。因此，我们需要考虑商品数量超过上限、低于下限、等于上限、等于下限以及0作为特殊数字和正常数字等各种边界情况的测试用例。\n\n 很多情况下，通过UI无法提交的数字，若接口测试通过协议直接发送请求，在应用程序的后台没有进行校验并且数据库没有添加约束条件的情况下，仍然可以正常提交，这可能导致数据无法正常存储等严重问题。\n\n 例如，购物车中单个商品数量最大可以为9999，考虑边界值测试用例方法设计接口测试用例，则可以获得以下边界用例，ChatGPT生成的边界值接口测试用例如下所示。\n\n**ChatGPT****生成的边界值接口测试用例**\n\n> \n用例编号：TC001\n用例名称:添加数量为0的商品\n输入参数:\nitem_id: 123456\nnum: 0\n预期结果:\n添加失败，提示数量不能为0\n......\n用例编号：TC005\n用例名称:添加超过库存的商品\n输入参数:\nitem_id: 123456\nnum: 10000\n预期结果:\n添加失败，提示超过库存\n......\n\n\n\n",
    "labels": [],
    "created_at": "2024-12-20T16:17:55Z"
  },
  {
    "id": 1,
    "title": "LLM后训练绝招：1%预训练成本，实现最高20倍算力扩展效果",
    "url": "https://blog.csdn.net/OneFlow_Official/article/details/144124481",
    "content": "\n<img alt=\"d752e993e29569be524ff4dbc483663b.png\" src=\"https://img-blog.csdnimg.cn/img_convert/d752e993e29569be524ff4dbc483663b.png\" referrerpolicy=\"no-referrer\">\n\n\n\n根据规模定律，扩大训练计算规模可以提高大型语言模型（LLM）性能的关键，但调研机构Epoch AI的研究，LLM再训练无需高额费用，也能让AI能力获得显著提升。\n\n 在该研究中，他们引入了一个基本框架，用于量化后训练增强的收益和成本，特别是通过计算等效增益来衡量收益。他们将该框架应用于一系列具有代表性的后训练增强，并发现性能提升非常显著，但微调成本通常与预训练成本相比非常小，某些后训练增强技术可以在不到1%预训练成本的情况下，提供相当于增加5到20倍预训练计算资源获得的效果。\n\n （本文由OneFlow编译发布，转载请联系授权。原文：https://epochai.org/blog/ai-capabilities-can-be-significantly-improved-without-expensive-retraining）\n\n**<strong>作者**|</strong>**<strong>EPOCH AI**</strong>\n\n**OneFlow编译**\n\n<strong>翻译｜刘乾裕\n**题图由****[SiliconCloud]()平台生成**</strong>\n\n近年来，训练大语言模型和类似基础模型所需的高强度计算资源，已成为推动人工智能进步的主驱动力之一。这也让人们深刻认识到一个“惨痛教训”：更好利用计算资源的通用方法最终被证明是最有效的。如今，训练最前沿模型的成本已经高到只有少数参与者能够承担（https://epochai.org/blog/trends-in-the-dollar-training-cost-of-machine-learning-systems）。\n\n我们的研究探讨了在训练完成后提升模型性能的方法，这些方法无需依赖大量计算资源。我们将这些改进措施分为五类，详见下表。\n\n<img alt=\"9f94c575ad16d35fefad5a9b4e08391f.png\" src=\"https://img-blog.csdnimg.cn/img_convert/9f94c575ad16d35fefad5a9b4e08391f.png\" referrerpolicy=\"no-referrer\">\n\n\n\n后训练增强的类别及示例\n \n\n你可以在此处阅读完整论文（https://arxiv.org/abs/2312.07413）。本文由Epoch AI、Open Philanthropy、加州大学伯克利分校和ORCG合作完成。\n\n### **1**\n\n### **关键成果**\n\n**计算等效增益 (CEG）**：我们提出了计算等效增益这一概念，用于量化各类增强方法带来的性能提升。CEG被定义为在不采用增强的情况下，预训练计算量需要增加多少才能达到与增强方法相同的基准性能提升。我们开发了一种基于公开基准进行评估的估算方法，以此来计算CEG。\n\n<img alt=\"2e24ca31a86d2cbd9f8bc911a8e12cc4.jpeg\" src=\"https://img-blog.csdnimg.cn/img_convert/2e24ca31a86d2cbd9f8bc911a8e12cc4.jpeg\" referrerpolicy=\"no-referrer\">\n\n\n\n该示例中，CEG（计算等效增益）是5倍。同样的性能提升可以通过应用后训练增强（PTE）或将预训练计算规模扩大5倍来实现。\n\n**后训练增强效果调查（PTE）**：我们对PTE进行了研究，涵盖了五个方面即工具、提示、辅助结构（scaffolding）、解决方案选择和数据增强。他们的CEG估算值通常在相关基准数值的5到30倍之间。\n\n我们还对这些PTE的计算成本进行了估算，主要包括两类：一是使模型能够应用PTE所需的初始成本（例如，微调模型以学习使用某种工具）；二是推理过程中产生的持续成本（例如，增强方法需要生成多个样本并从中选取最佳结果）。对于我们评估的所有PTE，其初始成本通常低于预训练成本的10%，大部分甚至不到0.1%。尽管大多数情况下推理成本不会受到明显影响，但在个别情况下推理成本可能会增加至100倍左右。\n\n<img alt=\"45c9d55646bfa168b6cc2697f2759dc3.png\" src=\"https://img-blog.csdnimg.cn/img_convert/45c9d55646bfa168b6cc2697f2759dc3.png\" referrerpolicy=\"no-referrer\">\n\n\n\n结果概要：使用计算等效增益量化的技术所产生的改进。x轴展示了相关的一次性成本（左）和推理成本（右）。\n\n### **2**\n\n### **政策影响**\n\n随着后训练增强技术的不断发展和完善，已部署的大语言模型功能将会随着时间推移而增强。这一发现表明，安全策略（例如，负责任扩展策略，metr.org/blog/2023-09-26-rsp）应当包含一个“安全缓冲区”：限制那些在未来随着后训练增强，可能达到危险水平的模型功能。\n\n由于这些增强功能的计算成本较低，更多参与者能够加入开发，不再局限于那些具备大规模预训练能力的主体。这使得能力提升趋向民主化，但也为AI发展的监管带来新挑战，因为仅关注那些拥有大量计算资源的实体已不足以有效应对未来的风险。\n\n其他人都在看\n    [推算LLM训练的GPU内存需求]()        [用初中数学理解LLM工作原理]()        [企业AI调查:AI支出激增6倍，多模型部署盛行]()        [10倍工程师编码工具：Cursor x SiliconCloud]()        [强化学习之父Rich Sutton：AGI研究下一个范式]()        [大模型训练秘方:1000次超参数优化实验的发现]()        [LLM逻辑推演策略：推理时计算vs训练时计算]()    \n\n\n\n**让超级产品开发者实现“Token自由”**\n \n\n**<em>**[邀请好友体验SiliconCloud]()，****</em>**<em>**狂送2000万Token/人****</em>\n\n****邀请越多，Token奖励越多****\n****[siliconflow.cn/zh-cn/siliconcloud](http://siliconflow.cn/zh-cn/siliconcloud)****\n",
    "labels": [],
    "created_at": "2024-12-20T23:16:47Z"
  }
]